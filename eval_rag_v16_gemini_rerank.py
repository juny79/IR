import os
import json
import time
import hashlib
from pathlib import Path

import numpy as np
import faiss
from tqdm import tqdm
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder
from dotenv import load_dotenv

# .env ë¡œë“œ (Gemini client import ì´ì „ì— ìˆ˜í–‰)
load_dotenv()

from models.gemini_client import gemini_client

# ==========================================
# 1. ì„¤ì • / ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
OUTPUT_FILE = os.getenv("SUBMISSION_FILE") or "/root/IR/submission_v16_gemini_rerank.csv"

BGE_M3_MODEL = "BAAI/bge-m3"
RERANK_MODEL = "BAAI/bge-reranker-v2-m3"

TOP_CANDIDATES = int(os.getenv("TOP_CANDIDATES", "200"))
FINAL_TOPK = int(os.getenv("FINAL_TOPK", "5"))
GEMINI_RERANK_TOPK = int(os.getenv("GEMINI_RERANK_TOPK", "10"))
ALPHA = float(os.getenv("ALPHA", "0.5"))
RRF_K = int(os.getenv("RRF_K", "60"))

# v9 ê¸°ì¤€ íŠœë‹ëœ ê²Œì´íŒ…(empty topk)
EMPTY_IDS = {
    276, 261, 283, 32, 94, 90, 220, 245, 229,
    247, 67, 57, 2, 227, 301, 222, 83, 64, 103, 218
}

CACHE_DIR = Path(os.getenv("CACHE_DIR", "/root/IR/cache/v16_gemini"))
CACHE_DIR.mkdir(parents=True, exist_ok=True)


def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            data.append(json.loads(line))
    return data


print("ğŸš€ ë°ì´í„° ë¡œë”© ì¤‘...")
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

docid_to_index = {doc_id: i for i, doc_id in enumerate(doc_ids)}

# ==========================================
# 2. ëª¨ë¸ ë¡œë”©
# ==========================================
print("â³ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘...")
model = BGEM3FlagModel(BGE_M3_MODEL, use_fp16=True)

BGE_CACHE_DIR = "/root/IR/cache/bge_m3"
DENSE_EMB_PATH = os.path.join(BGE_CACHE_DIR, "doc_dense_embs.npy")
SPARSE_EMB_PATH = os.path.join(BGE_CACHE_DIR, "doc_sparse_embs.json")
FAISS_INDEX_PATH = os.path.join(BGE_CACHE_DIR, "bge_m3_dense.index")

print("âœ… ìºì‹œëœ BGE-M3 ì¸ë±ìŠ¤ ë¡œë“œ")
doc_dense_embs = np.load(DENSE_EMB_PATH)
with open(SPARSE_EMB_PATH, "r", encoding="utf-8") as f:
    doc_sparse_embs = json.load(f)
index = faiss.read_index(FAISS_INDEX_PATH)

print("â³ Reranker ë¡œë”© ì¤‘...")
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device="cuda")


# ==========================================
# 3. í•µì‹¬ ìœ í‹¸
# ==========================================

def _safe_extract_json_object(text: str):
    if not text:
        return None

    cleaned = str(text).strip()
    if "```" in cleaned:
        parts = cleaned.split("```")
        cleaned = max((p.strip() for p in parts if p.strip()), key=len, default=cleaned)
        if cleaned.lower().startswith("json"):
            cleaned = cleaned[4:].strip()

    start = cleaned.find("{")
    end = cleaned.rfind("}")
    if start == -1 or end == -1 or end <= start:
        return None

    candidate = cleaned[start : end + 1].strip()
    try:
        return json.loads(candidate)
    except Exception:
        return None


def get_multi_queries_gemini(messages):
    """Geminië¡œ 3ê°œ ë©€í‹°ì¿¼ë¦¬ ìƒì„± (v9ì˜ Solar ë©€í‹°ì¿¼ë¦¬ ëŒ€ì²´)."""
    # Cache by conversation text to avoid repeated Gemini calls across reruns/resume.
    try:
        conversation_text = "\n".join(
            [f"{m.get('role','user')}: {m.get('content','')}" for m in (messages or [])]
        ).strip()
    except Exception:
        conversation_text = str(messages)

    cache_key = hashlib.md5(conversation_text.encode("utf-8", errors="ignore")).hexdigest()
    cache_path = CACHE_DIR / f"gemini_multiq_{cache_key}.json"
    if cache_path.exists():
        try:
            cached = json.loads(cache_path.read_text(encoding="utf-8"))
            queries = cached.get("queries", [])
            queries = [q for q in queries if isinstance(q, str) and q.strip()]
            if queries:
                return queries[:3]
        except Exception:
            pass

    system_prompt = """ë‹¹ì‹ ì€ ê³¼í•™ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ì—”ì§„ì— ì…ë ¥í•  '3ê°€ì§€ ë²„ì „ì˜ ê²€ìƒ‰ì–´'ë¥¼ JSONìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
{
  "queries": [
    "êµ¬ì²´ì ì´ê³  ì™„ê²°ëœ ì„œìˆ í˜• ì§ˆë¬¸ (ê°€ì¥ ì¤‘ìš”)",
    "í•µì‹¬ í‚¤ì›Œë“œ ë‚˜ì—´ (ëª…ì‚¬ ìœ„ì£¼)",
    "ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ ì§ˆë¬¸"
  ]
}
"""

    resp = gemini_client._call_with_retry(
        prompt=[{"role": "system", "content": system_prompt}] + messages,
        temperature=0,
        max_tokens=256,
        response_format={"type": "json_object"},
    )

    parsed = _safe_extract_json_object(resp) or {}
    queries = parsed.get("queries", [])

    # fallback: ì› ì§ˆë¬¸ í¬í•¨ ë³´ì¥
    original_q = messages[-1].get("content", "") if messages else ""
    if original_q and original_q not in queries:
        queries.append(original_q)

    # ë¬¸ìì—´ë§Œ ì¶”ì¶œ
    queries = [q for q in queries if isinstance(q, str) and q.strip()]

    final = queries[:3] if queries else ([original_q] if original_q else [])
    try:
        cache_path.write_text(json.dumps({"queries": final}, ensure_ascii=False), encoding="utf-8")
    except Exception:
        pass

    return final


def hybrid_search_multi(queries, top_k=200):
    """v9ì™€ ë™ì¼í•œ ë°©ì‹:
    - ê° ì¿¼ë¦¬ë³„ë¡œ dense top_k í›„ë³´ë¥¼ ë½‘ê³ 
    - ê·¸ í›„ë³´ë“¤ì— ëŒ€í•´ lexical matching score ê³„ì‚°
    - ALPHAë¡œ í˜¼í•© í›„ ì¿¼ë¦¬ë³„ ë­í‚¹ì„ RRFë¡œ í•©ì¹¨
    """
    if not queries:
        return []

    all_results = []
    for q_text in queries:
        q_output = model.encode([q_text], return_dense=True, return_sparse=True, max_length=8192)
        q_dense = q_output["dense_vecs"][0].astype("float32")
        q_sparse = q_output["lexical_weights"][0]

        dense_scores, dense_indices = index.search(np.expand_dims(q_dense, 0), top_k)
        dense_indices = dense_indices[0]
        dense_scores = dense_scores[0]

        if len(dense_scores) > 0:
            d_min, d_max = dense_scores.min(), dense_scores.max()
            if d_max > d_min:
                dense_scores = (dense_scores - d_min) / (d_max - d_min)
            else:
                dense_scores = np.ones_like(dense_scores)

        sparse_scores = []
        for idx in dense_indices:
            score = model.compute_lexical_matching_score(q_sparse, doc_sparse_embs[idx])
            sparse_scores.append(score)
        sparse_scores = np.array(sparse_scores)

        if len(sparse_scores) > 0:
            s_min, s_max = sparse_scores.min(), sparse_scores.max()
            if s_max > s_min:
                sparse_scores = (sparse_scores - s_min) / (s_max - s_min)
            else:
                sparse_scores = np.ones_like(sparse_scores)

        hybrid_scores = ALPHA * dense_scores + (1 - ALPHA) * sparse_scores
        sorted_indices = np.argsort(hybrid_scores)[::-1]
        all_results.append([dense_indices[i] for i in sorted_indices])

    rrf_scores = {}
    for results in all_results:
        for rank, idx in enumerate(results):
            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (RRF_K + rank)

    final_indices = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
    return final_indices[:top_k]


def gemini_rerank_topk(query: str, candidates: list[str], candidate_docids: list[str] | None = None) -> int:
    """Geminië¡œ top-k í›„ë³´ ì¤‘ best_index ì„ íƒ (ìºì‹± í¬í•¨)."""
    if len(candidates) <= 1:
        return 0

    # cache key
    h = hashlib.md5()
    h.update(query.encode("utf-8", errors="ignore"))
    if candidate_docids:
        h.update("|".join(candidate_docids).encode("utf-8", errors="ignore"))
    else:
        h.update("|".join(str(len(c)) for c in candidates).encode("utf-8", errors="ignore"))

    cache_path = CACHE_DIR / f"gemini_rerank_{h.hexdigest()}.json"
    if cache_path.exists():
        try:
            cached = json.loads(cache_path.read_text(encoding="utf-8"))
            return int(cached.get("best_index", 0))
        except Exception:
            pass

    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ê³¼í•™ ì§€ì‹ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ í›„ë³´(Candidate)ë“¤ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ì •í™•í•˜ê³ , ì§ì ‘ì ì¸ í•´ë‹µì„ í¬í•¨í•˜ê³  ìˆëŠ” ë¬¸ì„œë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.

ì„ íƒ ê¸°ì¤€:
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì— ì™„ë²½íˆ ë¶€í•©í•˜ëŠ”ê°€?
2. ê³¼í•™ì  ì‚¬ì‹¤ì´ ì •í™•í•œê°€?
3. ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ”ê°€?

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ {"best_index": 0} ì™€ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”."""

    candidate_text = ""
    for i, content in enumerate(candidates):
        candidate_text += f"Candidate {i}:\n{content[:2000]}\n\n"

    user_prompt = f"## ì§ˆë¬¸:\n{query}\n\n## ê²€ìƒ‰ í›„ë³´:\n{candidate_text}"

    resp = gemini_client._call_with_retry(
        prompt=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0,
        max_tokens=120,
        response_format={"type": "json_object"},
    )

    parsed = _safe_extract_json_object(resp) or {}
    best_index = parsed.get("best_index", 0)

    try:
        best_index = int(best_index)
    except Exception:
        best_index = 0

    if best_index < 0 or best_index >= len(candidates):
        best_index = 0

    try:
        cache_path.write_text(json.dumps({"best_index": best_index}, ensure_ascii=False), encoding="utf-8")
    except Exception:
        pass

    return best_index


# ==========================================
# 4. ì‹¤í–‰ (Resume)
# ==========================================
processed_ids = set()
if os.path.exists(OUTPUT_FILE):
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    processed_ids.add(json.loads(line)["eval_id"])
                except Exception:
                    continue
    except Exception:
        pass

limit = os.getenv("EVAL_LIMIT")
limit = int(limit) if limit and str(limit).isdigit() else None

with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
    written = 0
    for entry in tqdm(eval_data):
        eval_id = entry["eval_id"]
        if eval_id in processed_ids:
            continue

        messages = entry["msg"]

        if eval_id in EMPTY_IDS:
            f.write(json.dumps({"eval_id": eval_id, "topk": [], "answer": "ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ì…ë‹ˆë‹¤."}, ensure_ascii=False) + "\n")
            f.flush()
            written += 1
        else:
            # 1) multi-query (Gemini)
            queries = get_multi_queries_gemini(messages)
            rerank_query = queries[0]

            # 2) retrieve candidates
            candidate_indices = hybrid_search_multi(queries, top_k=TOP_CANDIDATES)

            if candidate_indices:
                # 3) cross-encoder rerank
                pairs = [[rerank_query, doc_contents[idx]] for idx in candidate_indices]
                rerank_scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)
                sorted_ranks = sorted(zip(candidate_indices, rerank_scores), key=lambda x: x[1], reverse=True)

                top_indices = [idx for idx, _ in sorted_ranks[:GEMINI_RERANK_TOPK]]
                top_docids = [doc_ids[idx] for idx in top_indices]
                top_contents = [doc_contents[idx] for idx in top_indices]

                # 4) gemini choose best among top-k
                best_idx = gemini_rerank_topk(rerank_query, top_contents, candidate_docids=top_docids)

                if best_idx >= len(top_indices) or best_idx < 0:
                    best_idx = 0

                best_doc_idx = top_indices.pop(best_idx)
                final_indices = [best_doc_idx] + top_indices

                final_ids = [doc_ids[idx] for idx in final_indices[:FINAL_TOPK]]

                res = {
                    "eval_id": eval_id,
                    "standalone_query": rerank_query,
                    "topk": final_ids,
                    "answer": ""  # ë¦¬ë”ë³´ë“œ ì±„ì ì€ topk ì¤‘ì‹¬ì´ë¼ ë‹µë³€ì€ ë¹„ì›Œë‘ 
                }
            else:
                res = {"eval_id": eval_id, "topk": [], "answer": ""}

            f.write(json.dumps(res, ensure_ascii=False) + "\n")
            f.flush()
            written += 1

        if limit is not None and written >= limit:
            break

print(f"âœ… Created/updated: {OUTPUT_FILE}")
