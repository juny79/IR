import os
import json
import sys
import numpy as np
import faiss
from pathlib import Path
from datetime import datetime, timezone, timedelta
from tqdm import tqdm
from kiwipiepy import Kiwi
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, CrossEncoder
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
print(f"ğŸ”‘ OpenAI API Key loaded: {os.getenv('OPENAI_API_KEY')[:5]}***")

# LLM í´ë¼ì´ì–¸íŠ¸ë“¤ ì„í¬íŠ¸
from models.solar_client import solar_client
from models.gemini_client import gemini_client
from models.openai_client import openai_client

# ==========================================
# 0. LLM ì„ íƒ (ì¸ì ì²˜ë¦¬)
# ==========================================
# ì‚¬ìš©ë²•: python eval_rag_e5_repro.py [solar_pro|gemini|gpt4o|solar_mini]
llm_type = sys.argv[1] if len(sys.argv) > 1 else "solar_mini"

if llm_type == "solar_pro":
    active_client = solar_client
    active_client.model = "solar-pro"
    model_label = "Solar Pro"
elif llm_type == "gemini":
    active_client = gemini_client
    active_client.model_name = "gemini-3-flash-preview"
    import google.generativeai as genai
    generation_config = {
        "temperature": 0.1,
        "response_mime_type": "application/json",
        "max_output_tokens": 2048,
    }
    active_client.model = genai.GenerativeModel(
        model_name=active_client.model_name,
        generation_config=generation_config
    )
    model_label = "Gemini 3 Flash Preview"
elif llm_type == "gpt4o":
    active_client = openai_client
    active_client.model = "gpt-4o"
    model_label = "GPT-4o"
else:
    active_client = solar_client
    active_client.model = "solar-1-mini-chat"
    model_label = "Solar 1 Mini"

print(f"ğŸŒŸ Active LLM: {model_label}")

# ==========================================
# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
OUTPUT_FILE = f"/root/IR/submission_e5_{llm_type}.csv"

# ëª¨ë¸ ì„¤ì •
EMBED_MODEL = "intfloat/multilingual-e5-large"
RERANK_MODEL = "BAAI/bge-reranker-v2-m3"

# íŒŒë¼ë¯¸í„° (ë™ë£Œì˜ 0.9174 ì„¸íŒ…)
RRF_K = 60
BM25_TOPN = 50
DENSE_TOPN = 50
TOP_CANDIDATES = 100
RERANK_BATCH = 32
FINAL_TOPK = 5
W3_WEIGHTS = [0.6, 0.3, 0.3, 1.6, 1.0, 1.0]

def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            data.append(json.loads(line))
    return data

print("ğŸš€ ë°ì´í„° ë¡œë”© ì¤‘...")
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

# ==========================================
# 2. ì¸ë±ì‹± (Kiwi + E5)
# ==========================================
kiwi = Kiwi()
def tokenizer(text: str):
    tokens = kiwi.tokenize(text)
    return [t.form for t in tokens if t.tag.startswith("N") or t.tag in ["SL", "SN"]]

print("BM25 ì¸ë±ì‹± ì¤‘...")
tokenized_corpus = [tokenizer(doc) for doc in doc_contents]
bm25 = BM25Okapi(tokenized_corpus)

print(f"Vector ì¸ë±ì‹± ë¡œë“œ ì¤‘ ({EMBED_MODEL})...")
embedder = SentenceTransformer(EMBED_MODEL, device="cuda")
# ê¸°ì¡´ì— ìƒì„±ëœ FAISS ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„± (ì—¬ê¸°ì„œëŠ” ìƒˆë¡œ ìƒì„±í•˜ëŠ” ë¡œì§ í¬í•¨)
FAISS_CACHE_PATH = "/root/IR/cache/faiss_e5_large.index"
EMB_CACHE_PATH = "/root/IR/cache/doc_embeddings_e5_large.npy"

if os.path.exists(FAISS_CACHE_PATH) and os.path.exists(EMB_CACHE_PATH):
    print("âœ… ìºì‹œëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
    index = faiss.read_index(FAISS_CACHE_PATH)
else:
    print("â³ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    os.makedirs("/root/IR/cache", exist_ok=True)
    passage_texts = ["passage: " + doc for doc in doc_contents]
    doc_embeddings = embedder.encode(passage_texts, normalize_embeddings=True, show_progress_bar=True).astype("float32")
    np.save(EMB_CACHE_PATH, doc_embeddings)
    index = faiss.IndexFlatIP(doc_embeddings.shape[1])
    index.add(doc_embeddings)
    faiss.write_index(index, FAISS_CACHE_PATH)

print(f"Reranker ë¡œë”© ì¤‘ ({RERANK_MODEL})...")
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device="cuda")

# ==========================================
# 3. í•µì‹¬ í•¨ìˆ˜ (ë™ë£Œ ë¡œì§ ë³µì œ)
# ==========================================
def process_query_expanded(messages):
    system_prompt = """ë‹¹ì‹ ì€ RAG(ë¬¸ì„œê²€ìƒ‰)ìš© ì§ˆë¬¸ ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ "ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€" íŒë‹¨í•˜ê³ , ê²€ìƒ‰ì— ì“¸ ì¿¼ë¦¬ 3ê°œì™€ HyDEë¥¼ ìƒì„±í•˜ì„¸ìš”.

[íŒë‹¨ ê¸°ì¤€]
- should_search=true:
  ì§€ì‹/ê¸°ìˆ /ì—­ì‚¬/ì‚¬íšŒ/ë¬¸í™”/ê³¼í•™/ì„¤ëª…/ì •ì˜/ì›ë¦¬/ë¹„êµ/ì›ì¸/ë°©ë²• ë“±
  ì½”í¼ìŠ¤ì—ì„œ ê·¼ê±° ë¬¸ì„œë¥¼ ì°¾ì•„ì•¼ ì •í™•í•´ì§€ëŠ” ì§ˆë¬¸
- should_search=false:
  ìˆœìˆ˜ ì¡ë‹´/ì¸ì‚¬/ê°ì •í‘œí˜„/ë©”íƒ€ ëŒ€í™”("ê³ ë§ˆì›Œ", "ì•ˆë…•", "ë„ˆ ëˆ„êµ¬ì•¼")
  ë˜ëŠ” ì‹¤ì‹œê°„ ì •ë³´(ë‚ ì”¨/í˜„ì¬ì‹œê°/ì£¼ê°€ ë“±) ê°™ì´ ì½”í¼ìŠ¤ë¡œ í•´ê²° ë¶ˆê°€í•œ ì§ˆë¬¸

[ì¶œë ¥ JSON í˜•ì‹ë§Œ!]
{
  "should_search": true/false,
  "confidence": 0.0~1.0,
  "standalone_query": "ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ ì§ˆë¬¸ë¬¸(ê°€ì¥ êµ¬ì²´ì )",
  "queries": [
    "êµ¬ì²´ì  ì„œìˆ í˜•(standalone_queryì™€ ê°™ê±°ë‚˜ ìœ ì‚¬)",
    "í•µì‹¬ í‚¤ì›Œë“œ ë‚˜ì—´",
    "ìœ ì‚¬ í‘œí˜„/ë‹¤ë¥¸ ê´€ì ì˜ ì§ˆë¬¸"
  ],
  "hyde": "ê°€ì„¤ì  ë‹µë³€(200ì ì´ë‚´, ë¬¸ì„œì— ìˆì„ ë²•í•œ ë‚´ìš©ìœ¼ë¡œ)"
}"""
    
    try:
        response_text = active_client._call_with_retry(
            prompt=[{"role": "system", "content": system_prompt}] + messages,
            temperature=0,
            max_tokens=2048,
            response_format={"type": "json_object"}
        )
        if not response_text:
            raise ValueError("Empty response from LLM")
            
        # Markdown code block ì œê±° (ìˆì„ ê²½ìš°)
        clean_text = response_text.strip()
        if clean_text.startswith("```"):
            # ```json ... ``` ë˜ëŠ” ``` ... ``` ì œê±°
            lines = clean_text.splitlines()
            if lines[0].startswith("```"):
                lines = lines[1:]
            if lines and lines[-1].startswith("```"):
                lines = lines[:-1]
            clean_text = "\n".join(lines).strip()
            
        parsed = json.loads(clean_text)
        
        standalone_query = parsed.get("standalone_query", "")
        queries = parsed.get("queries", [])
        if not queries: queries = [standalone_query]
        if queries[0] != standalone_query:
            queries = [standalone_query] + [q for q in queries if q != standalone_query]
        
        return {
            "is_science": bool(parsed.get("should_search", True)),
            "queries": queries[:3],
            "standalone_query": standalone_query
        }
    except Exception as e:
        print(f"âš ï¸ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        last_content = messages[-1]["content"]
        return {
            "is_science": True,
            "queries": [last_content],
            "standalone_query": last_content
        }

def reciprocal_rank_fusion_weighted(rank_lists, k=60, weights=None):
    if weights is None:
        weights = [1.0] * len(rank_lists)
    
    scores = {}
    for w, rank_list in zip(weights, rank_lists):
        for rank, doc_idx in enumerate(rank_list):
            scores[doc_idx] = scores.get(doc_idx, 0.0) + w * (1.0 / (k + rank + 1))
    
    return sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

# ì €ì¥
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for i, entry in enumerate(tqdm(eval_data)):
        eval_id = entry["eval_id"]
        messages = entry["msg"]
        
        print(f"[{i+1}/{len(eval_data)}] Processing eval_id: {eval_id}...")
        processed = process_query_expanded(messages)
        
        if not processed["is_science"]:
            res = {
                "eval_id": eval_id,
                "standalone_query": "",
                "topk": [],
                "answer": "ê³¼í•™ ìƒì‹ê³¼ ê´€ë ¨ ì—†ëŠ” ëŒ€í™”ì…ë‹ˆë‹¤.",
                "references": []
            }
        else:
            queries = processed["queries"]
            main_query = queries[0]
            
            all_bm25_lists = []
            all_faiss_lists = []
            
            for q in queries:
                # BM25
                tokens = tokenizer(q)
                if tokens:
                    bm25_scores = bm25.get_scores(tokens)
                    top_idx = np.argsort(bm25_scores)[::-1][:BM25_TOPN]
                    all_bm25_lists.append(top_idx.tolist())
                else:
                    all_bm25_lists.append([])
                    
                # Dense
                q_emb = embedder.encode(["query: " + q], normalize_embeddings=True)
                _, f_idx = index.search(q_emb, DENSE_TOPN)
                all_faiss_lists.append(f_idx[0].tolist())
                
            rank_lists = all_bm25_lists + all_faiss_lists
            weights = W3_WEIGHTS if len(rank_lists) == 6 else [1.0] * len(rank_lists)
            
            candidate_indices = reciprocal_rank_fusion_weighted(rank_lists, k=RRF_K, weights=weights)
            top_candidates = candidate_indices[:TOP_CANDIDATES]
            
            if not top_candidates:
                res = {
                    "eval_id": eval_id,
                    "standalone_query": main_query,
                    "topk": [],
                    "answer": "ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "references": []
                }
            else:
                # Rerank
                pairs = [[main_query, doc_contents[idx]] for idx in top_candidates]
                scores = reranker.predict(pairs, batch_size=RERANK_BATCH, show_progress_bar=False)
                sorted_ranks = sorted(zip(top_candidates, scores), key=lambda x: x[1], reverse=True)
                
                final_top_idx = [idx for idx, _ in sorted_ranks[:FINAL_TOPK]]
                final_topk = [doc_ids[idx] for idx in final_top_idx]
                final_contents = [doc_contents[idx] for idx in final_top_idx]
                
                res = {
                    "eval_id": eval_id,
                    "standalone_query": main_query,
                    "topk": final_topk,
                    "answer": final_contents[0] if final_contents else "ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "references": [{"score": 0, "content": c} for c in final_contents]
                }
        
        f.write(json.dumps(res, ensure_ascii=False) + "\n")
        f.flush()

print(f"âœ… ì¬í˜„ ì™„ë£Œ! íŒŒì¼ ì €ì¥ë¨: {OUTPUT_FILE}")

print(f"âœ… ì¬í˜„ ì™„ë£Œ! íŒŒì¼ ì €ì¥ë¨: {OUTPUT_FILE}")
