import sys
import os
import json
import numpy as np
import faiss
from tqdm import tqdm
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from retrieval.es_connector import es

# ì„¤ì •
INPUT_QA_FILE = "./data/synthetic_qa_solar.jsonl"
OUTPUT_TRAIN_FILE = "./data/train_data_v3.jsonl"
DOC_PATH = "./data/documents.jsonl"
NEG_COUNT = 7
POOL_SIZE = 50

def mine_hard_negatives_v3():
    if not os.path.exists(INPUT_QA_FILE):
        print(f"ì˜¤ë¥˜: {INPUT_QA_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(">>> 2ë‹¨ê³„ V3: ê³ ë„í™”ëœ Hard Negative Mining ì‹œì‘ (Hybrid + Reranker)...")
    
    # 1. ë¬¸ì„œ ë°ì´í„° ë¡œë“œ
    print("â³ ë¬¸ì„œ ë¡œë”© ì¤‘...")
    documents = {}
    with open(DOC_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            obj = json.loads(line)
            documents[obj['docid']] = obj['content']
    doc_ids = list(documents.keys())
    doc_contents = list(documents.values())

    # 2. ëª¨ë¸ ë° ì¸ë±ìŠ¤ ë¡œë“œ
    print("â³ BGE-M3 ëª¨ë¸ ë° FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
    
    CACHE_DIR = "./cache/bge_m3"
    FAISS_INDEX_PATH = os.path.join(CACHE_DIR, "bge_m3_dense.index")
    if not os.path.exists(FAISS_INDEX_PATH):
        print("ì˜¤ë¥˜: FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ë±ì‹±ì„ ì™„ë£Œí•˜ì„¸ìš”.")
        return
    index = faiss.read_index(FAISS_INDEX_PATH)

    print("â³ Reranker ë¡œë“œ ì¤‘...")
    reranker = CrossEncoder('BAAI/bge-reranker-v2-m3', device='cuda')

    # 3. QA ë°ì´í„° ë¡œë“œ ë° Flatten
    qa_pairs = []
    with open(INPUT_QA_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            docid = item['docid']
            content = item['content']
            for q in item['questions']:
                qa_pairs.append({
                    "query": q,
                    "docid": docid,
                    "content": content
                })
    
    training_data = []
    
    print(f"ğŸš€ Mining ì‹œì‘ (ì´ {len(qa_pairs)}ê°œ ì§ˆë¬¸)...")
    for item in tqdm(qa_pairs):
        query = item['query']
        positive_id = item['docid']
        positive_content = item['content']
        
        candidate_contents = set()
        
        try:
            # A. BM25 Retrieval
            res_bm25 = es.search(
                index="test", 
                query={"match": {"content": query}}, 
                size=POOL_SIZE 
            )
            for hit in res_bm25['hits']['hits']:
                if hit['_source']['docid'] != positive_id:
                    candidate_contents.add(hit['_source']['content'])
            
            # B. Dense Retrieval
            q_emb = model.encode([query], return_dense=True)['dense_vecs']
            _, indices = index.search(q_emb.astype('float32'), POOL_SIZE)
            for idx in indices[0]:
                if doc_ids[idx] != positive_id:
                    candidate_contents.add(doc_contents[idx])
            
            # C. Reranking to find the hardest negatives
            candidates = list(candidate_contents)
            if not candidates:
                continue
                
            pairs = [[query, c] for c in candidates]
            # Rerankerë¡œ ì ìˆ˜ ê³„ì‚°
            scores = reranker.predict(pairs, batch_size=128, show_progress_bar=False)
            
            # ì ìˆ˜ê°€ ë†’ì€ ìˆœ(Hardest)ìœ¼ë¡œ ì •ë ¬
            scored_candidates = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
            
            negatives = [c for c, s in scored_candidates[:NEG_COUNT]]
            
            if len(negatives) >= 1: # ìµœì†Œ 1ê°œë¼ë„ ìˆìœ¼ë©´ ì¶”ê°€
                training_data.append({
                    "query": query,
                    "pos": [positive_content],
                    "neg": negatives
                })
        except Exception as e:
            print(f"Error mining for query '{query}': {e}")
            continue
            
        # ì¤‘ê°„ ì €ì¥ (1000ê°œë§ˆë‹¤)
        if len(training_data) % 1000 == 0:
            with open(OUTPUT_TRAIN_FILE, 'w', encoding='utf-8') as f:
                for d in training_data:
                    f.write(json.dumps(d, ensure_ascii=False) + '\n')

    # ìµœì¢… ì €ì¥
    with open(OUTPUT_TRAIN_FILE, 'w', encoding='utf-8') as f:
        for d in training_data:
            f.write(json.dumps(d, ensure_ascii=False) + '\n')
    
    print(f"âœ… Mining ì™„ë£Œ! {len(training_data)}ê°œì˜ í•™ìŠµ ë°ì´í„°ê°€ {OUTPUT_TRAIN_FILE}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    mine_hard_negatives_v3()
