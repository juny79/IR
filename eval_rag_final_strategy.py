"""
ğŸ¯ ìµœì¢… ì „ëµ: submission_33 ê²Œì´íŒ… + Hard Voting + Multi-Query (Sparse ë³´ê°•)

1. ê²Œì´íŒ…: submission_33ì˜ 21ê°œ Empty Case ê°•ì œ ì ìš© (ê°ì  ë°©ì§€)
2. ê²€ìƒ‰: Hard Voting [5, 4, 2] + SBERT/Gemini ì•™ìƒë¸” (ê²€ì¦ëœ ì„±ëŠ¥)
3. ì¿¼ë¦¬: ë™ë£Œì˜ 3ê´€ì  Multi-Query ë„ì… -> Sparse ê²€ìƒ‰ ë³´ê°• (ì¬í˜„ìœ¨ í–¥ìƒ)
"""

import json
import os
from tqdm import tqdm
from models.solar_client import solar_client
from retrieval.hybrid_search import run_hybrid_search
from retrieval.es_connector import es

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 1. ê²Œì´íŒ…: submission_33ì˜ Empty ID (21ê°œ)ì¸)	21ê°œ ê°•ì œ ì ìš© (ê°ì  ì›ì²œ ì°¨ë‹¨)
ê²€ìƒ‰ ëª¨ë¸	SBERT ë‹¨ë… (ë„ˆí”„ë¨)	SBERT + Gemini ì•™ìƒë¸” (ê¸°ì¡´ ìµœê°•)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
EMPTY_IDS = {
    276, 261, 283, 32, 94, 90, 108, 220, 245, 229, 
    247, 67, 57, 2, 227, 301, 222, 83, 64, 103, 218
}

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 2. ì¿¼ë¦¬ ìƒì„±: ë™ë£Œì˜ 3ê´€ì  Multi-Query
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
def generate_multi_query_3(query_text, messages=None):
    """
    3ê´€ì  Multi-Query ìƒì„± (ë™ë£Œ ì½”ë“œ ë°©ì‹)
    Returns: [q1, q2, q3]
    """
    system_prompt = """ë‹¹ì‹ ì€ ê³¼í•™ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ì—”ì§„ì— ì…ë ¥í•  '3ê°€ì§€ ë²„ì „ì˜ ê²€ìƒ‰ì–´'ë¥¼ JSONìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.

[ì¶œë ¥ JSON í˜•ì‹]
{
    "queries": [
        "êµ¬ì²´ì ì´ê³  ì™„ê²°ëœ ì„œìˆ í˜• ì§ˆë¬¸ (ê°€ì¥ ì¤‘ìš”)",
        "í•µì‹¬ í‚¤ì›Œë“œ ë‚˜ì—´ (ëª…ì‚¬ ì¤‘ì‹¬)",
        "ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ ì§ˆë¬¸"
    ]
}"""

    try:
        if messages and isinstance(messages, list):
            call_messages = [{"role": "system", "content": system_prompt}] + messages
        else:
            call_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query_text}
            ]
        
        response = solar_client.client.chat.completions.create(
            model=solar_client.model,
            messages=call_messages,
            temperature=0,
            response_format={"type": "json_object"}
        )
        
        raw = response.choices[0].message.content
        result = json.loads(raw)
        
        queries = result.get("queries", [])
        if not isinstance(queries, list):
            queries = []
        queries = [str(q).strip() for q in queries if str(q).strip()]
        
        if not queries:
            queries = [query_text]
            
        return queries[:3]
        
    except Exception as e:
        print(f"âš ï¸ Multi-Query ìƒì„± ì‹¤íŒ¨: {e}")
        return [query_text]

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 3. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
def answer_question_final(entry):
    eval_id = entry["eval_id"]
    messages = entry["msg"]
    
    res = {"eval_id": eval_id, "standalone_query": "", "topk": [], "answer": "", "references": []}
    
    # ì›ë³¸ ì§ˆë¬¸ ì¶”ì¶œ
    original_user_query = ""
    try:
        if isinstance(messages, list) and messages:
            original_user_query = messages[-1].get('content', '')
        else:
            original_user_query = str(messages)
    except:
        original_user_query = str(messages)
    
    res["standalone_query"] = original_user_query

    # 1) ê²Œì´íŒ… ì²´í¬
    if eval_id in EMPTY_IDS:
        # ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ ë‹µë³€ ìƒì„± (Solar)
        res["topk"] = []
        res["answer"] = solar_client.generate_answer(messages, "")
        return res
    
    # 2) ì¿¼ë¦¬ í™•ì¥ (HyDE + Multi-Query)
    # HyDE
    hyde_answer = solar_client.generate_hypothetical_answer(original_user_query)
    hyde_query = f"{original_user_query}\n{hyde_answer}" if hyde_answer else original_user_query
    
    # Multi-Query (3ê´€ì )
    multi_queries = generate_multi_query_3(original_user_query, messages)
    
    # 3) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Hard Voting)
    # - voting_weights=[5, 4, 2] (ê¸°ì¡´ ìµœê³ ì  ì„¤ì •)
    # - use_multi_embedding=True (SBERT + Gemini)
    # - multi_queries ì „ë‹¬ -> Sparse ê²€ìƒ‰ ë³´ê°•
    final_ranked_results = run_hybrid_search(
        original_query=original_user_query,
        sparse_query=hyde_query,
        reranker_query=original_user_query,
        voting_weights=[5, 4, 2],
        use_multi_embedding=True,
        top_k_retrieve=80,
        candidate_pool_size=80,
        use_gemini_only=False,
        use_rrf=False,  # Hard Voting ì‚¬ìš©
        multi_queries=multi_queries
    )
    
    res["topk"] = final_ranked_results[:5]
    
    # 4) ë‹µë³€ ìƒì„±
    context_docs = []
    for docid in res["topk"][:3]:
        try:
            search_result = es.search(
                index="test",
                query={"term": {"docid": docid}},
                size=1
            )
            if search_result['hits']['hits']:
                context_docs.append(search_result['hits']['hits'][0]['_source']['content'])
        except: pass
    
    context = " ".join(context_docs)
    res["answer"] = solar_client.generate_answer(messages, context)
    
    return res

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì‹¤í–‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë“œ
    eval_path = "/root/IR/data/eval.jsonl"
    eval_data = []
    with open(eval_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                eval_data.append(json.loads(line))
    
    print(f"ğŸ“Š í‰ê°€ ë°ì´í„°: {len(eval_data)}ê°œ")
    print("ğŸš€ ìµœì¢… ì „ëµ ì‹¤í–‰: S33ê²Œì´íŒ… + HardVoting + MultiQuery")
    
    results = []
    empty_count = 0
    
    for entry in tqdm(eval_data):
        result = answer_question_final(entry)
        if not result["topk"]:
            empty_count += 1
        results.append(result)
    
    # ì €ì¥
    output_path = "submission_final_strategy.csv"
    with open(output_path, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
            
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")
    print(f"ğŸ“Œ Empty topk: {empty_count}/{len(eval_data)} (ëª©í‘œ: 21ê°œ)")
