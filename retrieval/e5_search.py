import faiss
import numpy as np
import json
from pathlib import Path
from sentence_transformers import SentenceTransformer

# ì „ì—­ ë³€ìˆ˜ë¡œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
_model = None
_index = None
_doc_ids = None

CACHE_DIR = Path("/root/IR/cache")
MODEL_NAME = "intfloat/multilingual-e5-large"
INDEX_PATH = CACHE_DIR / "faiss_e5.index"
DOC_IDS_PATH = CACHE_DIR / "doc_ids_e5.json"

def _load_resources():
    global _model, _index, _doc_ids
    
    if _model is None:
        print(f"âš¡ Loading E5 Model: {MODEL_NAME}")
        _model = SentenceTransformer(MODEL_NAME)
    
    if _index is None:
        print(f"ğŸ“‚ Loading FAISS Index: {INDEX_PATH}")
        _index = faiss.read_index(str(INDEX_PATH))
        
    if _doc_ids is None:
        print(f"ğŸ“„ Loading Doc IDs: {DOC_IDS_PATH}")
        with open(DOC_IDS_PATH, "r", encoding="utf-8") as f:
            _doc_ids = json.load(f)

def search_e5(query: str, top_k: int = 5):
    """
    E5 ëª¨ë¸ + FAISS ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰
    """
    _load_resources()
    
    # E5 ì¿¼ë¦¬ ì ‘ë‘ì‚¬ ì¶”ê°€
    query_text = f"query: {query}"
    
    # ì„ë² ë”© ìƒì„±
    query_embedding = _model.encode([query_text])
    
    # FAISS ê²€ìƒ‰
    distances, indices = _index.search(query_embedding, top_k)
    
    results = []
    for i in range(top_k):
        idx = indices[0][i]
        score = float(distances[0][i])
        doc_id = _doc_ids[idx]
        results.append({"docid": doc_id, "score": score})
        
    return results

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    res = search_e5("ê³¼í•™ ê¸°ìˆ ì˜ ë°œì „", top_k=3)
    print(json.dumps(res, indent=2, ensure_ascii=False))
