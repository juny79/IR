import os
import json
import numpy as np
import faiss
import torch
from pathlib import Path
from tqdm import tqdm
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder
from dotenv import load_dotenv
from models.solar_client import SolarClient

# .env ë¡œë“œ
load_dotenv()

# ==========================================
# 1. ì„¤ì •  ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
OUTPUT_FILE = "/root/IR/submission_v9_sota.csv"

# ëª¨ë¸ ì„¤ì •
BGE_M3_MODEL = 'BAAI/bge-m3'
RERANK_MODEL = 'BAAI/bge-reranker-v2-m3'

# íŒŒë¼ë¯¸í„°
TOP_CANDIDATES = 200
FINAL_TOPK = 5
SOLAR_RERANK_TOPK = 10 # Solarê°€ ê²€í† í•  ìƒìœ„ í›„ë³´ ìˆ˜
ALPHA = 0.5 
RRF_K = 60

# 0.9348 ê¸°ì¤€ ìµœì í™”ëœ ê²Œì´íŒ… ID
EMPTY_IDS = {
    276, 261, 283, 32, 94, 90, 220, 245, 229, 
    247, 67, 57, 2, 227, 301, 222, 83, 64, 103, 218
}

def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            data.append(json.loads(line))
    return data

print("ðŸš€ ë°ì´í„° ë¡œë”© ì¤‘...")
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

# ==========================================
# 2. ëª¨ë¸ ë¡œë”©
# ==========================================
print(f"â³ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘...")
model = BGEM3FlagModel(BGE_M3_MODEL, use_fp16=True)

CACHE_DIR = "/root/IR/cache/bge_m3"
DENSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_dense_embs.npy")
SPARSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_sparse_embs.json")
FAISS_INDEX_PATH = os.path.join(CACHE_DIR, "bge_m3_dense.index")

print("âœ… ìºì‹œëœ BGE-M3 ì¸ë±ìŠ¤ ë¡œë“œ")
doc_dense_embs = np.load(DENSE_EMB_PATH)
with open(SPARSE_EMB_PATH, 'r') as f:
    doc_sparse_embs = json.load(f)
index = faiss.read_index(FAISS_INDEX_PATH)

print(f"â³ Reranker ë¡œë”© ì¤‘...")
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device="cuda")
solar_client = SolarClient(model_name="solar-pro")

# ==========================================
# 3. í•µì‹¬ í•¨ìˆ˜ë“¤
# ==========================================
def get_multi_queries(messages):
    system_prompt = """ë‹¹ì‹ ì€ ê³¼í•™ ê²€ìƒ‰ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ì—”ì§„ì— ìž…ë ¥í•  '3ê°€ì§€ ë²„ì „ì˜ ê²€ìƒ‰ì–´'ë¥¼ JSONìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
{
    "queries": [
        "êµ¬ ì™„ê²°ëœ ì„œìˆ í˜• ì§ˆë¬¸ (ê°€ìž¥ ì¤‘ìš”)",
        "í•µì‹¬ í‚¤ì›Œë“œ ë‚˜ì—´ (ëª…ì‚¬ ì¤‘)",
        "ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ ì§ˆë¬¸"
    ]
}"""
    try:
        resp = solar_client._call_with_retry(
            prompt=[{"role": "system", "content": system_prompt}] + messages,
            temperature=0,
            max_tokens=512,
            response_format={"type": "json_object"}
        )
        parsed = json.loads(resp)
        queries = parsed.get("queries", [])
        original_q = messages[-1]["content"]
        if original_q not in queries:
            queries.append(original_q)
        return queries[:3]
    except:
        return [messages[-1]["content"]]

def hybrid_search_multi(queries, top_k=100):
    all_results = []
    for q_text in queries:
        q_output = model.encode([q_text], return_dense=True, return_sparse=True, max_length=8192)
        q_dense = q_output['dense_vecs'][0].astype('float32')
        q_sparse = q_output['lexical_weights'][0]
        
        dense_scores, dense_indices = index.search(np.expand_dims(q_dense, 0), top_k)
        dense_indices = dense_indices[0]
        dense_scores = dense_scores[0]
        
        if len(dense_scores) > 0:
            d_min, d_max = dense_scores.min(), dense_scores.max()
            if d_max > d_min: dense_scores = (dense_scores - d_min) / (d_max - d_min)
            else: dense_scores = np.ones_like(dense_scores)
                
        sparse_scores = []
        for idx in dense_indices:
            score = model.compute_lexical_matching_score(q_sparse, doc_sparse_embs[idx])
            sparse_scores.append(score)
        sparse_scores = np.array(sparse_scores)
        
        if len(sparse_scores) > 0:
            s_min, s_max = sparse_scores.min(), sparse_scores.max()
            if s_max > s_min: sparse_scores = (sparse_scores - s_min) / (s_max - s_min)
            else: sparse_scores = np.ones_like(sparse_scores)
                
        hybrid_scores = ALPHA * dense_scores + (1 - ALPHA) * sparse_scores
        sorted_indices = np.argsort(hybrid_scores)[::-1]
        all_results.append([dense_indices[i] for i in sorted_indices])
        
    rrf_scores = {}
    for results in all_results:
        for rank, idx in enumerate(results):
            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (RRF_K + rank)
            
    final_indices = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
    return final_indices[:top_k]

def solar_rerank_topk(query, candidates):
    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ì •ë³´ ê²€ìƒ‰ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. 
 ì§ˆë¬¸(Query)ê³¼ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì„œ í›„ë³´(Candidate)ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
 ëŒ€    ê°€ìž¥ ì •í™•í•˜ê³ , ì§ì ‘ì ì¸ í•´ë‹µì„ í¬í•¨í•˜ê³  ìžˆëŠ” ë¬¸ì„œë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.

 JSON í˜•ì‹ìœ¼ë¡œ {"best_index": 0} ì™€ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”."""

    candidate_text = ""
    for i, content in enumerate(candidates):
        candidate_text += f"Candidate {i}:\n{content[:2000]}\n\n"
        
    user_prompt = f"## ì§ˆë¬¸:\n{query}\n\n## ê²€ìƒ‰ í›„ë³´:\n{candidate_text}"
    
    try:
        resp = solar_client._call_with_retry(
            prompt=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0,
            max_tokens=100,
            response_format={"type": "json_object"}
        )
        parsed = json.loads(resp)
        return int(parsed.get("best_index", 0))
    except:
        return 0

def generate_answer(query, context):
    system_prompt = "ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µí•˜ì§€ ë§ˆì„¸ìš”."
    user_prompt = f"ì§ˆë¬¸: {query}\n\në¬¸ë§¥:\n{context}"
    try:
        return solar_client._call_with_retry(
            prompt=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            temperature=0, max_tokens=1024
        )
    except: return "ë‹µ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ==========================================
# 4. ì‹¤í–‰ (Resume ê¸°ëŠ¥ ì¶”ê°€)
# ==========================================
processed_ids = set()
if os.path.exists(OUTPUT_FILE):
    with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                processed_ids.add(json.loads(line)["eval_id"])

with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
    for entry in tqdm(eval_data):
        eval_id = entry["eval_id"]
        if eval_id in processed_ids:
            continue
            
        messages = entry["msg"]
        
        if eval_id in EMPTY_IDS:
            f.write(json.dumps({"eval_id": eval_id, "topk": [], "answer": "ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ìž…ë‹ˆë‹¤."}, ensure_ascii=False) + "\n")
            f.flush()
            continue

        queries = get_multi_queries(messages)
        candidate_indices = hybrid_search_multi(queries, top_k=TOP_CANDIDATES)
        
        if candidate_indices:
            rerank_query = queries[0]
            pairs = [[rerank_query, doc_contents[idx]] for idx in candidate_indices]
            rerank_scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)
            sorted_ranks = sorted(zip(candidate_indices, rerank_scores), key=lambda x: x[1], reverse=True)
            
            top_indices = [idx for idx, _ in sorted_ranks[:SOLAR_RERANK_TOPK]]
            top_contents = [doc_contents[idx] for idx in top_indices]
            best_idx = solar_rerank_topk(rerank_query, top_contents)
            
            if best_idx >= len(top_indices) or best_idx < 0:
                best_idx = 0
                
            best_doc_idx = top_indices.pop(best_idx)
            final_indices = [best_doc_idx] + top_indices
            
            final_ids = [doc_ids[idx] for idx in final_indices[:FINAL_TOPK]]
            context = "\n".join([doc_contents[idx] for idx in final_indices[:3]])
            answer = generate_answer(rerank_query, context)
            
            res = {
                "eval_id": eval_id,
                "standalone_query": rerank_query,
                "topk": final_ids,
                "answer": answer
            }
        else:
            res = {"eval_id": eval_id, "topk": []}
                
        f.write(json.dumps(res, ensure_ascii=False) + "\n")
        f.flush()

print(f"âœ… SOTA ê°±ì‹   ì™„ë£Œ: {OUTPUT_FILE}")
