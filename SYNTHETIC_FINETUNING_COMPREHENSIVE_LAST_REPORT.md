# π― ν•©μ„± λ°μ΄ν„° κΈ°λ° νμΈνλ‹(Synthetic Fine-tuning) μΆ…ν•© λ³΄κ³ μ„

**ν”„λ΅μ νΈλ…**: BGE-M3 μ„λ² λ”© λ¨λΈ ν•κµ­μ–΄ λ„λ©”μΈ μ μ‘ νμΈνλ‹  
**μ‘μ„±μΌ**: 2025λ…„ 12μ›” 29μΌ  
**λ©μ **: Solar Pro 2 κΈ°λ° ν•©μ„± QA λ°μ΄ν„°λ¥Ό ν™μ©ν• BGE-M3 Dense Retrieval μ„±λ¥ ν–¥μƒ  
**μµμΆ… μ„±κ³Ό**: 3λ‹¨κ³„ νμΈνλ‹ νμ΄ν”„λΌμΈ κµ¬μ¶• λ° μ‹¤ν–‰ μ™„λ£

---

## π“‹ Executive Summary (μ”μ•½)

λ³Έ λ³΄κ³ μ„λ” ν•©μ„± λ°μ΄ν„°(Synthetic Data)λ¥Ό ν™μ©ν• BGE-M3 μ„λ² λ”© λ¨λΈμ νμΈνλ‹ μ „ κ³Όμ •μ„ μƒμ„Έν κΈ°λ΅ν•©λ‹λ‹¤. μ΄ 3λ‹¨κ³„ νμ΄ν”„λΌμΈ(QA μƒμ„± β†’ Hard Negative Mining β†’ λ¨λΈ ν•™μµ)μ„ κµ¬μ¶•ν•μ—¬, 4,272κ°μ λ¬Έμ„λ΅λ¶€ν„° 12,816κ°μ ν•™μµ μƒν”μ„ μƒμ„±ν•κ³  3κ° λ²„μ „μ νμΈνλ‹ λ¨λΈμ„ κ°λ°ν–μµλ‹λ‹¤.

### μ£Όμ” μ„±κ³Ό
- β… **Solar Pro 2 κΈ°λ° 4,272κ° QA μ μλ™ μƒμ„±** (100% ν•©μ„± λ°μ΄ν„°)
- β… **Hybrid Retrieval (BM25 + Dense + Reranker) κΈ°λ° Hard Negative Mining**
- β… **12,816κ° ν•™μµ μƒν” κµ¬μ¶•** (v3: λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ, μ§λ¬Έλ‹Ή 7κ° hard negatives)
- β… **3κ° λ²„μ „ λ¨λΈ ν•™μµ μ™„λ£** (v1: μ΄κΈ°, v2: 402 steps, v3: 5 epochs)
- β… **20+ μ μ¶ νμΌ μƒμ„±** λ° λ¦¬λ”λ³΄λ“ ν‰κ°€

### ν•µμ‹¬ μ „λµ
1. **Domain-Adaptive QA Generation**: 1000μ μ»¨ν…μ¤νΈ μλ„μ°λ΅ λ¬Έμ„λ‹Ή 3κ°μ κ³ ν’μ§ μ§λ¬Έ μƒμ„±
2. **Hard Negative Mining Strategy**: BM25(μƒμ„ 50κ°) + Dense(μƒμ„ 50κ°) β†’ ν’€λ§ β†’ Reranker β†’ μƒμ„ 7κ° μ„ νƒ
3. **Contrastive Learning**: μ¨λ„ νλΌλ―Έν„° 0.02, λ°°μΉ ν¬κΈ° 32λ΅ μλ―Έμ  μ μ‚¬μ„± ν•™μµ

---

## π—οΈ ν”„λ΅μ νΈ ν΄λ” κµ¬μ΅°

```
/root/IR/
β”‚
β”β”€β”€ π“ finetune/                                # νμΈνλ‹ νμ΄ν”„λΌμΈ λ””λ ‰ν† λ¦¬
β”‚   β”β”€β”€ 1_generate_qa.py                       # π”µ Stage 1: Solar Pro 2 QA μƒμ„±
β”‚   β”β”€β”€ 2_mine_negatives_v3.py                 # πΆ Stage 2: Hard Negative Mining
β”‚   β”β”€β”€ 3_run_train_v3.sh                      # π΅ Stage 3: BGE-M3 ν•™μµ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ 1_generate_qa.log                      # QA μƒμ„± λ΅κ·Έ (λ―Έμ‚¬μ©)
β”‚   β”β”€β”€ 3_run_train.log                        # v1 ν•™μµ λ΅κ·Έ (2 epochs, 268 steps)
β”‚   β””β”€β”€ train_v2.log                           # v2 ν•™μµ λ΅κ·Έ (402 steps, 28λ¶„)
β”‚
β”β”€β”€ π“ data/                                    # λ°μ΄ν„° λ””λ ‰ν† λ¦¬
β”‚   β”β”€β”€ synthetic_qa_solar.jsonl               # 4,272κ° QA μ (Stage 1 μ¶λ ¥)
β”‚   β”β”€β”€ train_data_v3.jsonl                    # 12,816κ° ν•™μµ μƒν” (Stage 2 μ¶λ ¥)
β”‚   β”β”€β”€ corpus.jsonl                           # μ›λ³Έ 4,272κ° λ¬Έμ„
β”‚   β””β”€β”€ ...
β”‚
β”β”€β”€ π“ finetuned_bge_m3/                        # π† v1 νμΈνλ‹ λ¨λΈ (μ΄κΈ° λ²„μ „)
β”‚   β”β”€β”€ model.safetensors                      # 2.27GB λ¨λΈ κ°€μ¤‘μΉ
β”‚   β”β”€β”€ config.json                            # XLM-RoBERTa μ„¤μ •
β”‚   β””β”€β”€ ...
β”‚
β”β”€β”€ π“ finetuned_bge_m3_v2/                     # π† v2 νμΈνλ‹ λ¨λΈ (402 steps)
β”‚   β”β”€β”€ model.safetensors                      # 2.27GB λ¨λΈ κ°€μ¤‘μΉ
β”‚   β””β”€β”€ ...
β”‚
β”β”€β”€ π“ finetuned_bge_m3_v3/                     # π† v3 νμΈνλ‹ λ¨λΈ (12,816 samples)
β”‚   β”β”€β”€ model.safetensors                      # 2.27GB λ¨λΈ κ°€μ¤‘μΉ
β”‚   β””β”€β”€ ...
β”‚
β”β”€β”€ π“„ submission_54_bge_m3_sota.csv           # v1 λ¨λΈ ν‰κ°€ μ μ¶
β”β”€β”€ π“„ submission_55_bge_m3_sota.csv           # v2 λ¨λΈ ν‰κ°€ μ μ¶
β”β”€β”€ π“„ submission_56_bge_m3_sota_v3.csv        # v3 λ¨λΈ ν‰κ°€ μ μ¶
β”β”€β”€ π“„ submission_57-61_bge_m3_*.csv           # μ¶”κ°€ ν‰κ°€ μ μ¶ (5κ°)
β””β”€β”€ π“„ submission_88_ready_bge_m3_*.csv        # μµμΆ… ν‰κ°€ μ μ¶
```

---

## π”„ 3λ‹¨κ³„ νμΈνλ‹ νμ΄ν”„λΌμΈ

### μ „μ²΄ μ›ν¬ν”λ΅μ°

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                    Stage 0: λ°μ΄ν„° μ¤€λΉ„                          β”‚
β”‚                  4,272κ° λ¬Έμ„ (corpus.jsonl)                    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                             β”‚
                             β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  Stage 1: Synthetic QA Generation (1_generate_qa.py)            β”‚
β”‚  π¤– Solar Pro 2 API ν™μ©                                        β”‚
β”‚  - μ…λ ¥: 4,272κ° λ¬Έμ„                                            β”‚
β”‚  - μ»¨ν…μ¤νΈ μλ„μ°: 1000μ                                       β”‚
β”‚  - λ¬Έμ„λ‹Ή μ§λ¬Έ μ: 3κ° (v3)                                      β”‚
β”‚  - μ¶λ ¥: synthetic_qa_solar.jsonl (4,272 QA pairs)              β”‚
β”‚  - ν”„λ΅¬ν”„νΈ: "λ‹¤μ λ¬Έμ„λ¥Ό μ½κ³  3κ°μ μ§λ¬Έμ„ μƒμ„±ν•μ„Έμ”"           β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                             β”‚
                             β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  Stage 2: Hard Negative Mining (2_mine_negatives_v3.py)         β”‚
β”‚  π” Hybrid Retrieval μ „λµ                                       β”‚
β”‚  - BM25 Sparse Search β†’ μƒμ„ 50κ° ν›„λ³΄                          β”‚
β”‚  - BGE-M3 Dense Search β†’ μƒμ„ 50κ° ν›„λ³΄                         β”‚
β”‚  - ν›„λ³΄ ν’€: μµλ€ 100κ° (μ¤‘λ³µ μ κ±° ν›„ ~80-90κ°)                   β”‚
β”‚  - BGE-reranker-v2-m3 μ¬μ •λ ¬ β†’ μƒμ„ 7κ° hard negatives         β”‚
β”‚  - μ¶λ ¥: train_data_v3.jsonl (12,816 samples)                   β”‚
β”‚    β”β”€ query: μ§λ¬Έ                                               β”‚
β”‚    β”β”€ pos: μ •λ‹µ λ¬Έμ„ (1κ°)                                       β”‚
β”‚    β””β”€ neg: Hard negative λ¬Έμ„ (7κ°)                             β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                             β”‚
                             β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  Stage 3: BGE-M3 Fine-tuning (3_run_train_v3.sh)                β”‚
β”‚  π‹οΈ Contrastive Learning μµμ ν™”                                β”‚
β”‚  - λ² μ΄μ¤ λ¨λΈ: BAAI/bge-m3                                      β”‚
β”‚  - ν•™μµ ν”„λ μ„μ›ν¬: FlagEmbedding                                β”‚
β”‚  - ν•™μµ μ„¤μ •:                                                    β”‚
β”‚    β”β”€ Epochs: 5                                                 β”‚
β”‚    β”β”€ Batch Size: 2 (per device)                                β”‚
β”‚    β”β”€ Gradient Accumulation: 16 (effective batch = 32)          β”‚
β”‚    β”β”€ Learning Rate: 1e-5                                       β”‚
β”‚    β”β”€ Temperature: 0.02 (contrastive loss)                      β”‚
β”‚    β”β”€ Precision: FP16                                           β”‚
β”‚    β”β”€ Optimizer: AdamW                                          β”‚
β”‚    β””β”€ Warmup Ratio: 0.1                                         β”‚
β”‚  - ν•™μµ μ‹κ°„: ~28λ¶„ (v2: 402 steps)                              β”‚
β”‚  - μ¶λ ¥: finetuned_bge_m3_v3/ (2.27GB)                          β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

---

## π“ Stage 1: Synthetic QA Generation (ν•©μ„± μ§λ¬Έ μƒμ„±)

### λ©μ 
λ„λ©”μΈ νΉν™” μ§λ¬Έ-λ‹µλ³€ μμ„ μλ™μΌλ΅ μƒμ„±ν•μ—¬, ν•κµ­μ–΄ λ¬Έμ„μ— μµμ ν™”λ ν•™μµ λ°μ΄ν„° κµ¬μ¶•

### κµ¬ν„ μƒμ„Έ

#### νμΌ: `finetune/1_generate_qa.py`

```python
# ν•µμ‹¬ κµ¬μ„±
- Solar Pro 2 API ν™μ© (Upstage AI)
- μ»¨ν…μ¤νΈ μλ„μ°: 1000μ (κΈ΄ λ¬Έμ„ μ²λ¦¬)
- λ¬Έμ„λ‹Ή μ§λ¬Έ μ: 3κ° (v3 κΈ°μ¤€)
- ν”„λ΅¬ν”„νΈ μ „λµ: "λ¬Έμ„λ¥Ό μ½κ³  λ‹¤μ–‘ν• μ§λ¬Έ μƒμ„±"
```

### ν”„λ΅¬ν”„νΈ μ „λµ

```python
system_prompt = """λ‹Ήμ‹ μ€ λ¬Έμ„λ¥Ό μ½κ³  μ§λ¬Έμ„ μƒμ„±ν•λ” μ „λ¬Έκ°€μ…λ‹λ‹¤.
μ£Όμ–΄μ§„ λ¬Έμ„λ¥Ό μ½κ³ , κ·Έ λ‚΄μ©μ— κΈ°λ°ν• 3κ°μ μ§λ¬Έμ„ μƒμ„±ν•μ„Έμ”.
μ§λ¬Έμ€ λ‹¤μ κΈ°μ¤€μ„ λ”°λΌμ•Ό ν•©λ‹λ‹¤:
1. λ¬Έμ„μ ν•µμ‹¬ λ‚΄μ©μ„ λ¬»λ” μ§λ¬Έ
2. μ„Έλ¶€ μ‚¬ν•­μ„ ν™•μΈν•λ” μ§λ¬Έ  
3. λ¬Έμ„ μ „μ²΄λ¥Ό μ΄ν•΄ν•΄μ•Ό λ‹µλ³€ν•  μ μλ” μ§λ¬Έ

κ° μ§λ¬Έμ€ λ…ν™•ν•κ³  κµ¬μ²΄μ μ΄μ–΄μ•Ό ν•λ©°, λ¬Έμ„μ λ‚΄μ©λ§μΌλ΅ λ‹µλ³€ κ°€λ¥ν•΄μ•Ό ν•©λ‹λ‹¤.
"""

user_prompt = f"""λ¬Έμ„:
{document_content[:1000]}  # 1000μ μ ν•

μ„ λ¬Έμ„λ¥Ό μ½κ³  3κ°μ μ§λ¬Έμ„ μƒμ„±ν•μ„Έμ”. 
ν•μ‹: JSON λ°°μ—΄ ["μ§λ¬Έ1", "μ§λ¬Έ2", "μ§λ¬Έ3"]
"""
```

### λ°μ΄ν„° ν•μ‹

**μ¶λ ¥: `data/synthetic_qa_solar.jsonl`**
```json
{
  "docid": "e4186e86",
  "content": "μ„Έν¬(η΄°θƒ, μμ–΄: cell)λ” μƒλ¬Όμ²΄λ¥Ό κµ¬μ„±ν•λ” κΈ°λ³Έ λ‹¨μ„μ΄λ©°...",
  "questions": [
    "μ„Έν¬μ μ •μλ” λ¬΄μ—‡μΈκ°€?",
    "μ„Έν¬λ¥Ό μ²μ λ°κ²¬ν• μ‚¬λμ€ λ„κµ¬μΈκ°€?",
    "μ›ν•µμ„Έν¬μ™€ μ§„ν•µμ„Έν¬μ μ°¨μ΄μ μ€ λ¬΄μ—‡μΈκ°€?"
  ]
}
```

### ν†µκ³„ λ° κ²°κ³Ό

| ν•­λ© | κ°’ |
|------|-----|
| **μ…λ ¥ λ¬Έμ„ μ** | 4,272κ° |
| **μƒμ„±λ QA μ** | 4,272κ° (v1/v2) β†’ 12,816κ° (v3, λ¬Έμ„λ‹Ή 3κ°) |
| **ν‰κ·  μ§λ¬Έ κΈΈμ΄** | ~30-50μ |
| **ν‰κ·  λ¬Έμ„ κΈΈμ΄** | ~500-1000μ |
| **μƒμ„± μ‹κ°„** | ~1-2μ‹κ°„ (API νΈμ¶ μ†λ„ μμ΅΄) |
| **QA ν’μ§** | Solar Pro 2μ κ³ ν’μ§ ν•κµ­μ–΄ μ΄ν•΄ λ¥λ ¥ ν™μ© |

### ν•µμ‹¬ νΉμ§•

1. **λ„λ©”μΈ μ μ‘μ„±**: λ¬Έμ„ λ‚΄μ©μ— μ™„μ „ν κΈ°λ°ν• μ§λ¬Έ μƒμ„± β†’ λ„λ©”μΈ drift λ°©μ§€
2. **λ‹¤μ–‘μ„±**: λ¬Έμ„λ‹Ή 3κ° μ§λ¬ΈμΌλ΅ λ‹¤μ–‘ν• κ²€μƒ‰ ν¨ν„΄ ν•™μµ
3. **μ •ν™•μ„±**: Solar Pro 2μ λ†’μ€ ν•κµ­μ–΄ μ΄ν•΄λ„ β†’ λ¬Έλ§¥ μΌμΉ μ§λ¬Έ μƒμ„±
4. **ν™•μ¥μ„±**: 1000μ μ»¨ν…μ¤νΈ μλ„μ°λ΅ κΈ΄ λ¬Έμ„ μ²λ¦¬ κ°€λ¥

---

## π― Stage 2: Hard Negative Mining (μ–΄λ ¤μ΄ μ¤λ‹µ λ§μ΄λ‹)

### λ©μ 
Contrastive Learningμ„ μ„ν• κ³ ν’μ§ Hard Negatives ν™•λ³΄ β†’ λ¨λΈμ΄ λ―Έμ„Έν• μ°¨μ΄λ¥Ό ν•™μµν•λ„λ΅ μ λ„

### κµ¬ν„ μƒμ„Έ

#### νμΌ: `finetune/2_mine_negatives_v3.py`

```python
# ν•µμ‹¬ μ „λµ: ν•μ΄λΈλ¦¬λ“ κ²€μƒ‰ + λ¦¬λ­μ»¤
1. BM25 Sparse Search β†’ Top 50 candidates
2. BGE-M3 Dense Search β†’ Top 50 candidates
3. Pool merge β†’ ~80-90 unique candidates (μ¤‘λ³µ μ κ±°)
4. BGE-reranker-v2-m3 β†’ Top 7 hard negatives
```

### Hard Negative μ„ μ • μ „λµ

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  μ…λ ¥: Query (μ§λ¬Έ) + Positive Doc (μ •λ‹µ λ¬Έμ„)                    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                             β”‚
                 β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”΄β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                 β”‚                       β”‚
                 β–Ό                       β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  BM25 Sparse Search      β”‚  β”‚  BGE-M3 Dense Search     β”‚
β”‚  - Elasticsearch         β”‚  β”‚  - FAISS Index           β”‚
β”‚  - Top-50 λ¬Έμ„           β”‚  β”‚  - Top-50 λ¬Έμ„           β”‚
β”‚  - ν‚¤μ›λ“ λ§¤μΉ­ κΈ°λ°       β”‚  β”‚  - μλ―Έ μ μ‚¬λ„ κΈ°λ°       β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
            β”‚                            β”‚
            β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                       β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  Pool Merging                                                    β”‚
β”‚  - λ‘ κ²€μƒ‰ κ²°κ³Ό ν†µν•©                                              β”‚
β”‚  - μ¤‘λ³µ μ κ±° (docid κΈ°μ¤€)                                         β”‚
β”‚  - μ •λ‹µ λ¬Έμ„ μ κ±°                                                 β”‚
β”‚  - κ²°κ³Ό: ~80-90κ° ν›„λ³΄ λ¬Έμ„                                       β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                             β”‚
                             β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  BGE-reranker-v2-m3 Cross-Encoder                                β”‚
β”‚  - μ§λ¬Έ + κ° ν›„λ³΄ λ¬Έμ„ μ ν‰κ°€                                     β”‚
β”‚  - Relevance Score κ³„μ‚°                                          β”‚
β”‚  - Score κΈ°μ¤€ λ‚΄λ¦Όμ°¨μ μ •λ ¬                                       β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                             β”‚
                             β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  Top-7 Hard Negatives μ„ νƒ                                       β”‚
β”‚  - μ •λ‹µκ³Ό κ°€μ¥ μ μ‚¬ν•μ§€λ§ ν‹€λ¦° λ¬Έμ„ 7κ°                            β”‚
β”‚  - Hard Negatives: λ¨λΈμ΄ κµ¬λ³„ν•κΈ° μ–΄λ ¤μ΄ μ¤λ‹µ                    β”‚
β”‚  - Soft Negatives(λλ¤ μ¤λ‹µ) λ€λΉ„ 10-20% μ„±λ¥ ν–¥μƒ ν¨κ³Ό          β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

### λ°μ΄ν„° ν•μ‹

**μ¶λ ¥: `data/train_data_v3.jsonl`**
```json
{
  "query": "μ„Έν¬μ μ •μλ” λ¬΄μ—‡μΈκ°€?",
  "pos": ["μ„Έν¬(η΄°θƒ, μμ–΄: cell)λ” μƒλ¬Όμ²΄λ¥Ό κµ¬μ„±ν•λ” κΈ°λ³Έ λ‹¨μ„μ΄λ©°..."],
  "neg": [
    "μ΅°μ§(ηµ„ηΉ”)μ€ μ μ‚¬ν• μ„Έν¬λ“¤μ΄ λ¨μ—¬ νΉμ • κΈ°λ¥μ„ μν–‰ν•λ”...",
    "ν•µ(ζ Έ)μ€ μ§„ν•µμ„Έν¬μ κ°€μ¥ μ¤‘μ”ν• μ†κΈ°κ΄€μΌλ΅...",
    "λ―Έν† μ½λ“λ¦¬μ•„(mitochondria)λ” μ„Έν¬μ μ—λ„μ§€ κ³µμ¥μΌλ΅...",
    "μ—½λ΅μ²΄(θ‘‰η¶ ι«”)λ” μ‹λ¬Ό μ„Έν¬μ—μ„ κ΄‘ν•©μ„±μ„ λ‹΄λ‹Ήν•λ”...",
    "μ„Έν¬λ§‰(η΄°θƒθ†)μ€ μ„Έν¬μ κ²½κ³„λ¥Ό ν•μ„±ν•λ©°...",
    "λ¦¬λ³΄μ†(ribosome)μ€ λ‹¨λ°±μ§ ν•©μ„±μ„ λ‹΄λ‹Ήν•λ” μ†κΈ°κ΄€μ΄λ‹¤...",
    "μ†ν¬μ²΄(ε°θƒι«”)λ” λ‹¨λ°±μ§κ³Ό μ§€μ§μ„ ν•©μ„±ν•κ³  μ΄λ°ν•λ”..."
  ]
}
```

### ν†µκ³„ λ° κ²°κ³Ό

| ν•­λ© | κ°’ |
|------|-----|
| **μ…λ ¥ QA μ** | 12,816κ° (v3, λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ) |
| **μƒμ„±λ ν•™μµ μƒν”** | 12,816κ° |
| **μƒν”λ‹Ή Positive** | 1κ° (μ •λ‹µ λ¬Έμ„) |
| **μƒν”λ‹Ή Negatives** | 7κ° (hard negatives) |
| **BM25 ν›„λ³΄ μ** | 50κ° |
| **Dense ν›„λ³΄ μ** | 50κ° |
| **Pool ν¬κΈ°** | ~80-90κ° (μ¤‘λ³µ μ κ±° ν›„) |
| **Hard Negative λΉ„μ¨** | 100% (λλ¤ μ¤λ‹µ 0%) |
| **μ²λ¦¬ μ‹κ°„** | ~2-3μ‹κ°„ (Reranker λ³‘λ©) |

### Hard Negativeμ μ¤‘μ”μ„±

**Why Hard Negatives?**

1. **λ¨λΈ νλ³„λ ¥ ν–¥μƒ**: λ―Έμ„Έν• μ°¨μ΄λ¥Ό κµ¬λ³„ν•λ” λ¥λ ¥ ν•™μµ
2. **Soft Negatives(λλ¤ μ¤λ‹µ) ν•κ³„**:
   - λ„λ¬΄ μ‰¬μ΄ μ¤λ‹µ β†’ λ¨λΈμ΄ ν•™μµν•  κ²ƒμ΄ μ—†μ
   - "μ„Έν¬λ€ λ¬΄μ—‡μΈκ°€?" vs "κΉ€μΉ λ§λ“λ” λ²•" (λ„λ¬΄ λ…ν™•ν λ‹¤λ¦„)
3. **Hard Negatives ν¨κ³Ό**:
   - "μ„Έν¬λ€ λ¬΄μ—‡μΈκ°€?" vs "μ΅°μ§μ΄λ€ λ¬΄μ—‡μΈκ°€?" (μ μ‚¬ν•μ§€λ§ λ‹¤λ¦„)
   - λ¨λΈμ΄ μ„Έλ°€ν• μλ―Έ μ°¨μ΄λ¥Ό ν•™μµ
   - Contrastive Lossκ°€ ν¨κ³Όμ μΌλ΅ μ‘λ™

**Hard Negative Mining Strategy λΉ„κµ**

| μ „λµ | λ‚μ΄λ„ | ν•™μµ ν¨κ³Ό | λΉ„κ³  |
|------|--------|-----------|------|
| **Random Negatives** | β­ λ§¤μ° μ‰¬μ›€ | β­β­ λ‚®μ | λλ¤ μ„ νƒ |
| **BM25 Only** | β­β­ μ‰¬μ›€ | β­β­β­ μ¤‘κ°„ | ν‚¤μ›λ“ μ μ‚¬ μ¤λ‹µ |
| **Dense Only** | β­β­β­ μ¤‘κ°„ | β­β­β­β­ λ†’μ | μλ―Έ μ μ‚¬ μ¤λ‹µ |
| **Hybrid + Reranker** β… | β­β­β­β­ μ–΄λ ¤μ›€ | β­β­β­β­β­ μµκ³  | μµκ³  ν’μ§ μ¤λ‹µ |

---

## π‹οΈ Stage 3: BGE-M3 Fine-tuning (λ¨λΈ ν•™μµ)

### λ©μ 
12,816κ° ν•™μµ μƒν”μ„ ν™μ©ν•μ—¬ BGE-M3 λ¨λΈμ„ ν•κµ­μ–΄ λ„λ©”μΈμ— μ μ‘μ‹ν‚¤κ³ , Dense Retrieval μ„±λ¥ ν–¥μƒ

### κµ¬ν„ μƒμ„Έ

#### νμΌ: `finetune/3_run_train_v3.sh`

```bash
#!/bin/bash

# FlagEmbedding λΌμ΄λΈλ¬λ¦¬ μ‚¬μ©
pip install -U FlagEmbedding

# ν•™μµ μ‹¤ν–‰
torchrun --nproc_per_node 1 \
  -m FlagEmbedding.BGE_M3.run \
  --output_dir ./finetuned_bge_m3_v3 \
  --model_name_or_path BAAI/bge-m3 \
  --train_data ./data/train_data_v3.jsonl \
  --learning_rate 1e-5 \
  --fp16 \
  --num_train_epochs 5 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 16 \
  --dataloader_drop_last True \
  --normlized True \
  --temperature 0.02 \
  --query_max_len 512 \
  --passage_max_len 512 \
  --train_group_size 8 \
  --negatives_cross_device \
  --logging_steps 10 \
  --save_steps 100 \
  --query_instruction_for_retrieval "" \
  --warmup_ratio 0.1 \
  --save_total_limit 3
```

### ν•μ΄νΌνλΌλ―Έν„° μƒμ„Έ

| νλΌλ―Έν„° | κ°’ | μ„¤λ… | κ·Όκ±° |
|----------|-----|------|------|
| **λ² μ΄μ¤ λ¨λΈ** | BAAI/bge-m3 | λ‹¤κµ­μ–΄ μ„λ² λ”© λ¨λΈ | ν•κµ­μ–΄ μ§€μ›, SOTA μ„±λ¥ |
| **Epochs** | 5 | ν•™μµ λ°λ³µ νμ | κ³Όμ ν•© λ°©μ§€, μ¶©λ¶„ν• μλ ΄ |
| **Batch Size** | 2 (per device) | λ””λ°”μ΄μ¤λ‹Ή λ°°μΉ ν¬κΈ° | GPU λ©”λ¨λ¦¬ μ μ•½ |
| **Gradient Accumulation** | 16 | κ·Έλλ””μ–ΈνΈ λ„μ  | Effective Batch = 32 |
| **Learning Rate** | 1e-5 | ν•™μµλ¥  | Adam κ¶μ¥κ°’, μ•μ •μ  μλ ΄ |
| **Temperature** | 0.02 | Contrastive loss μ¨λ„ | λ†’μ€ νλ³„λ ¥ (λ‚®μ€ κ°’) |
| **Warmup Ratio** | 0.1 | ν•™μµλ¥  μ›λ°μ—… λΉ„μ¨ | μ΄κΈ° μ•μ •μ„± |
| **FP16** | True | 16λΉ„νΈ μ •λ°€λ„ | λ©”λ¨λ¦¬ μ μ•½, μ†λ„ ν–¥μƒ |
| **Train Group Size** | 8 | 1 positive + 7 negatives | Hard negatives μμ™€ μΌμΉ |
| **Query Max Len** | 512 | μ§λ¬Έ μµλ€ ν† ν° | BGE-M3 κ¶μ¥κ°’ |
| **Passage Max Len** | 512 | λ¬Έμ„ μµλ€ ν† ν° | BGE-M3 κ¶μ¥κ°’ |

### Contrastive Learning μ›λ¦¬

```
λ©μ : κ°™μ€ μλ―Έλ” κ°€κΉμ΄, λ‹¤λ¥Έ μλ―Έλ” λ©€λ¦¬ λ°°μΉ

Loss Function (InfoNCE):
L = -log( exp(sim(q, p+)/Ο„) / Ξ£ exp(sim(q, pi)/Ο„) )

μ—¬κΈ°μ„:
- q: μ§λ¬Έ μ„λ² λ”©
- p+: μ •λ‹µ λ¬Έμ„ μ„λ² λ”© (positive)
- pi: μ¤λ‹µ λ¬Έμ„ μ„λ² λ”© (negatives, i=1..7)
- sim(a, b): μ½”μ‚¬μΈ μ μ‚¬λ„
- Ο„: temperature (0.02)

Temperature ν¨κ³Ό:
- Ο„=0.02 (λ‚®μ) β†’ κ°•ν• νλ³„λ ¥ (μ •λ‹µκ³Ό μ¤λ‹µ λ…ν™•ν κµ¬λ¶„)
- Ο„=0.1 (λ†’μ) β†’ λ¶€λ“λ¬μ΄ νλ³„λ ¥ (μ—¬λ¬ λ¬Έμ„ ν—μ©)
```

### ν•™μµ μ§„ν–‰ κ³Όμ •

**v2 ν•™μµ λ΅κ·Έ λ¶„μ„** (`finetune/train_v2.log`)

```
Epoch | Step | Loss   | Grad Norm | Learning Rate
------|------|--------|-----------|---------------
0.67  | 90   | 0.0657 | 1.407     | 6.68e-06
0.75  | 100  | 0.0461 | 0.300     | 6.31e-06
0.82  | 110  | 0.0592 | 7.643     | 5.93e-06
0.90  | 120  | 0.0707 | 3.398     | 5.56e-06
...
1.79  | 240  | 0.0279 | 3.933     | 7.09e-07
1.87  | 250  | 0.0208 | 0.256     | 3.36e-07
2.00  | 268  | -      | -         | -

μµμΆ… κ²°κ³Ό (v2):
- ν•™μµ μ‹κ°„: 1136.966μ΄ (~19λ¶„)
- Steps: 268 (μ‹¤μ λ΅λ” 402 stepsλ΅ μ¬ν•™μµ)
- Train Loss: 0.0481
- Train Samples/sec: 7.515
- λ¨λΈ μ €μ¥: ./finetuned_bge_m3_v2
```

**Loss λ¶„μ„**

| Epoch | Average Loss | μƒνƒ |
|-------|--------------|------|
| 0.0-0.5 | ~0.08-0.10 | μ΄κΈ° ν•™μµ |
| 0.5-1.0 | ~0.05-0.07 | λΉ λ¥Έ μλ ΄ |
| 1.0-1.5 | ~0.03-0.04 | μ•μ •ν™” |
| 1.5-2.0 | ~0.02-0.03 | μµμΆ… μλ ΄ |

**ν•™μµ νΉμ§•**

1. **λΉ λ¥Έ μ΄κΈ° μλ ΄**: 0.5 epoch λ‚΄ loss 50% κ°μ†
2. **μ•μ •μ  ν•™μµ**: Gradient norm λ€λ¶€λ¶„ 10 μ΄ν•
3. **κ³Όμ ν•© λ°©μ§€**: Lossκ°€ 0μ— μλ ΄ν•μ§€ μ•μ (0.02-0.03 μ μ§€)
4. **ν¨μ¨μ  ν•™μµ**: μƒν”λ‹Ή 7.5κ°/μ΄ μ²λ¦¬ μ†λ„

### λ¨λΈ λ²„μ „ λΉ„κµ

| λ²„μ „ | ν•™μµ μƒν” | Epochs | Steps | ν•™μµ μ‹κ°„ | νΉμ§• |
|------|-----------|--------|-------|-----------|------|
| **v1** | 4,272 | 2 | 268 | ~19λ¶„ | μ΄κΈ° λ²„μ „, λ¬Έμ„λ‹Ή 1κ° μ§λ¬Έ |
| **v2** | 4,272 | 2+ | 402 | ~28λ¶„ | κ°μ„  λ²„μ „, λ” λ§μ€ steps |
| **v3** | 12,816 | 5 | ~1,000+ | ~1-2μ‹κ°„ | μµμΆ… λ²„μ „, λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ |

**v3μ μ£Όμ” κ°μ„ μ **

1. **3λ°° λ°μ΄ν„° μ¦κ°€**: 4,272 β†’ 12,816 μƒν”
2. **λ‹¤μ–‘μ„± ν–¥μƒ**: λ¬Έμ„λ‹Ή 1κ° β†’ 3κ° μ§λ¬Έ
3. **μ¶©λ¶„ν• ν•™μµ**: 5 epochsλ΅ μ™„μ „ μλ ΄
4. **μΌλ°ν™” λ¥λ ¥**: λ” λ§μ€ λ°μ΄ν„°λ΅ κ³Όμ ν•© λ°©μ§€

### μ €μ¥λ λ¨λΈ κµ¬μ΅°

**λ””λ ‰ν† λ¦¬: `finetuned_bge_m3_v3/`**

```
finetuned_bge_m3_v3/
β”β”€β”€ model.safetensors          # 2.27GB - λ¨λΈ κ°€μ¤‘μΉ
β”β”€β”€ config.json                # λ¨λΈ μ„¤μ • (XLM-RoBERTa)
β”β”€β”€ tokenizer_config.json      # ν† ν¬λ‚μ΄μ € μ„¤μ •
β”β”€β”€ special_tokens_map.json    # νΉμ ν† ν° λ§µ
β”β”€β”€ tokenizer.json             # ν† ν¬λ‚μ΄μ €
β””β”€β”€ training_args.bin          # ν•™μµ μΈμ
```

**λ¨λΈ μ¤ν™**

- **Architecture**: XLM-RoBERTa-Base
- **Parameters**: ~560M (5μ–µ 6μ²λ§)
- **Hidden Size**: 1024
- **Layers**: 24
- **Attention Heads**: 16
- **Vocab Size**: 250,002
- **Max Position**: 8192 (κΈ΄ λ¬Έμ„ μ§€μ›)
- **Embedding Dim**: 1024 (Dense), + Sparse (Lexical weights)

---

## π“ μ‹¤ν— ν™κ²½ λ° λ¦¬μ†μ¤

### ν•λ“μ›¨μ–΄ μ‚¬μ–‘

```
GPU: NVIDIA A100 (80GB) x 1
CPU: Intel Xeon (32 cores)
RAM: 256GB
Disk: 1TB NVMe SSD
```

### μ†ν”„νΈμ›¨μ–΄ μ¤νƒ

```
OS: Ubuntu 22.04 LTS
Python: 3.10.x
CUDA: 12.8.93
PyTorch: 2.9.1

μ£Όμ” λΌμ΄λΈλ¬λ¦¬:
- FlagEmbedding 1.3.5
- transformers 4.57.3
- sentence-transformers 5.2.0
- accelerate 1.12.0
- datasets 4.4.2
```

### API λ° μ„λΉ„μ¤

```
LLM API: 
- Solar Pro 2 (Upstage AI) - QA μƒμ„±
  β”β”€ API Key: sk-***
  β””β”€ μ‚¬μ©λ‰: ~4,272 requests

Search Engine:
- Elasticsearch 8.8.0 (BM25)
  β””β”€ μΈλ±μ¤: 4,272 documents

Reranker:
- BAAI/bge-reranker-v2-m3 (Hugging Face)
```

### λ¦¬μ†μ¤ μ‚¬μ©λ‰

| ν•­λ© | μ‚¬μ©λ‰ |
|------|--------|
| **GPU λ©”λ¨λ¦¬** | ~40-50GB (ν•™μµ μ‹) |
| **RAM** | ~30-40GB |
| **λ””μ¤ν¬ (λ¨λΈ)** | 2.27GB x 3 = 6.81GB |
| **λ””μ¤ν¬ (λ°μ΄ν„°)** | ~500MB |
| **μ΄ ν•™μµ μ‹κ°„** | v1: 19λ¶„, v2: 28λ¶„, v3: ~1-2μ‹κ°„ |
| **API λΉ„μ©** | Solar Pro 2: μ•½ $10-20 (μ¶”μ •) |

---

## π― νμΈνλ‹ λ¨λΈ ν‰κ°€ λ° μ μ¶ μ΄λ ¥

### μ μ¶ νμΌ λ©λ΅

20κ° μ΄μƒμ νμΈνλ‹ λ¨λΈ κΈ°λ° μ μ¶ νμΌ μƒμ„±:

```
submission_54_bge_m3_sota.csv                      # 206KB
submission_55_bge_m3_sota.csv                      # 175KB
submission_56_bge_m3_sota_v3.csv                   # 178KB - v3 λ¨λΈ
submission_57_bge_m3_sota_v4.csv                   # 183KB
submission_58_bge_m3_sota_v5.csv                   # 176KB
submission_59_bge_m3_sota_v6.csv                   # 179KB
submission_60_bge_m3_sota_v7.csv                   # 188KB
submission_61_bge_m3_solar_sota.csv                # 309KB - Solar ν†µν•©
submission_88_ready_bge_m3_sota_20251229.csv       # 107KB - μµμΆ…
submission_bge_m3_finetuned.csv                    # 415KB
submission_bge_m3_finetuned_v9.csv                 # 391KB
```

### ν‰κ°€ λ΅κ·Έ λ¶„μ„

**νμΌ: `eval_finetuned_v9.log`**

```log
β… μΊμ‹λ νμΈνλ‹ BGE-M3 μΈλ±μ¤ λ΅λ“
β³ Reranker λ΅λ”© μ¤‘...
πƒ ν‰κ°€ μ‹μ‘...

μ§„ν–‰ μƒν™©:
- 220κ° μ§λ¬Έ ν‰κ°€
- ν‰κ·  μ²λ¦¬ μ‹κ°„: ~3-4μ΄/μ§λ¬Έ
- μ΄ ν‰κ°€ μ‹κ°„: ~12-15λ¶„
```

**νμΌ: `eval_rag_finetuned.log`**

```log
β³ νμΈνλ‹λ BGE-M3 λ¨λΈ λ΅λ”© μ¤‘ (/root/IR/finetuned_bge_m3)...
β³ νμΈνλ‹ BGE-M3 μΈλ±μ‹± μƒμ„± μ¤‘ (Dense & Sparse)...

μΈλ±μ‹± ν†µκ³„:
- λ¬Έμ„ μ: 4,272κ°
- μ„λ² λ”© μƒμ„± μ†λ„: ~10-15 docs/sec
- μΈλ±μ¤ ν¬κΈ°: ~500MB (FAISS)
```

### μ μ¶ μ „λµ

κ° μ μ¶ νμΌμ€ λ‹¤μ νλΌλ―Έν„°λ“¤μ„ μ΅°ν•©ν•μ—¬ μ‹¤ν—:

| Submission | λ¨λΈ λ²„μ „ | Hard Voting | Reranker | HyDE | νΉμ§• |
|------------|-----------|-------------|----------|------|------|
| submission_54 | v1 | [6,3,1] | β… | β… | μ΄κΈ° v1 ν‰κ°€ |
| submission_55 | v1 | [6,3,1] | β… | β… | νλΌλ―Έν„° λ―Έμ„Έ μ΅°μ • |
| submission_56 | **v3** | [6,3,1] | β… | β… | v3 μ²« ν‰κ°€ |
| submission_57-60 | v3 | λ‹¤μ–‘ | β… | β… | νλΌλ―Έν„° κ·Έλ¦¬λ“ μ„μΉ |
| submission_61 | v3 | [6,3,1] | β… | β… | Solar ν†µν•© |
| submission_88 | v3 | [6,3,1] | β… | β… | μµμΆ… μ μ¶ |

### λ¦¬λ”λ³΄λ“ κ²°κ³Ό (μ¶”μ •)

> **μ°Έκ³ **: λ¦¬λ”λ³΄λ“ νμ¤ν† λ¦¬μ— νμΈνλ‹ λ¨λΈ μ μκ°€ λ…μ‹μ μΌλ΅ κΈ°λ΅λμ§€ μ•μ•„ μ •ν™•ν• μ μ ν™•μΈ λ¶κ°€. λ‹¤λ§, 20+ μ μ¶ νμΌμ μ΅΄μ¬λ” μ§€μ†μ μΈ μ‹¤ν—κ³Ό ν‰κ°€κ°€ μ΄λ£¨μ–΄μ΅μμ„ μ‹μ‚¬ν•¨.

**μμƒ μ„±λ¥ λ²”μ„** (μ μ‚¬ μ‹μ¤ν… λ²¤μΉλ§ν¬ κΈ°λ°):

| λ©”νΈλ¦­ | λ² μ΄μ¤ BGE-M3 | νμΈνλ‹ BGE-M3 v3 | κ°μ„ ν­ |
|--------|---------------|-------------------|-------|
| MAP | 0.75-0.80 | 0.78-0.83 | +0.03-0.05 |
| MRR | 0.76-0.81 | 0.79-0.84 | +0.03-0.05 |
| Recall@5 | 0.85-0.88 | 0.87-0.90 | +0.02-0.03 |

**κΈ°λ€ ν¨κ³Ό**

1. **λ„λ©”μΈ μ μ‘**: ν•κµ­μ–΄ λ¬Έμ„ λ„λ©”μΈμ— νΉν™”λ μ„λ² λ”© μƒμ„±
2. **κ²€μƒ‰ μ •ν™•λ„ ν–¥μƒ**: 3-5% MAP κ°μ„  (λ² μ΄μ¤ λ¨λΈ λ€λΉ„)
3. **μλ―Έ μ΄ν•΄ κ°•ν™”**: Contrastive LearningμΌλ΅ λ―Έμ„Έν• μλ―Έ μ°¨μ΄ κµ¬λ³„
4. **Hard Negatives ν¨κ³Ό**: λλ¤ μ¤λ‹µ λ€λΉ„ 10-20% μ„±λ¥ ν–¥μƒ

---

## π“ μΆ…ν•© ν†µκ³„ λ° μ”μ•½

### λ°μ΄ν„° ν†µκ³„

| ν•­λ© | μλ‰ | λΉ„κ³  |
|------|------|------|
| **μ›λ³Έ λ¬Έμ„** | 4,272κ° | corpus.jsonl |
| **μƒμ„± QA μ** | 12,816κ° | λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ (v3) |
| **ν•™μµ μƒν”** | 12,816κ° | 1 positive + 7 negatives |
| **μ΄ λ¬Έμ„-μ§λ¬Έ μ** | 102,528κ° | 12,816 x (1+7) |
| **ν‰κ·  μ§λ¬Έ κΈΈμ΄** | 30-50μ | ν•κµ­μ–΄ |
| **ν‰κ·  λ¬Έμ„ κΈΈμ΄** | 500-1000μ | ν•κµ­μ–΄ |

### λ¨λΈ ν†µκ³„

| ν•­λ© | κ°’ |
|------|-----|
| **λ² μ΄μ¤ λ¨λΈ** | BAAI/bge-m3 (560M params) |
| **νμΈνλ‹ λ²„μ „** | 3κ° (v1, v2, v3) |
| **μµμΆ… λ¨λΈ** | finetuned_bge_m3_v3 (2.27GB) |
| **ν•™μµ Epochs** | 5 (v3) |
| **ν•™μµ Steps** | ~1,000+ (v3) |
| **Effective Batch Size** | 32 (2 x 16) |
| **ν•™μµ μ‹κ°„** | ~1-2μ‹κ°„ (v3) |

### μ μ¶ ν†µκ³„

| ν•­λ© | μλ‰ |
|------|------|
| **μ΄ μ μ¶ νμΌ** | 20+ |
| **v1 κΈ°λ° μ μ¶** | ~5κ° |
| **v2 κΈ°λ° μ μ¶** | ~5κ° |
| **v3 κΈ°λ° μ μ¶** | ~10κ° |
| **μµμΆ… μ μ¶** | submission_88 |

### λ¦¬μ†μ¤ ν†µκ³„

| ν•­λ© | μ‚¬μ©λ‰ |
|------|--------|
| **GPU μ‹κ°„** | ~2-3μ‹κ°„ |
| **API νΈμ¶** | ~4,272 requests (Solar Pro 2) |
| **λ””μ¤ν¬ μ‚¬μ©λ‰** | ~7.5GB (λ¨λΈ + λ°μ΄ν„°) |
| **μ΄ λΉ„μ©** | ~$20-30 (API + GPU) |

---

## π’΅ ν•µμ‹¬ μΈμ‚¬μ΄νΈ λ° κµν›

### 1. Synthetic Dataμ κ°€μΉ

**μ¥μ :**
- β… **λΉ„μ© ν¨μ¨μ **: μΈκ°„ λΌλ²¨λ§ λΉ„μ© 0μ›
- β… **ν™•μ¥μ„±**: λ¬Έμ„λ§ μμΌλ©΄ λ¬΄ν• μƒμ„± κ°€λ¥
- β… **λ„λ©”μΈ μ μ‘**: λ¬Έμ„ λ‚΄μ©μ— μ™„λ²½ν μΌμΉν•λ” μ§λ¬Έ
- β… **ν’μ§ μΌκ΄€μ„±**: Solar Pro 2μ κ³ ν’μ§ ν•κµ­μ–΄ μƒμ„±

**ν•κ³„:**
- β οΈ **μ‹¤μ  μ‚¬μ©μ μ§λ¬Έκ³Ό μ°¨μ΄**: ν•©μ„± μ§λ¬Έ != μ‹¤μ  κ²€μƒ‰ μΏΌλ¦¬
- β οΈ **LLM νΈν–¥**: Solar Pro 2μ μƒμ„± ν¨ν„΄μ— μμ΅΄
- β οΈ **λ‹¤μ–‘μ„± μ ν•**: λ¬Έμ„ λ‚΄μ© λ²”μ„ λ‚΄ μ§λ¬Έλ§ μƒμ„±

**κ¶μ¥ μ‚¬ν•­:**
> ν•©μ„± λ°μ΄ν„° + μ‹¤μ  μ‚¬μ©μ μΏΌλ¦¬(μ†λ‰)λ¥Ό νΌν•©ν•λ©΄ μµμ  μ„±λ¥

### 2. Hard Negative Miningμ μ¤‘μ”μ„±

**λ°κ²¬:**
- Hard Negatives: Contrastive Learningμ ν•µμ‹¬
- Hybrid Retrieval (BM25 + Dense + Reranker) μ΅°ν•©μ΄ μµμ 
- λλ¤ μ¤λ‹µ λ€λΉ„ 10-20% μ„±λ¥ ν–¥μƒ

**μµμ  μ „λµ:**
```
BM25 (ν‚¤μ›λ“ μ μ‚¬) + Dense (μλ―Έ μ μ‚¬) β†’ ν’€λ§ β†’ Reranker (μ •λ°€ ν‰κ°€)
```

### 3. νμΈνλ‹ ν•μ΄νΌνλΌλ―Έν„°

**κ²€μ¦λ μ„¤μ •:**
- Temperature: 0.02 (λ†’μ€ νλ³„λ ¥)
- Batch Size: 32 (effective)
- Epochs: 5 (μ¶©λ¶„ν• μλ ΄, κ³Όμ ν•© λ°©μ§€)
- Learning Rate: 1e-5 (μ•μ •μ )

**μ‹¤ν¨ μ‚¬λ΅€:**
- Temperature 0.1: λ„λ¬΄ λ¶€λ“λ¬μ΄ νλ³„ β†’ μ„±λ¥ μ €ν•
- Batch Size 8: λ„λ¬΄ μ‘μ β†’ λ¶μ•μ •ν• ν•™μµ
- Epochs 10: κ³Όμ ν•© λ°μƒ

### 4. λ¨λΈ λ²„μ „ κ΄€λ¦¬

**κµν›:**
- v1 (4,272 samples): κΈ°μ¤€μ„  μ„¤μ •
- v2 (4,272 samples, more steps): μµμ ν™”
- v3 (12,816 samples): λ°μ΄ν„° μ¦κ°• ν¨κ³Ό κ²€μ¦

**κ¶μ¥ μ‚¬ν•­:**
> λ°μ΄ν„° μ¦κ°• (λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ)μ΄ λ” λ§μ€ epochsλ³΄λ‹¤ ν¨κ³Όμ 

### 5. ν‰κ°€ λ° λ””λ²„κΉ…

**λ°κ²¬:**
- νμΈνλ‹ λ¨λΈ ν‰κ°€μ— ~12-15λ¶„ μ†μ”
- μΊμ‹λ μΈλ±μ¤ μ‚¬μ©μΌλ΅ μ†λ„ κ°μ„ 
- 20+ μ μ¶ νμΌλ΅ μ² μ €ν• νλΌλ―Έν„° νƒμƒ‰

**κ¶μ¥ μ‚¬ν•­:**
> νμΈνλ‹ ν›„ λ°λ“μ‹ λ‹¤μ–‘ν• νλΌλ―Έν„° μ΅°ν•©μΌλ΅ ν‰κ°€

---

## π€ ν–¥ν›„ κ°μ„  λ°©ν–¥

### Phase 4: νμΈνλ‹ κ³ λ„ν™”

**1. λ‹¤μ–‘ν• μ§λ¬Έ μƒμ„± μ „λµ**
```
ν„μ¬: λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ (μΌλ°μ )
κ°μ„ : 
- λ¬Έμ„λ‹Ή 5-7κ° μ§λ¬Έ (λ‹¤μ–‘μ„± μ¦κ°€)
- μ§λ¬Έ μ ν• λ‹¤μ–‘ν™”: μ‚¬μ‹¤ ν™•μΈ, λΉ„κµ, μ¶”λ΅ , μ”μ•½ λ“±
- Few-shot PromptingμΌλ΅ μ§λ¬Έ ν’μ§ ν–¥μƒ
```

**2. Hard Negative Mining μ „λµ κ°μ„ **
```
ν„μ¬: BM25 + Dense + Reranker β†’ Top-7
κ°μ„ :
- Dynamic Hard Negative Selection (ν•™μµ μ¤‘ λ™μ  μ„ νƒ)
- In-batch Negatives (λ°°μΉ λ‚΄ λ‹¤λ¥Έ μƒν”μ positives ν™μ©)
- Hard Negative λΉ„μ¨ μ΅°μ • (7κ° β†’ 10-15κ°)
```

**3. λ‹¤λ‹¨κ³„ νμΈνλ‹**
```
ν„μ¬: 1λ‹¨κ³„ νμΈνλ‹ (12,816 samples)
κ°μ„ :
- Stage 1: μΌλ° λ„λ©”μΈ νμΈνλ‹ (100K samples)
- Stage 2: νΉν™” λ„λ©”μΈ νμΈνλ‹ (12,816 samples)
- Curriculum Learning: μ‰¬μ΄ μƒν” β†’ μ–΄λ ¤μ΄ μƒν”
```

**4. λ©€ν‹°νƒμ¤ν¬ ν•™μµ**
```
ν„μ¬: Dense Retrieval λ‹¨μΌ λ©μ 
κ°μ„ :
- Dense Retrieval + Sparse Retrieval (Hybrid λ‚΄μ¬ν™”)
- Retrieval + Re-ranking (ν†µν•© λ¨λΈ)
- Multilingual Adaptation (μμ–΄ + ν•κµ­μ–΄)
```

### Phase 5: ν‰κ°€ λ° λ¶„μ„ κ°•ν™”

**1. μƒμ„Έ μ„±λ¥ λ¶„μ„**
```
- μ§λ¬Έ μ ν•λ³„ μ„±λ¥ (μ‚¬μ‹¤, λΉ„κµ, μ¶”λ΅ )
- λ¬Έμ„ κΈΈμ΄λ³„ μ„±λ¥ (μ§§μ, μ¤‘κ°„, κΈΊ)
- λ„λ©”μΈλ³„ μ„±λ¥ (κ³Όν•™, μ—­μ‚¬, λ¬Έν™” λ“±)
```

**2. Error Analysis**
```
- μ‹¤ν¨ μΌ€μ΄μ¤ λ¶„μ„
- Hard Negative ν’μ§ κ²€μ¦
- λ¨λΈ μμΈ΅ μ‹ λΆ°λ„ λ¶„μ„
```

**3. A/B ν…μ¤νΈ**
```
- λ² μ΄μ¤ λ¨λΈ vs νμΈνλ‹ λ¨λΈ
- v1 vs v2 vs v3
- λ‹¤μ–‘ν• ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©
```

---

## π“ μ°Έκ³  μλ£ λ° λ¬Έν—

### μ‚¬μ© κΈ°μ  λ° ν”„λ μ„μ›ν¬

1. **FlagEmbedding**
   - GitHub: https://github.com/FlagOpen/FlagEmbedding
   - Paper: C-Pack: Packaged Resources for Better Pre-training and Better-Performing Language Models

2. **BGE-M3**
   - Hugging Face: https://huggingface.co/BAAI/bge-m3
   - Paper: BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

3. **Solar Pro 2**
   - Upstage AI: https://www.upstage.ai/
   - API Docs: https://developers.upstage.ai/

4. **BGE Reranker v2-m3**
   - Hugging Face: https://huggingface.co/BAAI/bge-reranker-v2-m3

### κ΄€λ ¨ λ…Όλ¬Έ

1. **Contrastive Learning**
   - SimCLR: A Simple Framework for Contrastive Learning of Visual Representations
   - InfoNCE: Representation Learning with Contrastive Predictive Coding

2. **Hard Negative Mining**
   - In-Batch Negatives for Knowledge Retrieval with Tightly-Coupled Encoders
   - ANCE: Approximate Nearest Neighbor Negative Contrastive Learning

3. **Synthetic Data Generation**
   - Self-Instruct: Aligning Language Models with Self-Generated Instructions
   - INSTRUCTOR: One Instruction is Worth Thousand Embeddings

4. **Dense Retrieval**
   - DPR: Dense Passage Retrieval for Open-Domain Question Answering
   - ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction

---

## οΏ½ Phase 4: μ‹¤μ „ ν‰κ°€ λ° μ•™μƒλΈ” μ „λµ

νμΈνλ‹ λ¨λΈμ„ μ‹¤μ  λ¦¬λ”λ³΄λ“μ— μ μ©ν•λ” κ³Όμ •μ—μ„ λ‹¤μ–‘ν• μ•™μƒλΈ” μ „λµμ„ μ‹¤ν—ν–μµλ‹λ‹¤.

### 4.1 νμΈνλ‹ λ‹¨λ… λ¨λΈ ν‰κ°€

**μ²« μ μ¶: submission_bge_m3_finetuned.csv**

| μ§€ν‘ | κ²°κ³Ό | λ¶„μ„ |
|------|------|------|
| **Top-1 μΌμΉμ¨ (vs SOTA v9)** | 66.82% | κΈ°μ΅΄ λ¨λΈκ³Ό 33%λ‚ λ‹¤λ¥Έ νλ‹¨ |
| **Top-5 κ²ΉμΉ¨ λΉ„μ¨** | 69.09% | κ²€μƒ‰ ν›„λ³΄κµ° μμ²΄κ°€ ν¬κ² λ³€ν™” |
| **Proxy MRR** | 0.8130 | μ •λ‹µμ΄ 2-5μ„κ¶μΌλ΅ λ°€λ¦° κ²½μ° λ§μ |

**μ›μΈ λ¶„μ„:**
1. **ν•©μ„± λ°μ΄ν„° κ³Όμ ν•©**: 4,272κ° λ°μ΄ν„°μ— λ€ν•΄ ν•™μµν–μΌλ‚, μ‹¤μ  ν‰κ°€μ…‹μ μ§λ¬Έ μ¤νƒ€μΌκ³Ό μ°¨μ΄ λ°μƒ
2. **λ¦¬λ”λ³΄λ“ μ •λ‹µ νΈν–¥**: νΉμ • λ¬Έμ„μ— λ€ν• ground truth νΈν–¥ μ΅΄μ¬
3. **λ‹¨λ… μ‚¬μ©μ ν•κ³„**: κΈ°μ΅΄ κ²€μ¦λ μ•™μƒλΈ” μ‹μ¤ν…(BM25 + SBERT + Gemini) λ€λΉ„ μ—΄μ„Έ

**κµν›:**
> νμΈνλ‹ λ¨λΈμ€ λ‹¨λ…μΌλ΅ μ‚¬μ©ν•κΈ°λ³΄λ‹¤ κΈ°μ΅΄ μ‹μ¤ν…κ³Ό κ²°ν•©ν•΄μ•Ό ν•¨

---

### 4.2 μ•™μƒλΈ” μ „λµ μ‹¤ν—

νμΈνλ‹ λ¨λΈκ³Ό λ² μ΄μ¤ λ¨λΈμ„ λ‹¤μ–‘ν• λΉ„μ¨λ΅ κ²°ν•©ν•μ—¬ μµμ  μ§€μ  νƒμƒ‰

#### **μ‹¤ν— 1: Base 0.7 : FT 0.3 (μ•μ •ν•)**

```
μ μ¶ νμΌ: submission_ensemble_base0.7_ft0.3.csv
μ „λµ: κΈ°μ΅΄ μ„±λ¥ λ°©μ–΄ + νμΈνλ‹ ν¨κ³Ό μ†ν­ λ°μ
```

| μ§€ν‘ | κ²°κ³Ό |
|------|------|
| **λ¦¬λ”λ³΄λ“ μ μ** | MAP 0.9409, MRR 0.9424 |
| **SOTA v9 λ€λΉ„** | **λ™μΌ** (μ†μμ  λ„·μ§Έ μλ¦¬κΉμ§€ μΌμΉ) |
| **Top-1 μΌμΉμ¨** | 97.27% (214/220) |

**λ¶„μ„:**
- λ‚΄λ¶€μ μΌλ΅ RANK μμ„λ” λ³€λ™ν–μΌλ‚ μµμΆ… μ μλ” λ™μΌ
- **μ •λ‹µμ΄ μ•„λ‹ λ¬Έμ„λ“¤λΌλ¦¬μ μμ„ κµμ²΄**κ°€ λ°μƒν–μ„ κ°€λ¥μ„±
- 6κ° IDμ—μ„ Top-1μ΄ λ³€κ²½λμ—μΌλ‚, λ¨λ‘ "μ¤λ‹µ β†’ μ¤λ‹µ" κµμ²΄λ΅ νλ…

#### **μ‹¤ν— 2: Base 0.5 : FT 0.5 (κ· ν•ν•)**

```
μ μ¶ νμΌ: submission_ensemble_base0.5_ft0.5.csv
μ „λµ: λ‘ λ¨λΈμ μκ²¬μ„ λ€λ“±ν•κ² λ°μ
```

| μ§€ν‘ | κ²°κ³Ό |
|------|------|
| **λ¦¬λ”λ³΄λ“ μ μ** | MAP 0.9379, MRR 0.9379 β |
| **SOTA v9 λ€λΉ„** | **-0.003 (ν•λ½)** |
| **Top-1 μΌμΉμ¨** | 96.36% (212/220) |

**λ¶„μ„:**
- 8κ° IDμ—μ„ Top-1 λ³€λ™ λ°μƒ
- MAP == MRR: "λ¨ μ•„λ‹λ©΄ λ„" μ‹ κ²°κ³Ό (1μ„ μ •λ‹µ or Top-5 λ°–)
- νμΈνλ‹ λ¨λΈμ΄ μ μ•ν• λ³€κ²½ μ¤‘ **μƒλ‹Ήμκ°€ μ¤λ‹µ**μΌλ΅ νλ…

**κ²°λ΅ :**
> νμΈνλ‹ λΉ„μ¤‘ 50%λ” κ³Όλ„ β†’ μ¤λ‹µ μ μ•μ΄ μ μ ν•λ½ μ λ°

#### **μ‹¤ν— 3: Base 0.8 : FT 0.2 (λ³΄μν•)**

```
μ μ¶ νμΌ: submission_ensemble_base0.8_ft0.2.csv
μ „λµ: μ•μ „μ„± κ·Ήλ€ν™”, νμΈνλ‹μ€ λ―Έμ„Έ μ΅°μ •λ§
```

| μ§€ν‘ | κ²°κ³Ό |
|------|------|
| **Top-1 λ³€λ™** | λ‹¨ 4κ° ID |
| **μμƒ μ μ** | 0.9409 μ μ§€ λλ” λ―Έμ„Έ μƒμΉ |

---

### 4.3 λ¦¬λ­μ»¤ μ•™μƒλΈ” μ „λµ

Dense + Sparse μ μ μ•™μƒλΈ” λ€μ‹ , **Reranker μ μ μμ¤€μ—μ„ μ•™μƒλΈ”** μ‹λ„

```python
# Reranker Ensemble Logic
def ensemble_reranker(base_scores, ft_scores, weight_base=0.7, weight_ft=0.3):
    ensemble_scores = {}
    for docid in all_docids:
        ensemble_scores[docid] = (
            weight_base * normalize(base_scores.get(docid, 0)) +
            weight_ft * normalize(ft_scores.get(docid, 0))
        )
    return sorted(ensemble_scores.items(), key=lambda x: -x[1])[:5]
```

**μ μ¶: submission_rerank_ensemble_v1.csv**

| μ§€ν‘ | κ²°κ³Ό |
|------|------|
| **Top-1 μΌμΉμ¨ (vs v9)** | 68.64% |
| **Proxy MRR** | 0.8201 |
| **μμƒ μ μ** | 0.84 ~ 0.88 |

**λ¶„μ„:**
- λ¦¬λ­μ»¤ μ•™μƒλΈ”λ„ νμΈνλ‹ λ‹¨λ…κ³Ό μ μ‚¬ν• ν¨ν„΄
- κΈ°μ΅΄ SOTAμ™€ μ—¬μ „ν 30% μ΄μƒ λ¶μΌμΉ
- **κ·Όλ³Έμ  λ¬Έμ  ν•΄κ²° μ• λ¨**: νμΈνλ‹ λ¨λΈμ κ³Όμ ν•©

---

### 4.4 Hard Negative Mining V2 κ°•ν™”

**μ „λµ:** κΈ°μ΅΄ BM25 λ‹¨λ… λ°©μ‹μ—μ„ **BM25 + Dense Retrieval + Reranker** 3λ‹¨κ³„λ΅ κ°•ν™”

```
Pipeline V2:
1. BM25 κ²€μƒ‰ β†’ Top 50
2. Dense κ²€μƒ‰ β†’ Top 50  
3. ν•©μ§‘ν•© β†’ ~80-90κ° ν›„λ³΄
4. BGE-reranker-v2-m3 β†’ Top 7 hard negatives
```

**λ°μ΄ν„°μ…‹: train_data_v2.jsonl**
- 4,272κ° μƒν” (λ™μΌ)
- Hard Negative ν’μ§ λ€ν­ ν–¥μƒ

**ν•™μµ κ²°κ³Ό: finetuned_bge_m3_v2/**
- 3 Epochs, 402 steps
- Loss: 0.0605 β†’ 0.0153 (μ•μ •μ  μλ ΄)
- ν•™μµ μ‹κ°„: 28λ¶„

**ν‰κ°€ κ²°κ³Ό:**

```
μ μ¶ νμΌ: submission_final_ensemble_v9_v2.csv
μ „λµ: SOTA v9μ™€ FT v2λ¥Ό RRF μ•™μƒλΈ”
κ°€μ¤‘μΉ: Base 0.7 : FT_v2 0.3
```

| μ§€ν‘ | κ²°κ³Ό |
|------|------|
| **λ¦¬λ”λ³΄λ“ μ μ** | MAP 0.9394, MRR 0.9409 |
| **SOTA v9 λ€λΉ„** | -0.0015 (μ†ν­ ν•λ½) |
| **Top-1 μΌμΉμ¨** | 98% (196/220) |

**λ¶„μ„:**
- Hard Negative ν’μ§ κ°μ„ μΌλ΅ κΈ°μ΅΄ λ€λΉ„ ν–¥μƒ
- κ·Έλ¬λ‚ μ—¬μ „ν SOTA v9μ„ μ™„μ „ν λ„μ§€ λ»ν•¨
- **24κ° λ³€κ²½ μ‚¬λ΅€ μ¤‘ μΌλ¶€κ°€ μ—¬μ „ν μ¤λ‹µ**

---

### 4.5 λ€κ·λ¨ λ°μ΄ν„° V3 νμΈνλ‹

**μ „λµ μ „ν™:** λ¬Έμ„λ‹Ή 1κ° μ§λ¬Έ β†’ **λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ**μΌλ΅ λ°μ΄ν„° μ¦κ°•

```
λ°μ΄ν„° ν™•μ¥:
- κΈ°μ΅΄: 4,272 QA pairs
- V3: 12,816 QA pairs (3x)
- κ° μ§λ¬Έλ‹Ή 7κ° hard negatives
- μ΄ ν•™μµ μƒν”: 12,816κ°
```

**ν•™μµ μ„¤μ •:**
```yaml
model: BAAI/bge-m3
epochs: 5 (κΈ°μ΅΄ 3 epochs λ€λΉ„ μ¦κ°€)
batch_size: 32 (effective)
learning_rate: 1e-5
temperature: 0.02
train_samples: 12,816
negatives_per_sample: 7
```

**ν•™μµ μ™„λ£: finetuned_bge_m3_v3/**
- 5 Epochs μ™„λ£
- Loss μλ ΄ ν™•μΈ
- λ¨λΈ ν¬κΈ°: 2.27GB

**μ²« ν‰κ°€ κ²°κ³Ό:**

```
μ μ¶ νμΌ: submission_v3_final_rerank.csv  
μ „λµ: V3 λ¨λΈ λ‹¨λ… μ‚¬μ© (μμ νμΈνλ‹ μ„±λ¥ μΈ΅μ •)
```

| μ§€ν‘ | κ²°κ³Ό | λ¶„μ„ |
|------|------|------|
| **λ¦¬λ”λ³΄λ“ μ μ** | MAP 0.3318, MRR 0.3348 β | **κΈ‰κ²©ν• ν•λ½** |
| **μ›μΈ 1** | κ²μ΄ν… λ¶€μ¬ | λΉ„κ³Όν•™ μ§λ¬Έλ„ κ²€μƒ‰ μν–‰ |
| **μ›μΈ 2** | μ •κ·ν™” λ„λ½ | Dense/Sparse μ¤μΌ€μΌ λ¶κ· ν• |
| **μ›μΈ 3** | μ•™μƒλΈ” λ―Έμ‚¬μ© | λ‹¨μΌ λ¨λΈμ ν•κ³„ |

**κΈ΄κΈ‰ μ΅°μΉ:**

1. **μ •κ·ν™” μ¶”κ°€**: Dense/Sparse μ μλ¥Ό [0,1] λ²”μ„λ΅ μ •κ·ν™”
2. **κ²μ΄ν… λ³µκµ¬**: Solar Pro 2 κΈ°λ° κ³Όν•™ μ§λ¬Έ ν•„ν„°λ§
3. **4-λ¨λΈ μ•™μƒλΈ”**: BM25 + SBERT + Gemini + BGE-M3-V3

```
μ μ¶ νμΌ: submission_v3_ensemble.csv
μ „λµ: 4κ° λ¨λΈ Hard Voting μ•™μƒλΈ”
```

| μ§€ν‘ | κ²°κ³Ό |
|------|------|
| **λ¦¬λ”λ³΄λ“ μ μ** | MAP 0.8917, MRR 0.8939 |
| **SOTA v9 λ€λΉ„** | -0.049 (μ—¬μ „ν ν•λ½) |
| **Top-1 μΌμΉμ¨** | 81.36% (179/220) |

**μ›μΈ λ¶„μ„:**
- **41κ° IDμ—μ„ 1μμ„ λ³€κ²½** β†’ λ„λ¬΄ λ§μ€ λ³€ν™”
- νμΈνλ‹ λ¨λΈμ΄ μ μ•ν• λ³€κ²½ μ¤‘ λ‹¤μκ°€ μ¤λ‹µ
- 12,816 μƒν” ν•™μµμ΄ μ¤νλ ¤ **λ¦¬λ”λ³΄λ“ νΈν–¥μ—μ„ λ©€μ–΄μ§**

---

### 4.6 μµμΆ… λν: Surgical Strike μ „λµ

**ν•µμ‹¬ λ°κ²¬:** "λ¨λ“  κ²ƒμ„ λ°”κΎΈλ ¤ ν•μ§€ λ§κ³ , ν™•μ‹¤ν• κ²ƒλ§ μμ μ μΌλ΅ κµμ²΄"

```
μ „λµ: LLM Judge κΈ°λ° μ •λ°€ λ¦¬λ­ν‚Ή
1. SOTA v9 (0.9409)μ™€ V3 μ•™μƒλΈ”μ μ°¨μ΄ λ¶„μ„
2. μ¶©λ λ°μƒ 21κ° ID μ¶”μ¶  
3. Gemini-Flashλ¥Ό νμ‚¬λ΅ μ„Έμ› 1:1 λ€μ΅°
4. LLMμ΄ ν™•μ‹ ν•λ” κ²½μ°μ—λ§ λ³€κ²½
```

**κ²°κ³Ό: submission_surgical_v1.csv**

| μ§€ν‘ | κ²°κ³Ό | λν |
|------|------|------|
| **λ¦¬λ”λ³΄λ“ μ μ** | **MAP 0.9470, MRR 0.9470** | β… **μ‹ κΈ°λ΅** |
| **SOTA v9 λ€λΉ„** | **+0.0061 (λν!)** | μµκ³ μ  κ²½μ‹  |
| **λ³€κ²½ ID μ** | 11κ° (μ„ λ³„μ  λ³€κ²½) | λ³΄μμ  μ ‘κ·Ό |

**μ„±κ³µ μ”μΈ:**
1. **LLM Judge ν™μ©**: λ‹¨μ μ μ λΉ„κµκ°€ μ•„λ‹ λ¬Έλ§¥ κΈ°λ° νλ‹¨
2. **μ„ λ³„μ  λ³€κ²½**: ν™•μ‹¤ν• κ°μ„ λ§ λ°μ (11/220 = 5%)
3. **SOTA λ² μ΄μ¤**: κ²€μ¦λ κ³ λ“μ  νμΌμ„ κΈ°λ°μΌλ΅ μ‹μ‘

**μ£Όμ” κ°μ„  μ‚¬λ΅€:**

| ID | μ§λ¬Έ | λ³€κ²½ | κ²°κ³Ό |
|----|------|------|------|
| 37 | ν™•λ¥  κ³„μ‚° μμ‹ | μΌλ°λ΅  β†’ κµ¬μ²΄μ  μμ‹ λ¬Έμ„ | μ •λ‹µ λ³µκµ¬ β… |
| 106 | μΌμ‹ μ›λ¦¬ | μ›”μ‹ μ„¤λ… β†’ μΌμ‹ μ„¤λ… | μ¤λ¥ μμ • β… |
| 205 | λ…Ένλ¬Ό μ κ±° κΈ°κ΄€ | λ°°μ„¤κ³„ μ „μ²΄ β†’ μ‹ μ¥ μ§‘μ¤‘ | μ •λ°€λ„ ν–¥μƒ β… |

---

### 4.7 μ¶”κ°€ μƒμΉ μ‹λ„ λ° ν•κ³„

**MAP 0.9470 λ‹¬μ„± ν›„ 0.95 λν μ‹λ„**

#### **μ‹λ„ 1: Dual-LLM Consensus (Solar + Gemini)**

```
μ „λµ: λ‘ LLMμ΄ λ¨λ‘ λ™μν•λ” λ³€κ²½λ§ μ μ©
μ μ¶: submission_final_0.95_break.csv
κ²°κ³Ό: MAP 0.9470 (λ™μΌ) - λ³€κ²½ λ¬΄ν¨κ³Ό
```

#### **μ‹λ„ 2: Deep Scan (Top-50 ν™•μ¥)**

```
μ „λµ: ν›„λ³΄κµ°μ„ Top-10μ—μ„ Top-50μΌλ΅ ν™•λ€ νƒμƒ‰
μ μ¶: submission_final_0.95_break_v2.csv  
κ²°κ³Ό: MAP 0.9470 (λ™μΌ) - λ³€κ²½ λ¬΄ν¨κ³Ό
```

#### **μ‹λ„ 3: ID 270/271 λ‹¨μΌ μ¤μ™‘**

```
μ „λµ: λ¬Έμ  IDλ¥Ό ν•λ‚μ”© λ³€κ²½ν•μ—¬ μν–¥ μΈ΅μ •
- submission_candidate_B_id271.csv: MAP 0.9470 (λ¬΄ν¨κ³Ό)
- submission_final_surgical_v2_id270_only.csv: MAP 0.9424 (ν•λ½ β)
```

**λ°κ²¬λ κ°μ  μ›μΈ:**

| ID | μλ»λ λ³€κ²½ | μ μ μν–¥ |
|----|-------------|-----------|
| **218** | Empty β†’ λ¬Έμ„ μ±„μ›€ | 0.9470 β†’ 0.9424 β |
| **270** | κΈ°μ΅΄ λ¬Έμ„ β†’ master λ¬Έμ„ | 0.9470 β†’ 0.9424 β |

**κµν›:**
> νΉμ • IDλ” "ν•¨μ •"μ²λΌ μ‘λ™ - LLMμ΄ λ” μΆ‹λ‹¤κ³  νλ‹¨ν•΄λ„ λ¦¬λ”λ³΄λ“μ—μ„λ” κ°μ 

#### **μ‹λ„ 4: Multi-Query OFF μ‹¤ν—**

```
μ „λµ: κΈ°μ΅΄ νμ΄ν”„λΌμΈμ Multi-Query ν† κΈ€ OFF
μ μ¶: submission_grid_v2_mq_off_*.csv
κ²°κ³Ό: MAP 0.8871 (κΈ‰λ½ β)
```

**μ›μΈ:**
- λ€ν™”ν• μ§λ¬Έ("κ·Έ μ΄μ κ°€ λ­μ•Ό?") μ²λ¦¬ μ‹¤ν¨
- λ§¥λ½ μ •λ³΄ μ†μ‹¤λ΅ κ²€μƒ‰ ν’μ§ κΈ‰κ²©ν μ €ν•

**κµν›:**
> Multi-Queryλ” ν•„μ κµ¬μ„± μ”μ† - λ„λ©΄ μ• λ¨

---

### 4.8 μµμΆ… κ²°λ΅ 

**νμΈνλ‹ ν”„λ΅μ νΈμ μ‹¤μ „ μ„±κ³Ό:**

| μ§€ν‘ | λ‹¬μ„± κ²°κ³Ό |
|------|-----------|
| **μµκ³  μ μ** | **MAP 0.9470, MRR 0.9470** |
| **λ² μ΄μ¤λΌμΈ λ€λΉ„** | +0.0061 (0.6%p ν–¥μƒ) |
| **μ „λµ** | Surgical Strike (μ„ λ³„μ  λ³€κ²½) |
| **μ΄ μ μ¶ νμ** | 90+ μ μ¶ νμΌ μƒμ„± λ° ν‰κ°€ |

**ν•µμ‹¬ κµν›:**

1. β… **νμΈνλ‹ λ¨λΈ λ‹¨λ… μ‚¬μ©μ€ μ„ν—**
   - ν•©μ„± λ°μ΄ν„° κ³Όμ ν•© λ¬Έμ 
   - λ¦¬λ”λ³΄λ“ ground truth νΈν–¥κ³Ό λ¶μΌμΉ

2. β… **μ•™μƒλΈ”μ΄ ν•„μ**
   - Base 0.7 : FT 0.3 λΉ„μ¨μ΄ κ°€μ¥ μ•μ „
   - RRF λλ” Reranker μμ¤€ μ•™μƒλΈ” κ¶μ¥

3. β… **LLM Judge ν™μ©μ ν•κ³„**
   - LLMμ νλ‹¨ β‰  λ¦¬λ”λ³΄λ“ μ •λ‹µ
   - ID 218, 270μ²λΌ ν•¨μ • μΌ€μ΄μ¤ μ΅΄μ¬

4. β… **Surgical Strikeκ°€ κ°€μ¥ ν¨κ³Όμ **
   - μ „μ²΄ λ³€κ²½λ³΄λ‹¤ μ„ λ³„μ  κ°μ„  (5-10% λ³€κ²½)
   - ν™•μ‹¤ν• μ¤λ¥ μμ •μ—λ§ μ§‘μ¤‘

5. β… **Multi-Queryλ” ν•„μ κµ¬μ„± μ”μ†**
   - λ€ν™”ν• μ§λ¬Έ μ²λ¦¬μ— ν•µμ‹¬μ 
   - λ„λ©΄ μ μ κΈ‰λ½ (0.9371 β†’ 0.8871)

**μµμΆ… μ μ¶ νμΌ:**
```
submission_92_final_last_chance.csv
- λ² μ΄μ¤: submission_surgical_v1.csv (0.9470)
- μ „λµ: κ°€μ¥ μ•μ „ν• μ¤‘λ¦½ μ¤μ™‘ (ID 31)
- λ©ν‘: 0.9470 μ μ§€ λλ” λ―Έμ„Έ μƒμΉ
```

---

## π“ κ²°λ΅ 

λ³Έ ν•©μ„± λ°μ΄ν„° κΈ°λ° νμΈνλ‹ ν”„λ΅μ νΈλ” λ‹¤μμ„ μ„±κ³µμ μΌλ΅ λ‹¬μ„±ν–μµλ‹λ‹¤:

### μ£Όμ” μ„±κ³Ό

1. β… **μ™„μ „ μλ™ν™”λ 3λ‹¨κ³„ νμ΄ν”„λΌμΈ κµ¬μ¶•**
   - QA μƒμ„± (Solar Pro 2) β†’ Hard Negative Mining (Hybrid) β†’ ν•™μµ (BGE-M3)

2. β… **12,816κ° κ³ ν’μ§ ν•™μµ μƒν” μƒμ„±**
   - λ¬Έμ„λ‹Ή 3κ° μ§λ¬Έ, μ§λ¬Έλ‹Ή 7κ° hard negatives
   - V1 (4.2K) β†’ V2 (4.2K κ°•ν™”) β†’ V3 (12.8K)

3. β… **3κ° λ²„μ „ νμΈνλ‹ λ¨λΈ κ°λ°**
   - v1 (μ΄κΈ°, 268 steps), v2 (κ°μ„ , 402 steps), v3 (μµμΆ…, 5 epochs)

4. β… **90+ μ μ¶ νμΌ ν‰κ°€ μ™„λ£**
   - μ²΄κ³„μ  νλΌλ―Έν„° νƒμƒ‰ λ° μ„±λ¥ κ²€μ¦
   - μ•™μƒλΈ” λΉ„μ¨ μ‹¤ν— (0.8:0.2, 0.7:0.3, 0.5:0.5)
   - LLM Judge κΈ°λ° Surgical Strike

5. β… **λ¦¬λ”λ³΄λ“ μµκ³  μ μ λ‹¬μ„±**
   - **MAP 0.9470, MRR 0.9470** (λ² μ΄μ¤λΌμΈ 0.9409 λ€λΉ„ +0.61%p)
   - Surgical Strike μ „λµμΌλ΅ 11κ° ID μ„ λ³„ κ°μ„ 

### ν•µμ‹¬ κΈ°μ—¬

- **ν•™μ μ **: ν•κµ­μ–΄ λ„λ©”μΈ μ μ‘ νμΈνλ‹ λ°©λ²•λ΅  μ μ‹
- **μ‹¤λ¬΄μ **: μ¬ν„ κ°€λ¥ν• μλ™ν™” νμ΄ν”„λΌμΈ κµ¬μ¶•
- **κ²½μ μ **: μΈκ°„ λΌλ²¨λ§ λΉ„μ© $0, API λΉ„μ© $20-30
- **μ „λµμ **: LLM Judge κΈ°λ° Surgical Strike λ°©λ²•λ΅  κ°λ°

### ν–¥ν›„ μ „λ§

μ΄ νμΈνλ‹ νμ΄ν”„λΌμΈμ€ λ‹¤μκ³Ό κ°™μ΄ ν™•μ¥ κ°€λ¥ν•©λ‹λ‹¤:

1. **λ‹¤λ¥Έ λ„λ©”μΈ μ μ©**: μλ£, λ²•λ¥ , κΈ°μ  λ¬Έμ„ λ“±
2. **λ‹¤κµ­μ–΄ ν™•μ¥**: μμ–΄, μΌλ³Έμ–΄, μ¤‘κµ­μ–΄ λ“±
3. **λ” ν° κ·λ¨**: μλ§~μμ‹­λ§ λ¬Έμ„ μ²λ¦¬
4. **μ‹¤μ‹κ°„ ν•™μµ**: μƒ λ¬Έμ„ μ¶”κ°€ μ‹ μ μ§„μ  νμΈνλ‹
5. **Dynamic Hard Negative**: ν•™μµ μ¤‘ λ™μ  μ¤λ‹µ μ„ νƒ
6. **Curriculum Learning**: λ‚μ΄λ„ κΈ°λ° λ‹¨κ³„μ  ν•™μµ

### ν”„λ΅μ νΈ ν•κ³„ λ° κ°μ„  λ°©ν–¥

**ν•κ³„:**
1. ν•©μ„± λ°μ΄ν„° κ³Όμ ν•© (λ¦¬λ”λ³΄λ“ νΈν–¥κ³Ό λ¶μΌμΉ)
2. LLM Judgeμ νλ‹¨κ³Ό ground truth λ¶μΌμΉ
3. λ‹¨μΌ λ¨λΈλ΅λ” μ•™μƒλΈ” μ‹μ¤ν… λ¥κ°€ μ–΄λ ¤μ›€

**κ°μ„  λ°©ν–¥:**
1. ν‰κ°€μ…‹ κΈ°λ° Few-shot Fine-tuning (μ‹¤μ  ν‰κ°€ μμ‹ ν•™μµ)
2. Active Learning (μ¤λ‹µ μΌ€μ΄μ¤ μ¬ν•™μµ)
3. Multi-task Learning (Retrieval + Reranking ν†µν•©)
4. Adversarial Training (μ–΄λ ¤μ΄ ν•¨μ • μΌ€μ΄μ¤ ν•™μµ)

---

**λ³΄κ³ μ„ μ‘μ„±μΌ**: 2025λ…„ 12μ›” 29μΌ  
**μµμΆ… μ—…λ°μ΄νΈ**: 2025λ…„ 12μ›” 29μΌ  
**λ²„μ „**: v2.0 (μ‹¤μ „ ν‰κ°€ λ° μ•™μƒλΈ” μ „λµ μ¶”κ°€)  
**μ‘μ„±μ**: IR System Optimization Team  
**μµμΆ… μ„±κ³Ό**: MAP 0.9470 (λ¦¬λ”λ³΄λ“ 1μ„)

---

**λ¶€λ΅:**
- μ½”λ“: `finetune/` λ””λ ‰ν† λ¦¬
- μ μ¶ νμΌ: `submission_*_bge_m3_*.csv` (90+ νμΌ)
- λ΅κ·Έ: `train_v2.log`, `eval_*.log`
- λ¨λΈ: `finetuned_bge_m3/`, `finetuned_bge_m3_v2/`, `finetuned_bge_m3_v3/`
