import json
import os
from models.solar_client import solar_client
from retrieval.hybrid_search import run_hybrid_search
from retrieval.es_connector import es

# ğŸ¯ Phase 4D: ê³ ì„±ëŠ¥ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ê²Œì´íŒ… ì—†ìŒ)
# MAP 0.8424 ë‹¬ì„± ì„¤ì •
# - Solar Pro 2 HyDE (300ì)
# - Hard Voting [5,4,2]
# - SBERT + Gemini embedding
# - ê²Œì´íŒ… ì •ì±… ì œê±° â†’ ëª¨ë“  ì§ˆë¬¸ ê²€ìƒ‰ ìˆ˜í–‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
def _env_bool(name: str, default: bool) -> bool:
    v = os.getenv(name)
    if v is None:
        return default
    v = v.strip().lower()
    return v in ("1", "true", "t", "yes", "y", "on")


def _env_int(name: str, default: int) -> int:
    v = os.getenv(name)
    if v is None:
        return default
    try:
        return int(v)
    except Exception:
        return default


def _env_float(name: str, default: float) -> float:
    v = os.getenv(name)
    if v is None:
        return default
    try:
        return float(v)
    except Exception:
        return default


def _env_json_list(name: str, default_list):
    v = os.getenv(name)
    if not v:
        return default_list
    try:
        parsed = json.loads(v)
        if isinstance(parsed, list) and all(isinstance(x, int) for x in parsed):
            return parsed
    except Exception:
        pass
    return default_list


# ê¸°ë³¸ê°’(í˜„ì¬ ìµœê³ ì  ê·¼ì²˜ ì„¤ì •). ì‹¤í—˜ ëŸ¬ë„ˆì—ì„œ í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥.
VOTING_WEIGHTS = _env_json_list("VOTING_WEIGHTS_JSON", [5, 4, 2])
USE_MULTI_EMBEDDING = _env_bool("USE_MULTI_EMBEDDING", True)   # SBERT + Gemini
USE_GEMINI_ONLY = _env_bool("USE_GEMINI_ONLY", False)
TOP_K_RETRIEVE = _env_int("TOP_K_RETRIEVE", 80)
USE_RRF = _env_bool("USE_RRF", False)
RRF_K = _env_int("RRF_K", 60)
USE_GATING = _env_bool("USE_GATING", False)
HYDE_MAX_LENGTH = _env_int("HYDE_MAX_LENGTH", 200)
USE_SOLAR_ANALYZER = _env_bool("USE_SOLAR_ANALYZER", True)
USE_MULTI_QUERY = _env_bool("USE_MULTI_QUERY", False)

# is_science=falseì¼ ë•Œ(topk=[] ì •ì±…) ì ìš©í•  ìµœì†Œ ì‹ ë¢°ë„. ë‚®ìœ¼ë©´ ê²€ìƒ‰ìœ¼ë¡œ ê°•ì œ(ì˜¤íŒ ë°©ì§€).
NO_SEARCH_CONFIDENCE_THRESHOLD = _env_float("NO_SEARCH_CONFIDENCE_THRESHOLD", 0.0)

# intent ê¸°ë°˜ìœ¼ë¡œ topk=[] ì¼€ì´ìŠ¤ë¥¼ ë” ì¡ì•„ë‚´ëŠ” ì˜µì…˜(ê¸°ë³¸ off: ê¸°ì¡´ ìµœê³ ì  ì¬í˜„ ì•ˆì „)
ENABLE_INTENT_NO_SEARCH = _env_bool("ENABLE_INTENT_NO_SEARCH", False)
# intent/ê·œì¹™ ê¸°ë°˜ ë³´ì¡° ê²Œì´íŒ…ì„ ì ìš©í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì„ê³„ê°’(ë³´ìˆ˜ì ìœ¼ë¡œ ì‹œì‘ ê¶Œì¥)
NO_SEARCH_STRICT_THRESHOLD = _env_float("NO_SEARCH_STRICT_THRESHOLD", NO_SEARCH_CONFIDENCE_THRESHOLD)
NO_SEARCH_HEURISTIC_THRESHOLD = _env_float("NO_SEARCH_HEURISTIC_THRESHOLD", 0.85)
NO_SEARCH_OVERRIDE_SCIENCE_MAX_CONF = _env_float("NO_SEARCH_OVERRIDE_SCIENCE_MAX_CONF", 0.10)

# í›„ë³´êµ°(ë¦¬ë­ì»¤ ì…ë ¥) í¬ê¸°
CANDIDATE_POOL_SIZE = _env_int("CANDIDATE_POOL_SIZE", 80)


def _last_user_text(messages) -> str:
    try:
        if isinstance(messages, list) and messages:
            for m in reversed(messages):
                if str(m.get("role", "user")) == "user":
                    return str(m.get("content", "") or "")
        return str(messages)
    except Exception:
        return str(messages)


def _looks_like_science(text: str) -> bool:
    t = (text or "").lower()
    science_markers = [
        # Korean science/tech keywords
        "ê´‘í•©ì„±", "dna", "rna", "ì„¸í¬", "ë¯¸í† ì½˜ë“œë¦¬ì•„", "ë‹¨ë°±ì§ˆ", "íš¨ì†Œ", "ìœ ì „ì",
        "ë‰´í„´", "ë²•ì¹™", "ë¬¼ë¦¬", "í™”í•™", "ìƒë¬¼", "ì§€êµ¬ê³¼í•™", "ì²œë¬¸", "ìš°ì£¼", "ì–‘ì",
        "ì „ê¸°", "ìê¸°", "ì „ì", "ì›ì", "ë¶„ì", "ë°˜ì‘", "ì´‰ë§¤", "ì‚°í™”", "í™˜ì›",
        "ì•Œê³ ë¦¬ì¦˜", "ë³µì¡ë„", "ë¹…ì˜¤", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ì‹ ê²½ë§",
    ]
    return any(m in t for m in science_markers)


def _looks_like_knowledge_query(text: str) -> bool:
    """ì§€ì‹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ íŒ¨í„´ ê°ì§€ (ê²Œì´íŒ… ì˜¤ë²„ë¼ì´ë“œìš©)
    
    ì£¼ì˜: AI ë©”íƒ€ ì§ˆë¬¸("ë„ˆ ë­ì•¼?", "ë„ˆ ì˜í•˜ëŠ”ê²Œ ë­ì•¼?")ì€ ì œì™¸í•´ì•¼ í•¨
    """
    t = (text or "").lower()
    
    # AI ë©”íƒ€ ì§ˆë¬¸ íŒ¨í„´ ì œì™¸ (ê²€ìƒ‰ ë¶ˆí•„ìš”)
    ai_meta_patterns = ["ë„ˆ ", "ë„Œ ", "ë‹ˆê°€ ", "ë„ˆëŠ” ", "ë„¤ê°€ "]
    if any(p in t for p in ai_meta_patterns):
        return False
    
    # ì¼ìƒ ëŒ€í™”/ê°ì • í‘œí˜„ ì œì™¸ (ê²€ìƒ‰ ë¶ˆí•„ìš”)
    casual_patterns = ["í˜ë“œ", "ìš°ìš¸", "ê¸°ë¶„", "ì¦ê±°", "ì‹ ë‚˜", "ë°˜ê°€", "ì•ˆë…•"]
    if any(p in t for p in casual_patterns) and len(t) < 30:
        return False
    
    # ì¥ì /ë‹¨ì /íš¨ê³¼/ì´ìœ  ë“±ì„ ë¬»ëŠ” ì§ˆë¬¸ì€ ë¬¸ì„œ ê²€ìƒ‰ì´ ë„ì›€ë  ìˆ˜ ìˆìŒ
    knowledge_markers = [
        # ë¶„ì„/í‰ê°€ ì§ˆë¬¸
        "ì¢‹ì€ ì ", "ë‚˜ìœ ì ", "ì¥ì ", "ë‹¨ì ", "ì´ì ", "í˜œíƒ", "íš¨ê³¼", "ê°€ì¹˜",
        "ì›ì¸", "ì´ìœ ", "ë°©ë²•", "ë°©ì•ˆ", "ê³¼ì •", "ì›ë¦¬", "êµ¬ì¡°", "íŠ¹ì§•",
        "ì°¨ì´", "ë¹„êµ", "ì •ì˜", "ê°œë…", "ì—­ì‚¬", "ìœ ë˜", "ë°œì „",
        "ì¢…ë¥˜", "ë¶„ë¥˜", "ì˜ˆì‹œ", "ì‚¬ë¡€", "ì˜í–¥", "ê²°ê³¼", "í˜„í™©", "ìƒí™©",
        # ì˜ë¬¸ì‚¬
        "ë¬´ì—‡", "ë­ì•¼", "ë­ì§€", "ë­”ê°€", "ì–´ë–»ê²Œ", "ì™œ", "ì–¼ë§ˆë‚˜",
        "ì–´ë–¤", "ì–´ë””", "ì–¸ì œ",
        # ìš”ì²­í˜•
        "ì•Œë ¤", "ì„¤ëª…", "ì¡°ì‚¬", "ì—°êµ¬", "ë§í•´", "ê°€ë¥´ì³",
        # ì§€ì‹ ì£¼ì œ
        "ììœ ", "ê¶Œë¦¬", "ì œë„", "ì •ì±…", "ë²•ë¥ ", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”",
        "ê³¼í•™", "ê¸°ìˆ ", "í™˜ê²½", "ê±´ê°•", "êµìœ¡", "ì—­ì‚¬",
    ]
    return any(m in t for m in knowledge_markers)


def _looks_like_chitchat(text: str) -> bool:
    t = (text or "").strip().lower()
    if not t:
        return True

    # very short messages are often chit-chat/meta
    if len(t) <= 6 and any(x in t for x in ["ì•ˆë…•", "hi", "hello", "ã…‡ã…‡", "ã„´ã„´", "ã…‹ã…‹", "ã…ã…"]):
        return True

    markers = [
        # greetings
        "ì•ˆë…•", "ë°˜ê°€", "ì¢‹ì€ ì•„ì¹¨", "ì¢‹ì€ë°¤", "êµ¿ëª¨ë‹", "êµ¿ë‚˜ì‡",
        # thanks/apology
        "ê³ ë§ˆ", "ê°ì‚¬", "ì£„ì†¡", "ë¯¸ì•ˆ",
        # emotions
        "í˜ë“¤", "ìš°ìš¸", "ê¸°ë¶„", "ì§œì¦", "í–‰ë³µ", "ìŠ¬í¼",
        # daily talk
        "ë‚ ì”¨", "ë°¥", "ì ì‹¬", "ì €ë…", "ë­í•´", "ë­í•¨",
        # laughter/slang
        "ã…‹ã…‹", "ã…ã…", "lol",
        # assistant meta
        "ë„ˆëŠ” ëˆ„êµ¬", "ë„Œ ëˆ„êµ¬", "ë„ˆ ëˆ„êµ¬", "í•  ìˆ˜ ìˆì–´", "ê°€ëŠ¥í•´", "ë„ì™€ì¤˜",
    ]
    return any(m in t for m in markers)

def answer_question_optimized(messages):
    res = {"standalone_query": "", "topk": [], "answer": ""}
    solar_analysis = None
    # ì›ë¬¸ ì‚¬ìš©ì ì§ˆë¬¸(ê²€ìƒ‰/ë¦¬ë­ì»¤ ì¿¼ë¦¬ë¡œ ì‚¬ìš©): LLM rewriteë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ë°©ì§€
    original_user_query = ""
    try:
        if isinstance(messages, list) and messages:
            original_user_query = messages[-1].get('content', '')
        else:
            original_user_query = str(messages)
    except Exception:
        original_user_query = str(messages)
    last_user_text = _last_user_text(messages)
    if USE_SOLAR_ANALYZER:
        solar_analysis = solar_client.analyze_query_and_hyde(messages, hyde_max_chars=HYDE_MAX_LENGTH)
        is_science_question = bool(solar_analysis.get("is_science", False))
        solar_confidence = float(solar_analysis.get("confidence", 0.0) or 0.0)
        # ì œì¶œìš©(standalone_query)ì€ Solar ì •ê·œí™” ê²°ê³¼ ì‚¬ìš©
        query_text = solar_analysis.get("standalone_query", "") or original_user_query
    else:
        # (ë ˆê±°ì‹œ) ì•ˆì „ì¥ì¹˜: Solar analyzer ë¹„í™œì„±í™” ì‹œ ëª¨ë“  ì§ˆë¬¸ ê²€ìƒ‰
        is_science_question = True
        solar_confidence = 1.0
        query_text = original_user_query
    
    res["standalone_query"] = query_text
    
    # ë¹„ê³¼í•™(ê²€ìƒ‰ ë¶ˆí•„ìš”) ì§ˆë¬¸: topk=[]
    # - ê¸°ë³¸: Solar is_science=false + confidence ì„ê³„ê°’
    # - ì˜µì…˜(ENABLE_INTENT_NO_SEARCH=true): ê·œì¹™ ê¸°ë°˜(ì¡ë‹´/ë©”íƒ€) ë³´ì¡° ê²Œì´íŒ…ìœ¼ë¡œ ì¶”ê°€ ì¼€ì´ìŠ¤ í¬ì°©
    # - ì§€ì‹ ì§ˆë¬¸ íŒ¨í„´ ê°ì§€ ì‹œ ê²€ìƒ‰ ê°•ì œ(ê²Œì´íŒ… ì˜¤ë²„ë¼ì´ë“œ)
    predicted_no_search = False
    force_search = _looks_like_knowledge_query(last_user_text)
    
    if force_search:
        # ì§€ì‹ ì§ˆë¬¸ íŒ¨í„´ì´ ê°ì§€ë˜ë©´ ê²€ìƒ‰ ê°•ì œ
        predicted_no_search = False
    elif (not is_science_question) and (solar_confidence >= NO_SEARCH_STRICT_THRESHOLD):
        predicted_no_search = True
    elif ENABLE_INTENT_NO_SEARCH:
        rule_chitchat = _looks_like_chitchat(last_user_text)
        rule_science_block = _looks_like_science(last_user_text)
        if rule_chitchat and (not rule_science_block):
            # Solarë„ ë¹„ê³¼í•™ìœ¼ë¡œ ë³´ê±°ë‚˜(conf ì¶©ë¶„) / ë˜ëŠ” Solarê°€ ê³¼í•™ì´ë¼ í•´ë„ í™•ì‹ ì´ ë§¤ìš° ë‚®ìœ¼ë©´ override
            if ((not is_science_question) and (solar_confidence >= NO_SEARCH_HEURISTIC_THRESHOLD)) or (
                is_science_question and (solar_confidence <= NO_SEARCH_OVERRIDE_SCIENCE_MAX_CONF)
            ):
                predicted_no_search = True

    if predicted_no_search:
        res["topk"] = []
        if solar_analysis and solar_analysis.get("direct_answer"):
            res["answer"] = solar_analysis.get("direct_answer")
        else:
            res["answer"] = solar_client.generate_answer(messages, "")
        return res
    
    # Solar analyzerê°€ HyDEë¥¼ í•¨ê»˜ ìƒì„±í–ˆë‹¤ë©´ ì¬ì‚¬ìš©(LLM í˜¸ì¶œ 1íšŒ ì ˆê°)
    hypothetical_answer = ""
    if solar_analysis and solar_analysis.get("hyde"):
        hypothetical_answer = solar_analysis.get("hyde")
    else:
        hypothetical_answer = solar_client.generate_hypothetical_answer(query_text)
    
    # HyDE í™•ì¥ ì¿¼ë¦¬ ìƒì„±
    if hypothetical_answer:
        # SparseëŠ” ì •ê·œí™” standalone_query + HyDEë¡œ ì¬í˜„ìœ¨ í™•ë³´
        hyde_query = f"{query_text}\n{hypothetical_answer}"
    else:
        hyde_query = query_text
    
    # Hybrid Search with Reranker ì‹¤í–‰ (Phase 4D: [5,4,2] + TopK=50)
    multi_queries = []
    if USE_MULTI_QUERY:
        try:
            # ë©€í‹° ì¿¼ë¦¬ëŠ” standalone_query ê¸°ë°˜(ì›ë¬¸ ì˜ë¯¸ ë³´ì¡´)
            multi_queries = solar_client.generate_multi_query(query_text) or []
        except Exception:
            multi_queries = []

    final_ranked_results = run_hybrid_search(
        # Dense/ë¦¬ë­ì»¤ëŠ” ì›ë¬¸ ì§ˆë¬¸ ìœ ì§€(LLM rewriteë¡œ ì˜ë¯¸ê°€ ë°”ë€ŒëŠ” ë¦¬ìŠ¤í¬ ê°ì†Œ)
        original_query=original_user_query,
        sparse_query=hyde_query,
        reranker_query=original_user_query,
        voting_weights=VOTING_WEIGHTS,  # [5, 4, 2]
        use_multi_embedding=USE_MULTI_EMBEDDING,  # SBERT + Gemini
        top_k_retrieve=TOP_K_RETRIEVE,  # 50ê°œ
        candidate_pool_size=CANDIDATE_POOL_SIZE,
        use_gemini_only=USE_GEMINI_ONLY,
        use_rrf=USE_RRF,  # False (Hard Voting)
        rrf_k=RRF_K,
        multi_queries=multi_queries
    )
    
    # ê²€ìƒ‰ ê²°ê³¼ ì„¤ì •: í•­ìƒ ë°˜í™˜ (ê²Œì´íŒ… ì—†ìŒ)
    res["topk"] = final_ranked_results[:5]  # ìƒìœ„ 5ê°œ
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±: Top-3 ë¬¸ì„œ ë‚´ìš© ì‚¬ìš©
    context_docs = []
    for docid in final_ranked_results[:3]:
        search_result = es.search(
            index="test",
            query={"term": {"docid": docid}},
            size=1
        )
        if search_result['hits']['hits']:
            context_docs.append(search_result['hits']['hits'][0]['_source']['content'])
    
    context = " ".join(context_docs)
    # Solar Pro 2ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    res["answer"] = solar_client.generate_answer(messages, context)
    
    return res