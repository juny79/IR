# IR ì‹œìŠ¤í…œ - íŒŒì¼ë³„ ì½”ë“œ ìœ„ì¹˜ ë° ì£¼ìš” í•¨ìˆ˜ ë§µí•‘

## ğŸ“ ê° íŒŒì¼ì˜ ì£¼ìš” í•¨ìˆ˜ ìœ„ì¹˜

### 1ï¸âƒ£ main.py (~70ì¤„)

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/main.py`

```python
# ì£¼ìš” ë¡œì§
for i, line in enumerate(f, 1):
    data = json.loads(line)
    if data["eval_id"] in processed_ids:
        continue
    print(f"[{i}/220] ID {data['eval_id']} ì²˜ë¦¬ ì¤‘...")
    result = answer_question_optimized(data["msg"])  # â† í•µì‹¬
    output = {"eval_id": data["eval_id"], "standalone_query": result["standalone_query"], ...}
    of.write(json.dumps(output, ensure_ascii=False) + "\n")
```

**í•¨ìˆ˜**: `main()`  
**ì…ì¶œë ¥**:
- ì…ë ¥: `data/eval.jsonl` (220ê°œ ì§ˆë¬¸)
- ì¶œë ¥: `submission.csv` (ì‹¤ì‹œê°„ ì €ì¥)

**ì—­í• **: í‰ê°€ ë£¨í”„ ì „ì²´ ê´€ë¦¬, ì¤‘ë‹¨/ì¬ì‹œì‘ ì•ˆì „

---

### 2ï¸âƒ£ eval_rag.py (~70ì¤„)

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/eval_rag.py`

**ğŸ”´ ì„¤ì •ê°’ (ê°€ì¥ ìì£¼ ìˆ˜ì •)**:
```python
# ë¼ì¸ 1-15
VOTING_WEIGHTS = [5, 4, 2]
USE_MULTI_EMBEDDING = True
USE_GEMINI_ONLY = False
TOP_K_RETRIEVE = 60
USE_RRF = False
RRF_K = 60
USE_GATING = True
```

**ì£¼ìš” í•¨ìˆ˜** `answer_question_optimized(messages)`:
```python
# ë¼ì¸ 17-70
def answer_question_optimized(messages):
    res = {"standalone_query": "", "topk": [], "answer": ""}
    
    # Step 1: ì¿¼ë¦¬ ë¶„ì„
    analysis = llm_client.analyze_query(messages)
    
    if analysis.tool_calls:
        # Step 2: ì¿¼ë¦¬ ì •ì œ
        query = json.loads(analysis.tool_calls[0].function.arguments)['standalone_query']
        res["standalone_query"] = query
        
        # Step 3: HyDE í™•ì¥
        hypothetical_answer = solar_client.generate_hypothetical_answer(query)
        hyde_query = f"{query}\n{hypothetical_answer}" if hypothetical_answer else query
        
        # Step 4: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        final_ranked_results = run_hybrid_search(
            original_query=query,
            sparse_query=hyde_query,
            reranker_query=query,
            voting_weights=VOTING_WEIGHTS,
            use_multi_embedding=USE_MULTI_EMBEDDING,
            top_k_retrieve=TOP_K_RETRIEVE,
            use_gemini_only=USE_GEMINI_ONLY,
            use_rrf=USE_RRF,
            rrf_k=RRF_K
        )
        
        # Step 5: ë¬¸ì„œ ë‚´ìš© ì¡°íšŒ
        res["topk"] = final_ranked_results[:5]
        context_docs = []
        for docid in final_ranked_results[:3]:
            search_result = es.search(index="test", query={"term": {"docid": docid}}, size=1)
            if search_result['hits']['hits']:
                context_docs.append(search_result['hits']['hits'][0]['_source']['content'])
        context = " ".join(context_docs)
        
        # Step 6: ë‹µë³€ ìƒì„±
        res["answer"] = solar_client.generate_answer(messages, context)
    else:
        # ë¹„ê³¼í•™ ì§ˆë¬¸ ì²˜ë¦¬ (ê²Œì´íŒ…)
        res["standalone_query"] = ""
        res["topk"] = []
        res["answer"] = analysis.content
    
    return res
```

**í•µì‹¬**: ì´ íŒŒì¼ì˜ ì„¤ì •ê°’ì„ ë°”ê¾¸ë©´ ì „ì²´ ë™ì‘ì´ ë³€ê²½ë¨

---

### 3ï¸âƒ£ models/llm_client.py

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/models/llm_client.py`

**í´ë˜ìŠ¤**: `LLMClient`

**ì£¼ìš” í•¨ìˆ˜**:
```python
def analyze_query(messages):
    """
    Gemini APIë¡œ ì¿¼ë¦¬ ë¶„ì„
    
    [ë°˜í™˜ê°’]
    - tool_calls ìˆìŒ: ê³¼í•™ ì§ˆë¬¸
      {
        "tool_calls": [{...}],
        "content": None
      }
    - tool_calls ì—†ìŒ: ë¹„ê³¼í•™ ì§ˆë¬¸
      {
        "tool_calls": None,
        "content": "ì•ˆë…•í•˜ì„¸ìš”!"
      }
    """
    # Gemini API í˜¸ì¶œ
    response = genai.GenerativeModel(...).generate_content(
        content=messages,
        tools=[...],
        tool_config=...
    )
    return response
```

**íŠ¹ì§•**: ìºì‹± ì—†ìŒ (ë§¤ë²ˆ ìƒˆ API í˜¸ì¶œ)

---

### 4ï¸âƒ£ models/solar_client.py

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/models/solar_client.py`

**í´ë˜ìŠ¤**: `SolarClient`

**ì£¼ìš” í•¨ìˆ˜ 1**: `generate_hypothetical_answer(query)`
```python
def generate_hypothetical_answer(self, query):
    """
    Solar Pro 2 HyDE ì¿¼ë¦¬ í™•ì¥
    
    [ìºì‹±]
    1. cache/hyde_cache.pkl í™•ì¸
    2. ìºì‹œ ë¯¸ìŠ¤ â†’ Upstage API í˜¸ì¶œ
    3. ìºì‹œ ì €ì¥ (20ê°œë§ˆë‹¤)
    
    [ë°˜í™˜]
    ê°€ì„¤ì  ë‹µë³€ ë¬¸ìì—´
    """
    # MD5 ìºì‹œ í‚¤ ìƒì„±
    cache_key = hashlib.md5(query.encode()).hexdigest()
    
    # ìºì‹œ ì¡°íšŒ
    if cache_key in self.hyde_cache:
        return self.hyde_cache[cache_key]
    
    # API í˜¸ì¶œ
    response = self.client.messages.create(
        model="solar-pro",
        messages=[{"role": "user", "content": f"ì¿¼ë¦¬ í™•ì¥: {query}"}]
    )
    
    # ìºì‹œ ì €ì¥
    self.hyde_cache[cache_key] = response.content
    return response.content
```

**ì£¼ìš” í•¨ìˆ˜ 2**: `generate_answer(messages, context)`
```python
def generate_answer(self, messages, context):
    """
    ìµœì¢… ë‹µë³€ ìƒì„±
    
    [ì…ë ¥]
    - messages: ì‚¬ìš©ì ë©”ì‹œì§€
    - context: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
    
    [ë°˜í™˜]
    ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
    """
    # Solar Pro 2 APIë¡œ ë‹µë³€ ìƒì„±
    system_prompt = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n{context}"
    response = self.client.messages.create(...)
    return response.content
```

**íŠ¹ì§•**:
- âœ… HyDE ìƒì„±ì€ ìºì‹± (pickle)
- âŒ ë‹µë³€ ìƒì„±ì€ ìºì‹± ì—†ìŒ

---

### 5ï¸âƒ£ models/embedding_client.py

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/models/embedding_client.py`

**í´ë˜ìŠ¤**: `EmbeddingClient`

**ì£¼ìš” í•¨ìˆ˜**: `get_query_embedding(query, use_gemini_only=False)`
```python
def get_query_embedding(self, query, use_gemini_only=False):
    """
    ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    
    [ê²½ìš° 1] use_gemini_only=False (ê¸°ë³¸)
    - SBERTë¡œ ë¡œì»¬ ì„ë² ë”©
    - ë¹ ë¦„ (~100ms)
    - ìºì‹± ì—†ìŒ
    
    [ê²½ìš° 2] use_gemini_only=True
    - Gemini API í˜¸ì¶œ
    - ìºì‹± ì ìš© (34,893ë°° ì†ë„)
    - MD5 í•´ì‹±ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±
    
    [ë°˜í™˜]
    768ì°¨ì› ë²¡í„°
    """
    
    if use_gemini_only:
        # Gemini ìºì‹± ë¡œì§
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.query_embedding_cache:
            return self.query_embedding_cache[cache_key]
        
        # API í˜¸ì¶œ
        response = genai.embed_content(
            model="models/text-embedding-004",
            content=query
        )
        embedding = response['embedding']
        
        # ìºì‹œ ì €ì¥
        self.query_embedding_cache[cache_key] = embedding
        return embedding
    else:
        # SBERT ë¡œì»¬ ì„ë² ë”©
        return self.sbert_model.encode(query)
```

**ë‘ ì„ë² ë”© ëª¨ë¸**:
1. SBERT: `snunlp/KR-SBERT-V40K-klueNLI-augSTS` (ë¡œì»¬, ë¹ ë¦„)
2. Gemini: `text-embedding-004` (API, ìºì‹±)

---

### 6ï¸âƒ£ retrieval/hybrid_search.py

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/retrieval/hybrid_search.py`

**ì£¼ìš” í•¨ìˆ˜**: `run_hybrid_search(...)`
```python
def run_hybrid_search(
    original_query,
    sparse_query,
    reranker_query,
    voting_weights=[5, 4, 2],
    use_multi_embedding=True,
    top_k_retrieve=50,
    use_gemini_only=False,
    use_rrf=False,
    rrf_k=60
):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ì¬ìˆœìœ„í™”
    
    [ì²˜ë¦¬ ìˆœì„œ]
    1. Sparse ê²€ìƒ‰ (BM25)
    2. Dense ê²€ìƒ‰ (SBERT)
    3. Dense ê²€ìƒ‰ (Gemini)
    4. ì ìˆ˜ í•©ì‚° (Hard Voting)
    5. Reranker ì ìš©
    """
    
    # Step 1: Sparse ê²€ìƒ‰
    sparse_results = es.search_sparse(sparse_query, top_k=top_k_retrieve)
    
    # Step 2-3: Dense ê²€ìƒ‰
    query_emb_sbert = embedding_client.get_query_embedding(original_query, use_gemini_only=False)
    dense_results_sbert = es.search_dense(query_emb_sbert, top_k=top_k_retrieve)
    
    query_emb_gemini = embedding_client.get_query_embedding(original_query, use_gemini_only=True)
    dense_results_gemini = es.search_dense(query_emb_gemini, top_k=top_k_retrieve)
    
    # Step 4: Hard Voting ë˜ëŠ” RRF
    if use_rrf:
        final_results = rrf_fusion([sparse_results, dense_results_sbert, dense_results_gemini], k=rrf_k)
    else:
        # Hard Voting [5, 4, 2]
        final_results = hard_vote_results([
            sparse_results,
            dense_results_sbert,
            dense_results_gemini
        ], voting_weights=voting_weights)
    
    # Step 5: Reranker
    final_ranked = reranker.rerank_documents(reranker_query, final_results[:top_k_retrieve])
    
    return [doc_id for doc_id, _ in final_ranked]
```

**Hard Voting í•¨ìˆ˜** `hard_vote_results()`
```python
def hard_vote_results(search_results_list, voting_weights):
    """
    ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ë¡œ íˆ¬í‘œ
    
    [ë¡œì§]
    ê° ë¬¸ì„œë³„ë¡œ:
    - Sparseì—ì„œ 1ìœ„ë©´: score += voting_weights[0] * (1 - rank/100)
    - SBERTì—ì„œ 2ìœ„ë©´: score += voting_weights[1] * (1 - rank/100)
    - Geminiì—ì„œ 3ìœ„ë©´: score += voting_weights[2] * (1 - rank/100)
    
    ì ìˆ˜ë¡œ ì¬ì •ë ¬ â†’ Top K ì„ ì •
    """
    vote_scores = defaultdict(float)
    
    for idx, results in enumerate(search_results_list):
        weight = voting_weights[idx]
        for rank, (doc_id, score) in enumerate(results, 1):
            vote_scores[doc_id] += weight * (1 - rank / 100)
    
    sorted_results = sorted(vote_scores.items(), key=lambda x: x[1], reverse=True)
    return [doc_id for doc_id, _ in sorted_results]
```

**í•µì‹¬**: [5, 4, 2] ê°€ì¤‘ì¹˜ë¡œ Sparse, SBERT, Gemini ê²°ê³¼ ìœµí•©

---

### 7ï¸âƒ£ retrieval/es_connector.py

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/retrieval/es_connector.py`

**í´ë˜ìŠ¤**: `ESConnector`

**ì£¼ìš” í•¨ìˆ˜ 1**: `search_sparse(query, top_k=50)`
```python
def search_sparse(self, query, top_k=50):
    """
    BM25 ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ sparse ê²€ìƒ‰
    
    [ì¿¼ë¦¬]
    Solar HyDE í™•ì¥ ì¿¼ë¦¬
    
    [ë°˜í™˜]
    [(doc_id, bm25_score), ...]
    """
    response = self.es.search(
        index="test",
        query={
            "match": {
                "content": {
                    "query": query,
                    "operator": "or"
                }
            }
        },
        size=top_k
    )
    
    results = []
    for hit in response['hits']['hits']:
        results.append((hit['_source']['docid'], hit['_score']))
    
    return results
```

**ì£¼ìš” í•¨ìˆ˜ 2**: `search_dense(embedding, top_k=50)`
```python
def search_dense(self, embedding, top_k=50):
    """
    Dense ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)
    
    [ì…ë ¥]
    embedding: 768ì°¨ì› ë²¡í„° (SBERT ë˜ëŠ” Gemini)
    
    [ë°˜í™˜]
    [(doc_id, similarity_score), ...]
    """
    response = self.es.search(
        index="test",
        query={
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'embeddings_field') + 1.0",
                    "params": {"query_vector": embedding}
                }
            }
        },
        size=top_k
    )
    
    results = []
    for hit in response['hits']['hits']:
        results.append((hit['_source']['docid'], hit['_score']))
    
    return results
```

**ì£¼ìš” í•¨ìˆ˜ 3**: `get_document(doc_id)`
```python
def get_document(self, doc_id):
    """
    íŠ¹ì • ë¬¸ì„œì˜ ë‚´ìš© ì¡°íšŒ
    
    [ë°˜í™˜]
    ë¬¸ì„œ ê°ì²´ (docid, content, metadata ë“±)
    """
    response = self.es.search(
        index="test",
        query={"term": {"docid": doc_id}},
        size=1
    )
    
    if response['hits']['hits']:
        return response['hits']['hits'][0]['_source']
    return None
```

---

### 8ï¸âƒ£ retrieval/reranker.py

**íŒŒì¼ ìœ„ì¹˜**: `/root/IR/retrieval/reranker.py`

**í´ë˜ìŠ¤**: `Reranker`

**ì£¼ìš” í•¨ìˆ˜**: `rerank_documents(query, documents)`
```python
def rerank_documents(self, query, document_ids, top_k=None):
    """
    BAAI Rerankerë¡œ ìµœì¢… ìˆœìœ„ ì¡°ì •
    
    [ì…ë ¥]
    - query: ì›ë³¸ ì¿¼ë¦¬
    - document_ids: ì¬ìˆœìœ„í™”í•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
    
    [ì²˜ë¦¬]
    ê° (query, document) ìŒì„ Rerankerì— ì…ë ¥
    ê´€ë ¨ì„± ì ìˆ˜ (0-1) ê³„ì‚°
    
    [ë°˜í™˜]
    ì¬ì •ë ¬ëœ [(doc_id, score), ...] ë¦¬ìŠ¤íŠ¸
    """
    
    # ë¬¸ì„œ ë‚´ìš© ì¡°íšŒ
    documents = []
    for doc_id in document_ids:
        doc = es.get_document(doc_id)
        documents.append(doc['content'])
    
    # Reranker ì…ë ¥: (query, document) ìŒ ë¦¬ìŠ¤íŠ¸
    pairs = [[query, doc] for doc in documents]
    
    # BAAI Reranker ì‹¤í–‰
    scores = self.model.predict(pairs)
    
    # ì ìˆ˜ë¡œ ì •ë ¬
    reranked = sorted(zip(document_ids, scores), key=lambda x: x[1], reverse=True)
    
    if top_k:
        return reranked[:top_k]
    return reranked
```

---

## ğŸ“Š í•¨ìˆ˜ í˜¸ì¶œ ì²´ì¸

```
main()
  â””â”€ answer_question_optimized()
      â”œâ”€ llm_client.analyze_query()
      â”œâ”€ solar_client.generate_hypothetical_answer()  âœ… ìºì‹±
      â”œâ”€ run_hybrid_search()
      â”‚   â”œâ”€ es_connector.search_sparse()
      â”‚   â”œâ”€ embedding_client.get_query_embedding()  (SBERT)
      â”‚   â”œâ”€ embedding_client.get_query_embedding()  âœ… ìºì‹± (Gemini)
      â”‚   â”œâ”€ hard_vote_results()
      â”‚   â””â”€ reranker.rerank_documents()
      â”œâ”€ es_connector.get_document()  (3íšŒ)
      â””â”€ solar_client.generate_answer()
```

---

## ğŸ”§ ì„¤ì • ë³€ê²½ ê°€ì´ë“œ

| ë³€ê²½ ëª©í‘œ | ìˆ˜ì • íŒŒì¼ | ì„¤ì •ê°’ |
|----------|----------|--------|
| ê°€ì¤‘ì¹˜ ì¡°ì • | eval_rag.py | VOTING_WEIGHTS |
| TOP_K ë³€ê²½ | eval_rag.py | TOP_K_RETRIEVE |
| SBERTë§Œ ì‚¬ìš© | eval_rag.py | USE_MULTI_EMBEDDING=False |
| RRF ì‚¬ìš© | eval_rag.py | USE_RRF=True |
| ê²Œì´íŒ… OFF | eval_rag.py | USE_GATING=False |
| Geminië§Œ ì‚¬ìš© | eval_rag.py | USE_GEMINI_ONLY=True |

---

**ê°€ì¥ ì¤‘ìš”í•œ íŒŒì¼**: eval_rag.py (ì„¤ì •) + hybrid_search.py (ê²€ìƒ‰ ë¡œì§)  
**ìºì‹± ì ìš© ëª¨ë“ˆ**: solar_client (HyDE), embedding_client (Gemini)  
**ì„±ëŠ¥ ì¡°ì • í¬ì¸íŠ¸**: VOTING_WEIGHTS, TOP_K, ë©€í‹° ì„ë² ë”©
