import os
import json
import sys
import numpy as np
import faiss
import torch
from pathlib import Path
from tqdm import tqdm
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# LLM í´ë¼ì´ì–¸íŠ¸
from models.openai_client import openai_client
from models.solar_client import solar_client

# ==========================================
# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
FINETUNED_MODEL_PATH = "/root/IR/finetuned_bge_m3"
OUTPUT_FILE = "/root/IR/submission_bge_m3_finetuned.csv"

# ëª¨ë¸ ì„¤ì •
RERANK_MODEL = 'BAAI/bge-reranker-v2-m3'

# íŒŒë¼ë¯¸í„° (v9 SOTA ê¸°ì¤€)
TOP_CANDIDATES = 100
FINAL_TOPK = 5
ALPHA = 0.5 # Hybrid weight (Dense vs Sparse)
RRF_K = 60

# ê°ì  ë°©ì§€ë¥¼ ìœ„í•œ ê²€ìƒ‰ ì œì™¸ ID (v9 SOTA ê¸°ì¤€)
EMPTY_IDS = {
    276, 261, 283, 32, 94, 90, 220, 245, 229, 
    247, 67, 57, 2, 227, 301, 222, 83, 64, 103, 218
}

def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            data.append(json.loads(line))
    return data

print("ğŸš€ ë°ì´í„° ë¡œë”© ì¤‘...")
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

# ==========================================
# 2. íŒŒì¸íŠœë‹ëœ BGE-M3 ëª¨ë¸ ë° ì¸ë±ì‹±
# ==========================================
print(f"â³ íŒŒì¸íŠœë‹ëœ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘ ({FINETUNED_MODEL_PATH})...")
# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì€ ë¡œì»¬ ê²½ë¡œì—ì„œ ë¡œë“œ
model = BGEM3FlagModel(FINETUNED_MODEL_PATH, use_fp16=True)

# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ìš© ìºì‹œ ë””ë ‰í† ë¦¬ ë³„ë„ ìš´ì˜
CACHE_DIR = "/root/IR/cache/bge_m3_finetuned"
os.makedirs(CACHE_DIR, exist_ok=True)
DENSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_dense_embs.npy")
SPARSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_sparse_embs.json")
FAISS_INDEX_PATH = os.path.join(CACHE_DIR, "bge_m3_dense.index")

# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì€ ê°€ì¤‘ì¹˜ê°€ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ ìƒˆë¡œ ì¸ë±ì‹±í•´ì•¼ í•¨
if os.path.exists(DENSE_EMB_PATH) and os.path.exists(SPARSE_EMB_PATH) and os.path.exists(FAISS_INDEX_PATH):
    print("âœ… ìºì‹œëœ íŒŒì¸íŠœë‹ BGE-M3 ì¸ë±ìŠ¤ ë¡œë“œ")
    doc_dense_embs = np.load(DENSE_EMB_PATH)
    with open(SPARSE_EMB_PATH, 'r') as f:
        doc_sparse_embs = json.load(f)
    index = faiss.read_index(FAISS_INDEX_PATH)
else:
    print("â³ íŒŒì¸íŠœë‹ BGE-M3 ì¸ë±ì‹± ìƒì„± ì¤‘ (Dense & Sparse)...")
    batch_size = 16
    all_dense = []
    all_sparse = []
    
    for i in tqdm(range(0, len(doc_contents), batch_size)):
        batch_texts = doc_contents[i:i+batch_size]
        output = model.encode(
            batch_texts,
            batch_size=batch_size,
            max_length=8192,
            return_dense=True,
            return_sparse=True
        )
        all_dense.append(output['dense_vecs'])
        all_sparse.extend(output['lexical_weights'])
        
    doc_dense_embs = np.vstack(all_dense).astype('float32')
    doc_sparse_embs = []
    for sparse_dict in all_sparse:
        doc_sparse_embs.append({k: float(v) for k, v in sparse_dict.items()})
        
    # FAISS Index ìƒì„±
    dim = doc_dense_embs.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(doc_dense_embs)
    
    # ì €ì¥
    np.save(DENSE_EMB_PATH, doc_dense_embs)
    with open(SPARSE_EMB_PATH, 'w') as f:
        json.dump(doc_sparse_embs, f)
    faiss.write_index(index, FAISS_INDEX_PATH)
    print("âœ… ì¸ë±ì‹± ì™„ë£Œ ë° ì €ì¥")

# Reranker ë¡œë“œ
print(f"â³ Reranker ë¡œë”© ì¤‘ ({RERANK_MODEL})...")
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device='cuda')

# ==========================================
# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (v9 SOTA ë¡œì§ ìœ ì§€)
# ==========================================

def get_multi_queries(messages):
    """v9 SOTA: Multi-Query + HyDE ê²°í•©"""
    # 1. Standalone Query ìƒì„±
    last_msg = messages[-1]["content"]
    history = "\n".join([f"{m['role']}: {m['content']}" for m in messages[:-1]])
    
    prompt = f"""ë‹¤ìŒ ëŒ€í™” ê¸°ë¡ê³¼ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ê²€ìƒ‰ ì—”ì§„ì— ì…ë ¥í•  ìµœì ì˜ 'ë…ë¦½ì ì¸ í•œêµ­ì–´ ê²€ìƒ‰ ì¿¼ë¦¬'ë¥¼ í•˜ë‚˜ ë§Œë“œì„¸ìš”.
ëŒ€í™” ë§¥ë½ì´ í•„ìš” ì—†ë‹¤ë©´ ì§ˆë¬¸ ê·¸ëŒ€ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{history}

[ì§ˆë¬¸]
{last_msg}

ë…ë¦½ì ì¸ ì¿¼ë¦¬:"""
    
    standalone_query = solar_client._call_with_retry(prompt).strip()
    
    # 2. HyDE ìƒì„±
    hyde_prompt = f"ì§ˆë¬¸: {standalone_query}\nìœ„ ì§ˆë¬¸ì— ëŒ€í•œ ê°€ìƒì˜ ê³¼í•™ì  ë‹µë³€ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
    hyde_answer = solar_client._call_with_retry(hyde_prompt).strip()
    
    return [standalone_query, hyde_answer]

def hybrid_search_multi(queries, top_k=100):
    """v9 SOTA: RRF Fusion for Multi-Query"""
    all_results = []
    
    for q in queries:
        # Encode query
        q_output = model.encode(q, return_dense=True, return_sparse=True)
        q_dense = q_output['dense_vecs'].reshape(1, -1).astype('float32')
        q_sparse = q_output['lexical_weights']
        
        # 1. Dense Search
        dense_scores, dense_indices = index.search(q_dense, top_k * 2)
        dense_scores = dense_scores[0]
        dense_indices = dense_indices[0]
        
        # 2. Sparse Search (Lexical)
        # BGE-M3ì˜ compute_lexical_matching ì‚¬ìš©
        sparse_scores = []
        for idx in dense_indices:
            score = model.compute_lexical_matching_score(q_sparse, doc_sparse_embs[idx])
            sparse_scores.append(score)
        sparse_scores = np.array(sparse_scores)
        
        # Normalize scores
        if dense_scores.max() > dense_scores.min():
            dense_scores = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min())
        if sparse_scores.max() > sparse_scores.min():
            sparse_scores = (sparse_scores - sparse_scores.min()) / (sparse_scores.max() - sparse_scores.min())
        
        # 3. Hybrid Fusion
        hybrid_scores = ALPHA * dense_scores + (1 - ALPHA) * sparse_scores
        sorted_indices = np.argsort(hybrid_scores)[::-1]
        query_top_indices = [dense_indices[i] for i in sorted_indices]
        all_results.append(query_top_indices)
        
    # 4. RRF Fusion
    rrf_scores = {}
    for results in all_results:
        for rank, idx in enumerate(results):
            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (RRF_K + rank)
            
    final_indices = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
    return final_indices[:top_k]

def generate_answer(query, context):
    prompt = f"""ë‹¹ì‹ ì€ ê³¼í•™ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ [ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

[ì§ˆë¬¸]
{query}

[ë¬¸ì„œ ë‚´ìš©]
{context}

ë‹µë³€:"""
    return solar_client._call_with_retry(prompt)

# ==========================================
# 4. ì‹¤í–‰
# ==========================================
print("ğŸƒ í‰ê°€ ì‹œì‘...")
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for i, entry in enumerate(tqdm(eval_data)):
        eval_id = entry["eval_id"]
        messages = entry["msg"]
        
        # 1) ê²Œì´íŒ… ì²´í¬
        if eval_id in EMPTY_IDS:
            res = {"eval_id": eval_id, "topk": [], "answer": "ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ì…ë‹ˆë‹¤."}
            f.write(json.dumps(res, ensure_ascii=False) + "\n")
            continue

        # Multi-Query Generation
        queries = get_multi_queries(messages)
        
        # Hybrid Search with RRF
        candidate_indices = hybrid_search_multi(queries, top_k=TOP_CANDIDATES)
        
        # Rerank
        if candidate_indices:
            rerank_query = queries[0]
            pairs = [[rerank_query, doc_contents[idx]] for idx in candidate_indices]
            rerank_scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)
            sorted_ranks = sorted(zip(candidate_indices, rerank_scores), key=lambda x: x[1], reverse=True)
            
            final_topk_indices = [idx for idx, _ in sorted_ranks[:FINAL_TOPK]]
            final_topk_ids = [doc_ids[idx] for idx in final_topk_indices]
            
            # Answer generation
            context = "\n".join([doc_contents[idx] for idx in final_topk_indices[:3]])
            answer = generate_answer(rerank_query, context)
            
            res = {
                "eval_id": eval_id,
                "standalone_query": rerank_query,
                "topk": final_topk_ids,
                "answer": answer
            }
        else:
            res = {"eval_id": eval_id, "topk": []}
                
        f.write(json.dumps(res, ensure_ascii=False) + "\n")
        f.flush()

print(f"âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ ì™„ë£Œ! ê²°ê³¼: {OUTPUT_FILE}")
