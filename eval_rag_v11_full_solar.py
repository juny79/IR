import os
import json
import numpy as np
import faiss
import torch
from pathlib import Path
from tqdm import tqdm
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder
from dotenv import load_dotenv
from models.solar_client import SolarClient

# .env ë¡œë“œ
load_dotenv()

# ==========================================
# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
OUTPUT_FILE = "/root/IR/submission_v11_sota.csv"

# ëª¨ë¸ ì„¤ì •
BGE_M3_MODEL = 'BAAI/bge-m3'
RERANK_MODEL = 'BAAI/bge-reranker-v2-m3'

# íŒŒë¼ë¯¸í„°
TOP_CANDIDATES = 200
FINAL_TOPK = 5
SOLAR_RERANK_TOPK = 10 
ALPHA = 0.5 
RRF_K = 60

# 0.9348 ê¸°ì¤€ ìµœì í™”ëœ ê²Œì´íŒ… ID
EMPTY_IDS = {
    276, 261, 283, 32, 94, 90, 220, 245, 229, 
    247, 67, 57, 2, 227, 301, 222, 83, 64, 103, 218
}

def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            data.append(json.loads(line))
    return data

print("ğŸš€ ë°ì´í„° ë¡œë”© ì¤‘...")
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

# ==========================================
# 2. ëª¨ë¸ ë¡œë”©
# ==========================================
print(f"â³ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = BGEM3FlagModel(BGE_M3_MODEL, use_fp16=True)
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device='cuda')
solar = SolarClient()

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)
CACHE_DIR = "/root/IR/cache/bge_m3"
DENSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_dense_embs.npy")
SPARSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_sparse_embs.json")
FAISS_INDEX_PATH = os.path.join(CACHE_DIR, "bge_m3_dense.index")

print("âœ… ìºì‹œëœ BGE-M3 ì¸ë±ìŠ¤ ë¡œë“œ")
doc_dense_embs = np.load(DENSE_EMB_PATH)
with open(SPARSE_EMB_PATH, 'r') as f:
    doc_sparse_embs = json.load(f)
index = faiss.read_index(FAISS_INDEX_PATH)

# ==========================================
# 3. ê²€ìƒ‰ ë° ì¬ì •ë ¬ í•¨ìˆ˜
# ==========================================
def hybrid_search(query_text, top_k=100):
    q_output = model.encode([query_text], return_dense=True, return_sparse=True, max_length=8192)
    q_dense = q_output['dense_vecs'][0].astype('float32')
    q_sparse = q_output['lexical_weights'][0]
    
    dense_scores, dense_indices = index.search(np.expand_dims(q_dense, 0), top_k)
    dense_indices = dense_indices[0]
    dense_scores = dense_scores[0]
    
    if len(dense_scores) > 0:
        d_min, d_max = dense_scores.min(), dense_scores.max()
        if d_max > d_min: dense_scores = (dense_scores - d_min) / (d_max - d_min)
        else: dense_scores = np.ones_like(dense_scores)
            
    sparse_scores = []
    for idx in dense_indices:
        score = model.compute_lexical_matching_score(q_sparse, doc_sparse_embs[idx])
        sparse_scores.append(score)
    sparse_scores = np.array(sparse_scores)
    
    if len(sparse_scores) > 0:
        s_min, s_max = sparse_scores.min(), sparse_scores.max()
        if s_max > s_min: sparse_scores = (sparse_scores - s_min) / (s_max - s_min)
        else: sparse_scores = np.ones_like(sparse_scores)
            
    hybrid_scores = ALPHA * dense_scores + (1 - ALPHA) * sparse_scores
    sorted_indices = np.argsort(hybrid_scores)[::-1]
    return [dense_indices[i] for i in sorted_indices]

def solar_rerank(query, candidates):
    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ì •ë³´ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì§ˆë¬¸(Query)ê³¼ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì„œ í›„ë³´(Candidate)ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ê° ë¬¸ì„œë¥¼ ê¼¼ê¼¼íˆ ì½ê³ , ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ì •í™•í•˜ê³  ì§ì ‘ì ì¸ í•´ë‹µì„ í¬í•¨í•˜ê³  ìˆëŠ” ë¬¸ì„œë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.
JSON í˜•ì‹ìœ¼ë¡œ {"best_index": 0} ì™€ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”."""

    candidate_text = ""
    for i, cand in enumerate(candidates):
        candidate_text += f"Candidate {i}:\n{cand}\n\n"
    
    user_prompt = f"Query: {query}\n\n{candidate_text}"
    
    try:
        response = solar._call_with_retry(
            [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        return json.loads(response)['best_index']
    except:
        return 0

# ==========================================
# 4. ì‹¤í–‰
# ==========================================
print(f"ğŸƒ í‰ê°€ ì‹œì‘... (ê²°ê³¼ íŒŒì¼: {OUTPUT_FILE})")

with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for item in tqdm(eval_data):
        eval_id = item["eval_id"]
        # msgê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ content ì¶”ì¶œ
        if isinstance(item["msg"], list):
            query = item["msg"][-1]["content"]
        else:
            query = item["msg"]
        
        if eval_id in EMPTY_IDS:
            res = {"eval_id": eval_id, "topk": []}
            f.write(json.dumps(res, ensure_ascii=False) + "\n")
            continue
            
        # 1. Hybrid Search
        candidate_indices = hybrid_search(query, top_k=TOP_CANDIDATES)
        
        # 2. BGE Reranker (Top 200 -> Top 10)
        pairs = [[query, doc_contents[idx]] for idx in candidate_indices]
        rerank_scores = reranker.predict(pairs)
        top_indices = [candidate_indices[i] for i in np.argsort(rerank_scores)[::-1]]
        
        # 3. Solar Super-Reranker (Top 10 -> Rank 1)
        solar_candidates = [doc_contents[idx] for idx in top_indices[:SOLAR_RERANK_TOPK]]
        best_idx = solar_rerank(query, solar_candidates)
        
        # Rank 1 êµì²´
        final_top_indices = top_indices[:FINAL_TOPK]
        if best_idx > 0 and best_idx < SOLAR_RERANK_TOPK:
            best_doc_idx = top_indices[best_idx]
            # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±° í›„ ë§¨ ì•ìœ¼ë¡œ
            if best_doc_idx in final_top_indices:
                final_top_indices.remove(best_doc_idx)
            final_top_indices = [best_doc_idx] + final_top_indices[:FINAL_TOPK-1]
            
        final_ids = [doc_ids[idx] for idx in final_top_indices]
        
        res = {
            "eval_id": eval_id,
            "topk": final_ids
        }
        f.write(json.dumps(res, ensure_ascii=False) + "\n")
        f.flush()

print(f"âœ… v11 SOTA ìƒì„± ì™„ë£Œ: {OUTPUT_FILE}")
