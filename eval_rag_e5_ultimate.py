import os
import json
import sys
import numpy as np
import faiss
import re
from pathlib import Path
from tqdm import tqdm
from kiwipiepy import Kiwi
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, CrossEncoder
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# LLM í´ë¼ì´ì–¸íŠ¸ë“¤
from models.solar_client import solar_client
from models.openai_client import openai_client

# ==========================================
# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
OUTPUT_FILE = "/root/IR/submission_e5_ultimate.csv"

# ëª¨ë¸ ì„¤ì •
EMBED_MODEL = "intfloat/multilingual-e5-large"
RERANK_MODEL = "BAAI/bge-reranker-v2-m3"

# íŒŒë¼ë¯¸í„°
RRF_K = 60
BM25_TOPN = 60
DENSE_TOPN = 60
TOP_CANDIDATES = 150
RERANK_BATCH = 32
FINAL_TOPK = 5
LLM_RERANK_TOPN = 10 # LLMì´ ë‹¤ì‹œ ë³¼ ìƒìœ„ ë¬¸ì„œ ìˆ˜

def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            data.append(json.loads(line))
    return data

print("ğŸš€ ë°ì´í„° ë¡œë”© ë° ì¸ë±ì‹± ì¤€ë¹„...")
sys.stdout.flush()
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

kiwi = Kiwi()
def tokenizer(text: str):
    tokens = kiwi.tokenize(text)
    return [t.form for t in tokens if t.tag.startswith("N") or t.tag in ["SL", "SN"]]

print("BM25 ì¸ë±ì‹±...")
sys.stdout.flush()
tokenized_corpus = [tokenizer(doc) for doc in doc_contents]
bm25 = BM25Okapi(tokenized_corpus)

print(f"Vector ì¸ë±ì‹± ({EMBED_MODEL})...")
sys.stdout.flush()
embedder = SentenceTransformer(EMBED_MODEL, device="cuda")
FAISS_CACHE_PATH = "/root/IR/cache/faiss_e5_large.index"
if os.path.exists(FAISS_CACHE_PATH):
    index = faiss.read_index(FAISS_CACHE_PATH)
else:
    print("FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
    sys.exit(1)

print(f"Reranker ë¡œë”© ({RERANK_MODEL})...")
sys.stdout.flush()
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device="cuda")

# ==========================================
# 2. í•µì‹¬ í•¨ìˆ˜ë“¤
# ==========================================
def get_queries(client, model_name, messages):
    system_prompt = """ë‹¹ì‹ ì€ RAGìš© ì§ˆë¬¸ ë¶„ì„ê¸°ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
{
  "should_search": true,
  "standalone_query": "êµ¬ì²´ì ì¸ ì§ˆë¬¸",
  "queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]
}"""
    try:
        if hasattr(client, 'model'): client.model = model_name
        resp = client._call_with_retry(
            prompt=[{"role": "system", "content": system_prompt}] + messages,
            temperature=0,
            max_tokens=1024,
            response_format={"type": "json_object"} if "gpt" in model_name else None
        )
        clean_text = resp.strip()
        if clean_text.startswith("```"):
            clean_text = clean_text.split("```")[1].replace("json", "").strip()
        parsed = json.loads(clean_text)
        return parsed.get("queries", [messages[-1]["content"]])[:3], parsed.get("standalone_query", messages[-1]["content"])
    except:
        return [messages[-1]["content"]], messages[-1]["content"]

def llm_rerank(query, docs, top_n=10):
    if not docs: return []
    
    doc_texts = ""
    for idx, doc in enumerate(docs[:top_n]):
        # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
        doc_texts += f"[{idx}] {doc[:600]}\n\n"
        
    prompt = f"""ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œì˜ ë²ˆí˜¸ë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
ì§ˆë¬¸: {query}

ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸:
{doc_texts}

ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë¶€í„° ë²ˆí˜¸ë§Œ ë‚˜ì—´í•˜ì„¸ìš”. (ì˜ˆ: 2, 0, 1, 3...)
ì¶œë ¥ì€ ë°˜ë“œì‹œ ìˆ«ìì™€ ì‰¼í‘œë¡œë§Œ êµ¬ì„±í•˜ì„¸ìš”. ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ë²ˆí˜¸ëŠ” ì“°ì§€ ë§ˆì„¸ìš”."""

    try:
        resp = openai_client._call_with_retry(
            prompt=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=100
        )
        order = [int(s) for s in re.findall(r'\d+', resp)]
        # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ í•„í„°ë§
        valid_order = [i for i in order if 0 <= i < len(docs[:top_n])]
        # ëˆ„ë½ëœ ì¸ë±ìŠ¤ ì¶”ê°€
        for i in range(len(docs[:top_n])):
            if i not in valid_order:
                valid_order.append(i)
        return valid_order
    except Exception as e:
        print(f"LLM Rerank Error: {e}")
        return list(range(len(docs[:top_n])))

def reciprocal_rank_fusion_weighted(rank_lists, k=60, weights=None):
    if weights is None: weights = [1.0] * len(rank_lists)
    scores = {}
    for w, rank_list in zip(weights, rank_lists):
        for rank, doc_idx in enumerate(rank_list):
            scores[doc_idx] = scores.get(doc_idx, 0.0) + w * (1.0 / (k + rank + 1))
    return sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

# ==========================================
# 3. ì‹¤í–‰
# ==========================================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for i, entry in enumerate(tqdm(eval_data)):
        eval_id = entry["eval_id"]
        messages = entry["msg"]
        
        # 1. ì¿¼ë¦¬ ìƒì„± (GPT-4o + Solar Pro)
        gpt_queries, main_q = get_queries(openai_client, "gpt-4o", messages)
        slr_queries, _ = get_queries(solar_client, "solar-pro", messages)
        
        all_queries = gpt_queries + slr_queries
        rank_lists = []
        
        # BM25 Search
        for q in all_queries:
            tokens = tokenizer(q)
            if tokens:
                scores = bm25.get_scores(tokens)
                rank_lists.append(np.argsort(scores)[::-1][:BM25_TOPN].tolist())
            else:
                rank_lists.append([])
        
        # Dense Search
        q_embs = embedder.encode(["query: " + q for q in all_queries], normalize_embeddings=True)
        for q_emb in q_embs:
            _, f_idx = index.search(np.expand_dims(q_emb, 0), DENSE_TOPN)
            rank_lists.append(f_idx[0].tolist())
            
        # RRF ë³‘í•© (ê°€ì¤‘ì¹˜ ì ìš©)
        current_weights = [0.7, 0.4, 0.4, 0.4, 0.2, 0.2, 1.8, 1.2, 1.2, 1.2, 0.8, 0.8]
        candidate_indices = reciprocal_rank_fusion_weighted(rank_lists, k=RRF_K, weights=current_weights)
        top_candidates = candidate_indices[:TOP_CANDIDATES]
        
        # 1ì°¨ Rerank (BGE-Reranker)
        pairs = [[main_q, doc_contents[idx]] for idx in top_candidates]
        rerank_scores = reranker.predict(pairs, batch_size=RERANK_BATCH, show_progress_bar=False)
        sorted_ranks = sorted(zip(top_candidates, rerank_scores), key=lambda x: x[1], reverse=True)
        
        # 2ì°¨ Rerank (GPT-4o ìµœì¢… ê²€ìˆ˜)
        top_bge_indices = [idx for idx, _ in sorted_ranks[:LLM_RERANK_TOPN]]
        top_bge_contents = [doc_contents[idx] for idx in top_bge_indices]
        
        new_order = llm_rerank(main_q, top_bge_contents, top_n=LLM_RERANK_TOPN)
        
        # ìµœì¢… ìˆœìœ„ ì¬ì¡°ì •
        final_top_idx = [top_bge_indices[i] for i in new_order]
        # ë‚˜ë¨¸ì§€ (10ìœ„ ì´í›„) ì¶”ê°€
        final_top_idx += [idx for idx, _ in sorted_ranks[LLM_RERANK_TOPN:]]
        
        final_topk = [doc_ids[idx] for idx in final_top_idx[:FINAL_TOPK]]
        
        res = {
            "eval_id": eval_id,
            "standalone_query": main_q,
            "topk": final_topk,
            "answer": doc_contents[final_top_idx[0]] if final_top_idx else ""
        }
        f.write(json.dumps(res, ensure_ascii=False) + "\n")
        f.flush()

print(f"ğŸ† ì–¼í‹°ë°‹ ì•™ìƒë¸” ì™„ë£Œ! ê²°ê³¼: {OUTPUT_FILE}")
