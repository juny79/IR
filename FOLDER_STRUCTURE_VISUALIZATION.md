# ğŸ“‚ í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡° ìƒì„¸ë„

## ğŸŒ³ ì „ì²´ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬

```
/root/IR/
â”‚
â”œâ”€â”€ ğŸ“ finetune/                                    # íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ ğŸ”µ 1_generate_qa.py                        # Stage 1: QA ìƒì„±
â”‚   â”œâ”€â”€ ğŸŸ¢ 2_mine_negatives_v3.py                  # Stage 2: Hard Negative Mining
â”‚   â”œâ”€â”€ ğŸŸ¡ 3_run_train_v3.sh                       # Stage 3: BGE-M3 í•™ìŠµ
â”‚   â”œâ”€â”€ ğŸ“Š 1_generate_qa.log                       # QA ìƒì„± ë¡œê·¸
â”‚   â”œâ”€â”€ ğŸ“Š 3_run_train.log                         # v1 í•™ìŠµ ë¡œê·¸ (268 steps)
â”‚   â””â”€â”€ ğŸ“Š train_v2.log                            # v2 í•™ìŠµ ë¡œê·¸ (402 steps)
â”‚
â”œâ”€â”€ ğŸ“ data/                                        # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ ğŸ“„ corpus.jsonl                            # ì›ë³¸ ë¬¸ì„œ (4,272ê°œ)
â”‚   â”œâ”€â”€ ğŸ“„ synthetic_qa_solar.jsonl                # ìƒì„± QA (12,816ê°œ)
â”‚   â”œâ”€â”€ ğŸ“„ train_data_v3.jsonl                     # í•™ìŠµ ë°ì´í„° (12,816ê°œ)
â”‚   â”œâ”€â”€ ğŸ“„ test.jsonl                              # í‰ê°€ ì§ˆë¬¸ (220ê°œ)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ finetuned_bge_m3/                            # v1 íŒŒì¸íŠœë‹ ëª¨ë¸
â”‚   â”œâ”€â”€ ğŸ† model.safetensors                       # 2.27GB ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ âš™ï¸ config.json                             # ëª¨ë¸ ì„¤ì •
â”‚   â”œâ”€â”€ ğŸ“ tokenizer_config.json                   # í† í¬ë‚˜ì´ì € ì„¤ì •
â”‚   â”œâ”€â”€ ğŸ“ tokenizer.json                          # í† í¬ë‚˜ì´ì €
â”‚   â”œâ”€â”€ ğŸ“ special_tokens_map.json                 # íŠ¹ìˆ˜ í† í°
â”‚   â””â”€â”€ ğŸ“ training_args.bin                       # í•™ìŠµ ì¸ì
â”‚
â”œâ”€â”€ ğŸ“ finetuned_bge_m3_v2/                         # v2 íŒŒì¸íŠœë‹ ëª¨ë¸ (402 steps)
â”‚   â”œâ”€â”€ ğŸ† model.safetensors                       # 2.27GB
â”‚   â””â”€â”€ ... (ë™ì¼ êµ¬ì¡°)
â”‚
â”œâ”€â”€ ğŸ“ finetuned_bge_m3_v3/                         # v3 íŒŒì¸íŠœë‹ ëª¨ë¸ (ìµœì¢…, 12K)
â”‚   â”œâ”€â”€ ğŸ† model.safetensors                       # 2.27GB
â”‚   â””â”€â”€ ... (ë™ì¼ êµ¬ì¡°)
â”‚
â”œâ”€â”€ ğŸ“„ eval_rag.py                                  # ë©”ì¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“„ eval_rag_finetuned.py                        # íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€
â”œâ”€â”€ ğŸ“„ eval_finetuned_v9.log                        # v9 í‰ê°€ ë¡œê·¸
â”œâ”€â”€ ğŸ“„ eval_rag_finetuned.log                       # íŒŒì¸íŠœë‹ í‰ê°€ ë¡œê·¸
â”‚
â”œâ”€â”€ ğŸ“„ submission_surgical_v1.csv                   # í˜„ì¬ ìµœê³  (MAP 0.9470)
â”œâ”€â”€ ğŸ“„ submission_54_bge_m3_sota.csv                # v1 í‰ê°€ (206KB)
â”œâ”€â”€ ğŸ“„ submission_55_bge_m3_sota.csv                # v2 í‰ê°€ (175KB)
â”œâ”€â”€ ğŸ“„ submission_56_bge_m3_sota_v3.csv             # v3 í‰ê°€ (178KB)
â”œâ”€â”€ ğŸ“„ submission_57_bge_m3_sota_v4.csv             # íŒŒë¼ë¯¸í„° ì¡°ì • (183KB)
â”œâ”€â”€ ğŸ“„ submission_58_bge_m3_sota_v5.csv             # íŒŒë¼ë¯¸í„° ì¡°ì • (176KB)
â”œâ”€â”€ ğŸ“„ submission_59_bge_m3_sota_v6.csv             # íŒŒë¼ë¯¸í„° ì¡°ì • (179KB)
â”œâ”€â”€ ğŸ“„ submission_60_bge_m3_sota_v7.csv             # íŒŒë¼ë¯¸í„° ì¡°ì • (188KB)
â”œâ”€â”€ ğŸ“„ submission_61_bge_m3_solar_sota.csv          # Solar í†µí•© (309KB)
â”œâ”€â”€ ğŸ“„ submission_88_ready_bge_m3_*.csv             # ìµœì¢… ì œì¶œ (107KB)
â”œâ”€â”€ ğŸ“„ submission_bge_m3_finetuned.csv              # ê¸°ë³¸ í‰ê°€ (415KB)
â”œâ”€â”€ ğŸ“„ submission_bge_m3_finetuned_v9.csv           # v9 í‰ê°€ (391KB)
â””â”€â”€ ... (20+ ë” ë§ì€ submission íŒŒì¼)
â”‚
â”œâ”€â”€ ğŸ“„ SYNTHETIC_FINETUNING_COMPREHENSIVE_REPORT.md # ì¢…í•© ë³´ê³ ì„œ
â”œâ”€â”€ ğŸ“„ FINETUNING_WORKFLOW_SUMMARY.md              # ì›Œí¬í”Œë¡œìš° ìš”ì•½
â”œâ”€â”€ ğŸ“„ LEADERBOARD_SUBMISSION_HISTORY.md           # ë¦¬ë”ë³´ë“œ ì´ë ¥
â”‚
â””â”€â”€ ... (ê¸°íƒ€ ë¶„ì„ ë° ì‹¤í—˜ íŒŒì¼)
```

---

## ğŸ” ì£¼ìš” ë””ë ‰í† ë¦¬ ì„¤ëª…

### 1. `/finetune/` - íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸
**ëª©ì **: í•©ì„± ë°ì´í„° ìƒì„± ë° ëª¨ë¸ í•™ìŠµ ìë™í™”

```
finetune/
â”œâ”€â”€ 1_generate_qa.py          # Solar Pro 2ë¡œ QA ìƒì„±
â”œâ”€â”€ 2_mine_negatives_v3.py    # BM25+Dense+Rerankerë¡œ Hard Negatives
â””â”€â”€ 3_run_train_v3.sh         # BGE-M3 Contrastive Learning
```

**ì›Œí¬í”Œë¡œìš°**:
```
Documents â†’ QA Generation â†’ Hard Negative Mining â†’ Model Training
```

---

### 2. `/data/` - ë°ì´í„° ë””ë ‰í† ë¦¬
**ëª©ì **: ì›ë³¸ ë¬¸ì„œ, ìƒì„± ë°ì´í„°, í•™ìŠµ ë°ì´í„° ì €ì¥

```
data/
â”œâ”€â”€ corpus.jsonl              # 4,272 documents
â”œâ”€â”€ synthetic_qa_solar.jsonl  # 12,816 QA pairs (3 Q per doc)
â”œâ”€â”€ train_data_v3.jsonl       # 12,816 samples (1 pos + 7 neg)
â””â”€â”€ test.jsonl                # 220 evaluation queries
```

**ë°ì´í„° ë³€í™˜**:
```
4,272 docs â†’ 12,816 QA â†’ 102,528 doc-query pairs
```

---

### 3. `/finetuned_bge_m3_*` - íŒŒì¸íŠœë‹ ëª¨ë¸
**ëª©ì **: í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ ì €ì¥

```
finetuned_bge_m3_v3/
â”œâ”€â”€ model.safetensors         # 2.27GB XLM-RoBERTa weights
â”œâ”€â”€ config.json               # Model configuration
â”œâ”€â”€ tokenizer*.json           # Tokenizer files
â””â”€â”€ training_args.bin         # Training arguments
```

**ëª¨ë¸ ë²„ì „**:
- **v1**: 4,272 samples, 2 epochs, 268 steps (ì´ˆê¸°)
- **v2**: 4,272 samples, 2+ epochs, 402 steps (ê°œì„ )
- **v3**: 12,816 samples, 5 epochs, ~1000+ steps (ìµœì¢…)

---

### 4. `/submission_*` - ì œì¶œ íŒŒì¼
**ëª©ì **: ë¦¬ë”ë³´ë“œ í‰ê°€ ê²°ê³¼ ì €ì¥

```
submission_*.csv íŒ¨í„´:
â”œâ”€â”€ submission_54-61_bge_m3_*.csv    # v1-v3 í‰ê°€ (8ê°œ)
â”œâ”€â”€ submission_88_*.csv              # ìµœì¢… ì œì¶œ
â”œâ”€â”€ submission_bge_m3_finetuned*.csv # ë‹¤ì–‘í•œ í‰ê°€ (2ê°œ)
â””â”€â”€ ... (ì´ 20+ íŒŒì¼)
```

**ì œì¶œ ì „ëµ**:
- ê° íŒŒì¼ì€ ì„œë¡œ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì¡°í•© í…ŒìŠ¤íŠ¸
- Hard Voting: [6,3,1], [7,4,2], [5,3,1] ë“±
- HyDE: Full, Sparse Only, None
- Reranker: Top-5, Top-10, Top-20

---

## ğŸ“Š íŒŒì¼ í¬ê¸° ë° í†µê³„

### ëª¨ë¸ íŒŒì¼
```
finetuned_bge_m3/           2.27GB
finetuned_bge_m3_v2/        2.27GB
finetuned_bge_m3_v3/        2.27GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ ëª¨ë¸ í¬ê¸°:               6.81GB
```

### ë°ì´í„° íŒŒì¼
```
corpus.jsonl                ~10MB   (4,272 docs)
synthetic_qa_solar.jsonl    ~15MB   (12,816 QA)
train_data_v3.jsonl         ~150MB  (12,816 samples Ã— 8 docs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ ë°ì´í„° í¬ê¸°:             ~175MB
```

### ì œì¶œ íŒŒì¼
```
submission_*.csv            48KB ~ 440KB (í‰ê·  ~180KB)
ì´ 20+ íŒŒì¼                 ~4MB
```

---

## ğŸ”¢ ë°ì´í„° ê·œëª¨ ìš”ì•½

| í•­ëª© | ìˆ˜ëŸ‰ | í¬ê¸° |
|------|------|------|
| **ì›ë³¸ ë¬¸ì„œ** | 4,272ê°œ | ~10MB |
| **ìƒì„± QA** | 12,816ê°œ | ~15MB |
| **í•™ìŠµ ìƒ˜í”Œ** | 12,816ê°œ | ~150MB |
| **íŒŒì¸íŠœë‹ ëª¨ë¸** | 3ê°œ | 6.81GB |
| **ì œì¶œ íŒŒì¼** | 20+ | ~4MB |
| **ì´ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰** | - | ~7.5GB |

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •
```bash
cd /root/IR
pip install -r requirements.txt
```

### 2ë‹¨ê³„: QA ìƒì„±
```bash
cd finetune
python 1_generate_qa.py
# â†’ data/synthetic_qa_solar.jsonl ìƒì„±
```

### 3ë‹¨ê³„: Hard Negative Mining
```bash
python 2_mine_negatives_v3.py
# â†’ data/train_data_v3.jsonl ìƒì„±
```

### 4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ
```bash
bash 3_run_train_v3.sh
# â†’ finetuned_bge_m3_v3/ ìƒì„±
```

### 5ë‹¨ê³„: í‰ê°€
```bash
cd ..
python eval_rag_finetuned.py
# â†’ submission_*.csv ìƒì„±
```

---

## ğŸ“ ì£¼ìš” íŒŒì¼ ìƒì„¸

### `finetune/1_generate_qa.py`
**ëª©ì **: Solar Pro 2 APIë¡œ ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸ ìƒì„±

**ì…ë ¥**:
- `data/corpus.jsonl` (4,272 docs)

**ì¶œë ¥**:
- `data/synthetic_qa_solar.jsonl` (12,816 QA pairs)

**í”„ë¡œì„¸ìŠ¤**:
```python
for each document:
    context = document[:1000]  # 1000ì ì œí•œ
    questions = solar_pro_2.generate(
        prompt="ë¬¸ì„œë¥¼ ì½ê³  3ê°œì˜ ì§ˆë¬¸ ìƒì„±",
        context=context
    )
    save_qa_pair(docid, questions, content)
```

---

### `finetune/2_mine_negatives_v3.py`
**ëª©ì **: Hybrid Retrievalë¡œ Hard Negatives 7ê°œ ì¶”ì¶œ

**ì…ë ¥**:
- `data/synthetic_qa_solar.jsonl` (12,816 QA pairs)

**ì¶œë ¥**:
- `data/train_data_v3.jsonl` (12,816 samples)

**í”„ë¡œì„¸ìŠ¤**:
```python
for each qa_pair:
    # 1. BM25 Sparse Search
    bm25_candidates = elasticsearch.search(query, top_k=50)
    
    # 2. Dense Search
    dense_candidates = faiss.search(query_embedding, top_k=50)
    
    # 3. Pool Merge
    pool = merge_and_dedupe(bm25_candidates, dense_candidates)
    
    # 4. Reranker
    reranked = bge_reranker.rerank(query, pool)
    hard_negatives = reranked[:7]
    
    save_training_sample(query, positive_doc, hard_negatives)
```

---

### `finetune/3_run_train_v3.sh`
**ëª©ì **: BGE-M3 Contrastive Learning ì‹¤í–‰

**ì…ë ¥**:
- `data/train_data_v3.jsonl` (12,816 samples)
- Base Model: `BAAI/bge-m3`

**ì¶œë ¥**:
- `finetuned_bge_m3_v3/` (2.27GB model)

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```bash
--num_train_epochs 5
--per_device_train_batch_size 2
--gradient_accumulation_steps 16  # effective batch = 32
--learning_rate 1e-5
--temperature 0.02
--fp16
```

---

### `eval_rag_finetuned.py`
**ëª©ì **: íŒŒì¸íŠœë‹ ëª¨ë¸ë¡œ í‰ê°€ ë° ì œì¶œ íŒŒì¼ ìƒì„±

**ì…ë ¥**:
- `finetuned_bge_m3_v3/` (í•™ìŠµëœ ëª¨ë¸)
- `data/test.jsonl` (220 queries)

**ì¶œë ¥**:
- `submission_*.csv` (220 rows)

**í”„ë¡œì„¸ìŠ¤**:
```python
# 1. Load fine-tuned model
model = load_finetuned_bge_m3("finetuned_bge_m3_v3")

# 2. Build index
index = build_faiss_index(corpus, model)

# 3. Evaluate
for query in test_queries:
    # HyDE expansion
    hyde_query = gemini_hyde(query)
    
    # Sparse + Dense retrieval
    bm25_results = bm25_search(hyde_query)
    dense_results = faiss_search(hyde_query, model, index)
    
    # Hard Voting
    voted = hard_vote(bm25_results, dense_results, weights=[6,3,1])
    
    # Reranker
    final = rerank(query, voted[:20], top_k=5)
    
    save_submission(query_id, final)
```

---

## ğŸ¯ íŒŒì¼ ì—­í•  ë§¤í•‘

| íŒŒì¼ | ì—­í•  | ì…ë ¥ | ì¶œë ¥ |
|------|------|------|------|
| `1_generate_qa.py` | QA ìƒì„± | corpus.jsonl | synthetic_qa_solar.jsonl |
| `2_mine_negatives_v3.py` | Hard Negative | synthetic_qa_solar.jsonl | train_data_v3.jsonl |
| `3_run_train_v3.sh` | ëª¨ë¸ í•™ìŠµ | train_data_v3.jsonl | finetuned_bge_m3_v3/ |
| `eval_rag_finetuned.py` | í‰ê°€ | test.jsonl + model | submission_*.csv |

---

## ğŸ’¡ íŒŒì¼ ëª…ëª… ê·œì¹™

### Submission íŒŒì¼
```
submission_{ë²ˆí˜¸}_{ëª¨ë¸}_{ë²„ì „}_{íŠ¹ì§•}.csv

ì˜ˆì‹œ:
- submission_54_bge_m3_sota.csv          # 54ë²ˆ ì œì¶œ, bge_m3, sota ì„¤ì •
- submission_56_bge_m3_sota_v3.csv       # v3 ëª¨ë¸ ì‚¬ìš©
- submission_61_bge_m3_solar_sota.csv    # Solar í†µí•©
- submission_88_ready_bge_m3_*.csv       # ìµœì¢… ì œì¶œ (88ë²ˆ)
```

### ëª¨ë¸ ë””ë ‰í† ë¦¬
```
finetuned_bge_m3_{ë²„ì „}/

ì˜ˆì‹œ:
- finetuned_bge_m3/           # v1 (ì´ˆê¸°)
- finetuned_bge_m3_v2/        # v2 (ê°œì„ )
- finetuned_bge_m3_v3/        # v3 (ìµœì¢…)
```

### ë°ì´í„° íŒŒì¼
```
{ëª©ì }_{ë²„ì „}.jsonl

ì˜ˆì‹œ:
- corpus.jsonl                # ì›ë³¸ (ë²„ì „ ì—†ìŒ)
- synthetic_qa_solar.jsonl    # Solarë¡œ ìƒì„±
- train_data_v3.jsonl         # v3 í•™ìŠµ ë°ì´í„°
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- **ì¢…í•© ë³´ê³ ì„œ**: [SYNTHETIC_FINETUNING_COMPREHENSIVE_REPORT.md](SYNTHETIC_FINETUNING_COMPREHENSIVE_REPORT.md)
- **ì›Œí¬í”Œë¡œìš° ìš”ì•½**: [FINETUNING_WORKFLOW_SUMMARY.md](FINETUNING_WORKFLOW_SUMMARY.md)
- **ë¦¬ë”ë³´ë“œ ì´ë ¥**: [LEADERBOARD_SUBMISSION_HISTORY.md](LEADERBOARD_SUBMISSION_HISTORY.md)

---

**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 29ì¼  
**ë²„ì „**: v1.0  
**ë¬¸ì„œ ìœ í˜•**: í´ë” êµ¬ì¡° ì‹œê°í™”
