import json
from models.llm_client import llm_client
from models.solar_client import solar_client
from retrieval.hybrid_search import run_hybrid_search
from retrieval.es_connector import es

# ğŸ¯ Phase 4D-NoGating: ê²Œì´íŒ… ì •ì±… ì œê±° í…ŒìŠ¤íŠ¸
# Phase 4D [5,4,2] ì„¤ì •ë§Œ ìœ ì§€, topk=[] ì •ì±… ì œê±°
VOTING_WEIGHTS = [5, 4, 2]  # Phase 4D ìµœê³ ì  ì„¤ì •
USE_MULTI_EMBEDDING = True  # SBERT + Gemini embedding ì¡°í•©
USE_GEMINI_ONLY = False
TOP_K_RETRIEVE = 50
USE_RRF = False
RRF_K = 60

def answer_question_optimized_no_gating(messages):
    res = {"standalone_query": "", "topk": [], "answer": ""}
    analysis = llm_client.analyze_query(messages)
    
    # â­ ê²Œì´íŒ… ì •ì±… ì œê±°: tool_calls ìƒê´€ì—†ì´ í•­ìƒ ê²€ìƒ‰ ìˆ˜í–‰
    query_text = ""
    if analysis.tool_calls:
        query_text = json.loads(analysis.tool_calls[0].function.arguments)['standalone_query']
    else:
        # ë¹„ê³¼í•™ ì§ˆë¬¸ë„ ì›ë³¸ ë©”ì‹œì§€ë¡œ ê²€ìƒ‰
        query_text = messages[0]['content']
    
    res["standalone_query"] = query_text
    
    # â­ Phase 4D: Solar Pro 2 HyDE
    hypothetical_answer = solar_client.generate_hypothetical_answer(query_text)
    
    if hypothetical_answer:
        hyde_query = f"{query_text}\n{hypothetical_answer}"
    else:
        hyde_query = query_text
    
    # Hybrid Search
    final_ranked_results = run_hybrid_search(
        original_query=query_text,
        sparse_query=hyde_query,
        reranker_query=query_text,
        voting_weights=VOTING_WEIGHTS,
        use_multi_embedding=USE_MULTI_EMBEDDING,
        top_k_retrieve=TOP_K_RETRIEVE,
        use_gemini_only=USE_GEMINI_ONLY,
        use_rrf=USE_RRF,
        rrf_k=RRF_K
    )
    
    # ê²€ìƒ‰ ê²°ê³¼ ì„¤ì •: í•­ìƒ ë°˜í™˜ (ê²Œì´íŒ… ì •ì±… ì—†ìŒ)
    res["topk"] = final_ranked_results[:5]
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±: Top-3 ë¬¸ì„œ ë‚´ìš© ì‚¬ìš©
    context_docs = []
    for docid in final_ranked_results[:3]:
        search_result = es.search(
            index="test",
            query={"term": {"docid": docid}},
            size=1
        )
        if search_result['hits']['hits']:
            context_docs.append(search_result['hits']['hits'][0]['_source']['content'])
    
    context = " ".join(context_docs)
    # Phase 4D: Solar Pro 2ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    res["answer"] = solar_client.generate_answer(messages, context)
    
    return res


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    test_messages = [{'role': 'user', 'content': 'ê´‘í•©ì„±ì´ë€?'}]
    print('=== Phase 4D-NoGating í…ŒìŠ¤íŠ¸ ===')
    print('ì„¤ì •:')
    print('  - Phase 4D ê¸°ë³¸ ì„¤ì • [5,4,2]')
    print('  - ê²Œì´íŒ… ì •ì±… ì œê±°')
    print('  - ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰\n')
    
    result = answer_question_optimized_no_gating(test_messages)
    print(f'ì›ë³¸ ì¿¼ë¦¬: {result["standalone_query"]}')
    print(f'Top-5 ë¬¸ì„œ: {len(result["topk"])}ê°œ')
    print(f'ë‹µë³€: {result["answer"][:100]}...')
