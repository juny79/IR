import os
import json
import numpy as np
import faiss
import torch
from pathlib import Path
from tqdm import tqdm
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder
from dotenv import load_dotenv
from models.solar_client import SolarClient

# .env ë¡œë“œ
load_dotenv()

# ==========================================
# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
FINETUNED_MODEL_PATH = "/root/IR/finetuned_bge_m3"
OUTPUT_FILE = "/root/IR/submission_bge_m3_finetuned_v9.csv"

# ëª¨ë¸ ì„¤ì •
RERANK_MODEL = 'BAAI/bge-reranker-v2-m3'

# íŒŒë¼ë¯¸í„° (v9 SOTA ê¸°ì¤€)
TOP_CANDIDATES = 200
FINAL_TOPK = 5
SOLAR_RERANK_TOPK = 10
ALPHA = 0.5 
RRF_K = 60

# 0.9409 ê¸°ì¤€ ê²Œì´íŒ… ID
EMPTY_IDS = {
    276, 261, 283, 32, 94, 90, 220, 245, 229, 
    247, 67, 57, 2, 227, 301, 222, 83, 64, 103, 218
}

def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            data.append(json.loads(line))
    return data

print("ðŸš€ ë°ì´í„° ë¡œë”© ì¤‘...")
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

# ==========================================
# 2. ëª¨ë¸ ë¡œë”© ë° ì¸ë±ì‹±
# ==========================================
print(f"â³ íŒŒì¸íŠœë‹ëœ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘ ({FINETUNED_MODEL_PATH})...")
model = BGEM3FlagModel(FINETUNED_MODEL_PATH, use_fp16=True)

# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ìš© ìºì‹œ ë””ë ‰í† ë¦¬
CACHE_DIR = "/root/IR/cache/bge_m3_finetuned"
os.makedirs(CACHE_DIR, exist_ok=True)
DENSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_dense_embs.npy")
SPARSE_EMB_PATH = os.path.join(CACHE_DIR, "doc_sparse_embs.json")
FAISS_INDEX_PATH = os.path.join(CACHE_DIR, "bge_m3_dense.index")

if os.path.exists(DENSE_EMB_PATH) and os.path.exists(SPARSE_EMB_PATH) and os.path.exists(FAISS_INDEX_PATH):
    print("âœ… ìºì‹œëœ íŒŒì¸íŠœë‹ BGE-M3 ì¸ë±ìŠ¤ ë¡œë“œ")
    doc_dense_embs = np.load(DENSE_EMB_PATH)
    with open(SPARSE_EMB_PATH, 'r') as f:
        doc_sparse_embs = json.load(f)
    index = faiss.read_index(FAISS_INDEX_PATH)
else:
    print("â³ íŒŒì¸íŠœë‹ BGE-M3 ì¸ë±ì‹± ìƒì„± ì¤‘...")
    batch_size = 16
    all_dense = []
    all_sparse = []
    
    for i in tqdm(range(0, len(doc_contents), batch_size)):
        batch_texts = doc_contents[i:i+batch_size]
        output = model.encode(
            batch_texts,
            batch_size=batch_size,
            max_length=8192,
            return_dense=True,
            return_sparse=True
        )
        all_dense.append(output['dense_vecs'])
        all_sparse.extend(output['lexical_weights'])
        
    doc_dense_embs = np.vstack(all_dense).astype('float32')
    doc_sparse_embs = [{k: float(v) for k, v in s.items()} for s in all_sparse]
    
    dim = doc_dense_embs.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(doc_dense_embs)
    
    np.save(DENSE_EMB_PATH, doc_dense_embs)
    with open(SPARSE_EMB_PATH, 'w') as f:
        json.dump(doc_sparse_embs, f)
    faiss.write_index(index, FAISS_INDEX_PATH)
    print("âœ… ì¸ë±ì‹± ì™„ë£Œ ë° ì €ìž¥")

print(f"â³ Reranker ë¡œë”© ì¤‘...")
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device="cuda")
solar_client = SolarClient(model_name="solar-pro")

# ==========================================
# 3. í•µì‹¬ í•¨ìˆ˜ë“¤ (v9 SOTA ë¡œì§)
# ==========================================
def get_multi_queries(messages):
    system_prompt = """ë‹¹ì‹ ì€ ê³¼í•™ ê²€ìƒ‰ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ì—”ì§„ì— ìž…ë ¥í•  '3ê°€ì§€ ë²„ì „ì˜ ê²€ìƒ‰ì–´'ë¥¼ JSONìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
{
    "queries": [
        "êµ¬ ì™„ê²°ëœ ì„œìˆ í˜• ì§ˆë¬¸ (ê°€ìž¥ ì¤‘ìš”)",
        "í•µì‹¬ í‚¤ì›Œë“œ ë‚˜ì—´ (ëª…ì‚¬ ì¤‘)",
        "ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ ì§ˆë¬¸"
    ]
}"""
    try:
        resp = solar_client._call_with_retry(
            prompt=[{"role": "system", "content": system_prompt}] + messages,
            temperature=0,
            max_tokens=512,
            response_format={"type": "json_object"}
        )
        parsed = json.loads(resp)
        queries = parsed.get("queries", [])
        original_q = messages[-1]["content"]
        if original_q not in queries:
            queries.append(original_q)
        return queries[:3]
    except:
        return [messages[-1]["content"]]

def hybrid_search_multi(queries, top_k=100):
    all_results = []
    for q_text in queries:
        q_output = model.encode([q_text], return_dense=True, return_sparse=True, max_length=8192)
        q_dense = q_output['dense_vecs'][0].astype('float32')
        q_sparse = q_output['lexical_weights'][0]
        
        dense_scores, dense_indices = index.search(np.expand_dims(q_dense, 0), top_k)
        dense_indices = dense_indices[0]
        dense_scores = dense_scores[0]
        
        if len(dense_scores) > 0:
            d_min, d_max = dense_scores.min(), dense_scores.max()
            if d_max > d_min: dense_scores = (dense_scores - d_min) / (d_max - d_min)
            else: dense_scores = np.ones_like(dense_scores)
                
        sparse_scores = []
        for idx in dense_indices:
            score = model.compute_lexical_matching_score(q_sparse, doc_sparse_embs[idx])
            sparse_scores.append(score)
        sparse_scores = np.array(sparse_scores)
        
        if len(sparse_scores) > 0:
            s_min, s_max = sparse_scores.min(), sparse_scores.max()
            if s_max > s_min: sparse_scores = (sparse_scores - s_min) / (s_max - s_min)
            else: sparse_scores = np.ones_like(sparse_scores)
                
        hybrid_scores = ALPHA * dense_scores + (1 - ALPHA) * sparse_scores
        sorted_indices = np.argsort(hybrid_scores)[::-1]
        all_results.append([dense_indices[i] for i in sorted_indices])
        
    rrf_scores = {}
    for results in all_results:
        for rank, idx in enumerate(results):
            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (RRF_K + rank)
            
    final_indices = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
    return final_indices[:top_k]

def solar_rerank_topk(query, candidates):
    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ì •ë³´ ê²€ìƒ‰ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. 
 ì§ˆë¬¸(Query)ê³¼ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì„œ í›„ë³´(Candidate)ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
 ê°€ìž¥ ì •í™•í•˜ê³ , ì§ì ‘ì ì¸ í•´ë‹µì„ í¬í•¨í•˜ê³  ìžˆëŠ” ë¬¸ì„œë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.

 JSON í˜•ì‹ìœ¼ë¡œ {"best_index": 0} ì™€ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”."""

    candidate_text = ""
    for i, content in enumerate(candidates):
        candidate_text += f"Candidate {i}:\n{content[:2000]}\n\n"
        
    user_prompt = f"## ì§ˆë¬¸:\n{query}\n\n## ê²€ìƒ‰ í›„ë³´:\n{candidate_text}"
    
    try:
        resp = solar_client._call_with_retry(
            prompt=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0,
            max_tokens=100,
            response_format={"type": "json_object"}
        )
        parsed = json.loads(resp)
        return int(parsed.get("best_index", 0))
    except:
        return 0

def generate_answer(query, context):
    system_prompt = "ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µí•˜ì§€ ë§ˆì„¸ìš”."
    user_prompt = f"ì§ˆë¬¸: {query}\n\në¬¸ë§¥:\n{context}"
    try:
        return solar_client._call_with_retry(
            prompt=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            temperature=0, max_tokens=1024
        )
    except: return "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ==========================================
# 4. ì‹¤í–‰
# ==========================================
print("ðŸƒ í‰ê°€ ì‹œìž‘...")
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for entry in tqdm(eval_data):
        eval_id = entry["eval_id"]
        messages = entry["msg"]
        
        if eval_id in EMPTY_IDS:
            f.write(json.dumps({"eval_id": eval_id, "topk": [], "answer": "ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ìž…ë‹ˆë‹¤."}, ensure_ascii=False) + "\n")
            f.flush()
            continue

        queries = get_multi_queries(messages)
        candidate_indices = hybrid_search_multi(queries, top_k=TOP_CANDIDATES)
        
        if candidate_indices:
            rerank_query = queries[0]
            pairs = [[rerank_query, doc_contents[idx]] for idx in candidate_indices]
            rerank_scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)
            sorted_ranks = sorted(zip(candidate_indices, rerank_scores), key=lambda x: x[1], reverse=True)
            
            top_indices = [idx for idx, _ in sorted_ranks[:SOLAR_RERANK_TOPK]]
            top_contents = [doc_contents[idx] for idx in top_indices]
            best_idx = solar_rerank_topk(rerank_query, top_contents)
            
            if best_idx >= len(top_indices) or best_idx < 0:
                best_idx = 0
                
            best_doc_idx = top_indices.pop(best_idx)
            final_indices = [best_doc_idx] + top_indices
            
            final_ids = [doc_ids[idx] for idx in final_indices[:FINAL_TOPK]]
            context = "\n".join([doc_contents[idx] for idx in final_indices[:3]])
            answer = generate_answer(rerank_query, context)
            
            res = {
                "eval_id": eval_id,
                "standalone_query": rerank_query,
                "topk": final_ids,
                "answer": answer
            }
        else:
            res = {"eval_id": eval_id, "topk": []}
                
        f.write(json.dumps(res, ensure_ascii=False) + "\n")
        f.flush()

print(f"âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ ì™„ë£Œ (v9 ë¡œì§): {OUTPUT_FILE}")
