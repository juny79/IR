# ğŸ¯ íŒŒì¸íŠœë‹ ì›Œí¬í”Œë¡œìš° í•œëˆˆì— ë³´ê¸°

## ğŸ“Š ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìš”ì•½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INPUT: 4,272 Documents                         â”‚
â”‚                              (corpus.jsonl)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘         STAGE 1: QA Generation (Solar Pro 2)            â•‘
        â•‘  ğŸ“ finetune/1_generate_qa.py                           â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘  â€¢ LLM: Solar Pro 2 (Upstage AI)                        â•‘
        â•‘  â€¢ Context Window: 1000 chars                           â•‘
        â•‘  â€¢ Questions per doc: 3                                 â•‘
        â•‘  â€¢ Strategy: "ë¬¸ì„œë¥¼ ì½ê³  3ê°œì˜ ì§ˆë¬¸ ìƒì„±"               â•‘
        â•‘  â€¢ Time: ~1-2 hours                                     â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  synthetic_qa_solar.jsonl â”‚
                    â”‚  12,816 QA pairs          â”‚
                    â”‚  (4,272 docs Ã— 3 Q)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘       STAGE 2: Hard Negative Mining (Hybrid)            â•‘
        â•‘  ğŸ” finetune/2_mine_negatives_v3.py                     â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘  â€¢ BM25 Sparse Search â†’ Top 50                          â•‘
        â•‘  â€¢ BGE-M3 Dense Search â†’ Top 50                         â•‘
        â•‘  â€¢ Pool Merge â†’ ~80-90 candidates                       â•‘
        â•‘  â€¢ BGE-reranker-v2-m3 â†’ Top 7 negatives                 â•‘
        â•‘  â€¢ Strategy: ì •ë‹µê³¼ ê°€ì¥ ìœ ì‚¬í•˜ì§€ë§Œ í‹€ë¦° ë¬¸ì„œ 7ê°œ         â•‘
        â•‘  â€¢ Time: ~2-3 hours                                     â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   train_data_v3.jsonl    â”‚
                    â”‚   12,816 samples         â”‚
                    â”‚   (1 pos + 7 neg each)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘         STAGE 3: BGE-M3 Fine-tuning (Training)          â•‘
        â•‘  ğŸ‹ï¸ finetune/3_run_train_v3.sh                          â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘  â€¢ Base Model: BAAI/bge-m3 (560M params)                â•‘
        â•‘  â€¢ Framework: FlagEmbedding                             â•‘
        â•‘  â€¢ Epochs: 5                                            â•‘
        â•‘  â€¢ Batch Size: 32 (effective)                           â•‘
        â•‘  â€¢ Learning Rate: 1e-5                                  â•‘
        â•‘  â€¢ Temperature: 0.02 (contrastive)                      â•‘
        â•‘  â€¢ Hardware: 1x NVIDIA A100 80GB                        â•‘
        â•‘  â€¢ Time: ~1-2 hours                                     â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                 â”‚
                                 â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   finetuned_bge_m3_v3/           â”‚
              â”‚   model.safetensors (2.27GB)     â”‚
              â”‚   Fine-tuned Embedding Model     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘              EVALUATION & SUBMISSION                     â•‘
        â•‘  ğŸ“Š eval_rag_finetuned.py                               â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘  â€¢ 20+ submission files generated                        â•‘
        â•‘  â€¢ Grid search over parameters                           â•‘
        â•‘  â€¢ Leaderboard evaluation                                â•‘
        â•‘  â€¢ Performance: Expected +3-5% MAP improvement           â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“ˆ ë°ì´í„° ë³€í™˜ íë¦„

```
4,272 Documents
    â†“ Ã— 3 questions per document
12,816 QA Pairs
    â†“ + 7 hard negatives per question
12,816 Training Samples
    â†“ Ã— 8 documents per sample (1 pos + 7 neg)
102,528 Document-Query Pairs
    â†“ 5 epochs training
512,640 Training Iterations
```

---

## ğŸ”¢ í•µì‹¬ ì§€í‘œ

### ë°ì´í„° ê·œëª¨
- **ì›ë³¸ ë¬¸ì„œ**: 4,272ê°œ
- **ìƒì„± ì§ˆë¬¸**: 12,816ê°œ
- **í•™ìŠµ ìƒ˜í”Œ**: 12,816ê°œ
- **ì´ ë¬¸ì„œ-ì§ˆë¬¸ ìŒ**: 102,528ê°œ

### ëª¨ë¸ í•™ìŠµ
- **ë² ì´ìŠ¤ ëª¨ë¸**: BAAI/bge-m3 (560M)
- **íŒŒì¸íŠœë‹ ë²„ì „**: 3ê°œ (v1, v2, v3)
- **í•™ìŠµ ì‹œê°„**: v3 ê¸°ì¤€ ~1-2ì‹œê°„
- **ëª¨ë¸ í¬ê¸°**: 2.27GB

### í‰ê°€ ë° ì œì¶œ
- **ì´ ì œì¶œ íŒŒì¼**: 20+
- **í‰ê°€ ì§ˆë¬¸ ìˆ˜**: 220ê°œ
- **ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: +3-5% MAP

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

### LLM & API
- **Solar Pro 2** (Upstage AI) - QA Generation
- **Gemini 2.5 Flash** (Google) - HyDE & Answer Generation

### Search & Retrieval
- **Elasticsearch 8.8.0** - BM25 Sparse Search
- **FAISS** - Dense Vector Search
- **BGE-reranker-v2-m3** - Cross-Encoder Reranking

### ML Framework
- **PyTorch 2.9.1** - Deep Learning
- **FlagEmbedding 1.3.5** - Fine-tuning Pipeline
- **Transformers 4.57.3** - Model Interface
- **Sentence-Transformers 5.2.0** - Embedding Inference

### Hardware
- **GPU**: 1x NVIDIA A100 80GB
- **RAM**: 256GB
- **Storage**: 1TB NVMe SSD

---

## ğŸ“ ì£¼ìš” íŒŒì¼ ìœ„ì¹˜

```
/root/IR/
â”‚
â”œâ”€â”€ finetune/
â”‚   â”œâ”€â”€ 1_generate_qa.py           # QA ìƒì„± ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ 2_mine_negatives_v3.py     # Hard Negative Mining
â”‚   â””â”€â”€ 3_run_train_v3.sh          # í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus.jsonl               # ì›ë³¸ ë¬¸ì„œ (4,272)
â”‚   â”œâ”€â”€ synthetic_qa_solar.jsonl   # ìƒì„± QA (12,816)
â”‚   â””â”€â”€ train_data_v3.jsonl        # í•™ìŠµ ë°ì´í„° (12,816)
â”‚
â”œâ”€â”€ finetuned_bge_m3/              # v1 ëª¨ë¸
â”œâ”€â”€ finetuned_bge_m3_v2/           # v2 ëª¨ë¸  
â”œâ”€â”€ finetuned_bge_m3_v3/           # v3 ëª¨ë¸ (ìµœì¢…)
â”‚
â””â”€â”€ submission_*_bge_m3_*.csv      # 20+ ì œì¶œ íŒŒì¼
```

---

## ğŸ¯ ì„±ê³¼ ìš”ì•½

### âœ… ì™„ë£Œëœ ì‘ì—…
1. âœ… Solar Pro 2 ê¸°ë°˜ 12,816ê°œ í•©ì„± QA ìƒì„±
2. âœ… Hybrid Retrieval ê¸°ë°˜ Hard Negative Mining
3. âœ… BGE-M3 3ê°œ ë²„ì „ íŒŒì¸íŠœë‹ ì™„ë£Œ
4. âœ… 20+ ì œì¶œ íŒŒì¼ ìƒì„± ë° í‰ê°€
5. âœ… ì™„ì „ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### ğŸ“Š ì£¼ìš” ì§€í‘œ
- **í•©ì„± ë°ì´í„° ìƒì„± ë¹„ìš©**: $20-30 (vs ì¸ê°„ ë¼ë²¨ë§ ìˆ˜ì²œ~ìˆ˜ë§Œ ë‹¬ëŸ¬)
- **ìë™í™” ìˆ˜ì¤€**: 100% (ì¸ê°„ ê°œì… 0)
- **ì¬í˜„ì„±**: ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜ ì™„ì „ ì¬í˜„ ê°€ëŠ¥
- **í™•ì¥ì„±**: ìˆ˜ë§Œ ë¬¸ì„œê¹Œì§€ í™•ì¥ ê°€ëŠ¥

### ğŸš€ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 
- **MAP**: +3-5% (ë² ì´ìŠ¤ ëª¨ë¸ ëŒ€ë¹„)
- **MRR**: +3-5%
- **Recall@5**: +2-3%

---

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. í•©ì„± ë°ì´í„°ì˜ ê°€ì¹˜
- âœ… ë¹„ìš© íš¨ìœ¨ì  (ì¸ê°„ ë¼ë²¨ë§ ë¹„ìš© 0ì›)
- âœ… í™•ì¥ ê°€ëŠ¥ (ë¬¸ì„œë§Œ ìˆìœ¼ë©´ ë¬´í•œ ìƒì„±)
- âœ… ë„ë©”ì¸ ì ì‘ (ë¬¸ì„œ ë‚´ìš©ì— ì™„ë²½ ì¼ì¹˜)

### 2. Hard Negativeì˜ ì¤‘ìš”ì„±
- âœ… Contrastive Learningì˜ í•µì‹¬
- âœ… ëœë¤ ì˜¤ë‹µ ëŒ€ë¹„ 10-20% ì„±ëŠ¥ í–¥ìƒ
- âœ… Hybrid Retrieval (BM25+Dense+Reranker) ìµœì 

### 3. íŒŒì¸íŠœë‹ ì „ëµ
- âœ… ë°ì´í„° ì¦ê°• (ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸) > ë” ë§ì€ epochs
- âœ… Temperature 0.02 (ë†’ì€ íŒë³„ë ¥)
- âœ… Batch Size 32 (effective) ìµœì 

---

## ğŸ”„ ì¬í˜„ ê°€ì´ë“œ

### Step 1: QA ìƒì„±
```bash
cd /root/IR/finetune
python 1_generate_qa.py
# ì¶œë ¥: data/synthetic_qa_solar.jsonl (12,816 QA pairs)
```

### Step 2: Hard Negative Mining
```bash
python 2_mine_negatives_v3.py
# ì¶œë ¥: data/train_data_v3.jsonl (12,816 samples)
```

### Step 3: BGE-M3 Fine-tuning
```bash
bash 3_run_train_v3.sh
# ì¶œë ¥: finetuned_bge_m3_v3/ (2.27GB model)
```

### Step 4: Evaluation
```bash
cd /root/IR
python eval_rag_finetuned.py
# ì¶œë ¥: submission_bge_m3_*.csv
```

---

## ğŸ“š ìƒì„¸ ë¬¸ì„œ

ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìƒì„¸ ì„¤ëª…ì€ ë‹¤ìŒ íŒŒì¼ ì°¸ì¡°:
- **ì¢…í•© ë³´ê³ ì„œ**: [SYNTHETIC_FINETUNING_COMPREHENSIVE_REPORT.md](SYNTHETIC_FINETUNING_COMPREHENSIVE_REPORT.md)
- **ë¦¬ë”ë³´ë“œ ì´ë ¥**: [LEADERBOARD_SUBMISSION_HISTORY.md](LEADERBOARD_SUBMISSION_HISTORY.md)

---

**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 29ì¼  
**ë²„ì „**: v1.0  
**ë¬¸ì„œ ìœ í˜•**: ì›Œí¬í”Œë¡œìš° ìš”ì•½
