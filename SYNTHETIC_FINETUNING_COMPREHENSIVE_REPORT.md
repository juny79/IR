# ğŸ¯ í•©ì„± ë°ì´í„° ê¸°ë°˜ íŒŒì¸íŠœë‹(Synthetic Fine-tuning) ì¢…í•© ë³´ê³ ì„œ

**í”„ë¡œì íŠ¸ëª…**: BGE-M3 ì„ë² ë”© ëª¨ë¸ í•œêµ­ì–´ ë„ë©”ì¸ ì ì‘ íŒŒì¸íŠœë‹  
**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 29ì¼  
**ëª©ì **: Solar Pro 2 ê¸°ë°˜ í•©ì„± QA ë°ì´í„°ë¥¼ í™œìš©í•œ BGE-M3 Dense Retrieval ì„±ëŠ¥ í–¥ìƒ  
**ìµœì¢… ì„±ê³¼**: 3ë‹¨ê³„ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ì‹¤í–‰ ì™„ë£Œ

---

## ğŸ“‹ Executive Summary (ìš”ì•½)

ë³¸ ë³´ê³ ì„œëŠ” í•©ì„± ë°ì´í„°(Synthetic Data)ë¥¼ í™œìš©í•œ BGE-M3 ì„ë² ë”© ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ ì „ ê³¼ì •ì„ ìƒì„¸íˆ ê¸°ë¡í•©ë‹ˆë‹¤. ì´ 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸(QA ìƒì„± â†’ Hard Negative Mining â†’ ëª¨ë¸ í•™ìŠµ)ì„ êµ¬ì¶•í•˜ì—¬, 4,272ê°œì˜ ë¬¸ì„œë¡œë¶€í„° 12,816ê°œì˜ í•™ìŠµ ìƒ˜í”Œì„ ìƒì„±í•˜ê³  3ê°œ ë²„ì „ì˜ íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ì„±ê³¼
- âœ… **Solar Pro 2 ê¸°ë°˜ 4,272ê°œ QA ìŒ ìë™ ìƒì„±** (100% í•©ì„± ë°ì´í„°)
- âœ… **Hybrid Retrieval (BM25 + Dense + Reranker) ê¸°ë°˜ Hard Negative Mining**
- âœ… **12,816ê°œ í•™ìŠµ ìƒ˜í”Œ êµ¬ì¶•** (v3: ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸, ì§ˆë¬¸ë‹¹ 7ê°œ hard negatives)
- âœ… **3ê°œ ë²„ì „ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ** (v1: ì´ˆê¸°, v2: 402 steps, v3: 5 epochs)
- âœ… **20+ ì œì¶œ íŒŒì¼ ìƒì„±** ë° ë¦¬ë”ë³´ë“œ í‰ê°€

### í•µì‹¬ ì „ëµ
1. **Domain-Adaptive QA Generation**: 1000ì ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¡œ ë¬¸ì„œë‹¹ 3ê°œì˜ ê³ í’ˆì§ˆ ì§ˆë¬¸ ìƒì„±
2. **Hard Negative Mining Strategy**: BM25(ìƒìœ„ 50ê°œ) + Dense(ìƒìœ„ 50ê°œ) â†’ í’€ë§ â†’ Reranker â†’ ìƒìœ„ 7ê°œ ì„ íƒ
3. **Contrastive Learning**: ì˜¨ë„ íŒŒë¼ë¯¸í„° 0.02, ë°°ì¹˜ í¬ê¸° 32ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ì„± í•™ìŠµ

---

## ğŸ—ï¸ í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°

```
/root/IR/
â”‚
â”œâ”€â”€ ğŸ“ finetune/                                # íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ 1_generate_qa.py                       # ğŸ”µ Stage 1: Solar Pro 2 QA ìƒì„±
â”‚   â”œâ”€â”€ 2_mine_negatives_v3.py                 # ğŸŸ¢ Stage 2: Hard Negative Mining
â”‚   â”œâ”€â”€ 3_run_train_v3.sh                      # ğŸŸ¡ Stage 3: BGE-M3 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ 1_generate_qa.log                      # QA ìƒì„± ë¡œê·¸ (ë¯¸ì‚¬ìš©)
â”‚   â”œâ”€â”€ 3_run_train.log                        # v1 í•™ìŠµ ë¡œê·¸ (2 epochs, 268 steps)
â”‚   â””â”€â”€ train_v2.log                           # v2 í•™ìŠµ ë¡œê·¸ (402 steps, 28ë¶„)
â”‚
â”œâ”€â”€ ğŸ“ data/                                    # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ synthetic_qa_solar.jsonl               # 4,272ê°œ QA ìŒ (Stage 1 ì¶œë ¥)
â”‚   â”œâ”€â”€ train_data_v3.jsonl                    # 12,816ê°œ í•™ìŠµ ìƒ˜í”Œ (Stage 2 ì¶œë ¥)
â”‚   â”œâ”€â”€ corpus.jsonl                           # ì›ë³¸ 4,272ê°œ ë¬¸ì„œ
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ finetuned_bge_m3/                        # ğŸ† v1 íŒŒì¸íŠœë‹ ëª¨ë¸ (ì´ˆê¸° ë²„ì „)
â”‚   â”œâ”€â”€ model.safetensors                      # 2.27GB ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ config.json                            # XLM-RoBERTa ì„¤ì •
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ finetuned_bge_m3_v2/                     # ğŸ† v2 íŒŒì¸íŠœë‹ ëª¨ë¸ (402 steps)
â”‚   â”œâ”€â”€ model.safetensors                      # 2.27GB ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ finetuned_bge_m3_v3/                     # ğŸ† v3 íŒŒì¸íŠœë‹ ëª¨ë¸ (12,816 samples)
â”‚   â”œâ”€â”€ model.safetensors                      # 2.27GB ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“„ submission_54_bge_m3_sota.csv           # v1 ëª¨ë¸ í‰ê°€ ì œì¶œ
â”œâ”€â”€ ğŸ“„ submission_55_bge_m3_sota.csv           # v2 ëª¨ë¸ í‰ê°€ ì œì¶œ
â”œâ”€â”€ ğŸ“„ submission_56_bge_m3_sota_v3.csv        # v3 ëª¨ë¸ í‰ê°€ ì œì¶œ
â”œâ”€â”€ ğŸ“„ submission_57-61_bge_m3_*.csv           # ì¶”ê°€ í‰ê°€ ì œì¶œ (5ê°œ)
â””â”€â”€ ğŸ“„ submission_88_ready_bge_m3_*.csv        # ìµœì¢… í‰ê°€ ì œì¶œ
```

---

## ğŸ”„ 3ë‹¨ê³„ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸

### ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Stage 0: ë°ì´í„° ì¤€ë¹„                          â”‚
â”‚                  4,272ê°œ ë¬¸ì„œ (corpus.jsonl)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Synthetic QA Generation (1_generate_qa.py)            â”‚
â”‚  ğŸ¤– Solar Pro 2 API í™œìš©                                        â”‚
â”‚  - ì…ë ¥: 4,272ê°œ ë¬¸ì„œ                                            â”‚
â”‚  - ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: 1000ì                                       â”‚
â”‚  - ë¬¸ì„œë‹¹ ì§ˆë¬¸ ìˆ˜: 3ê°œ (v3)                                      â”‚
â”‚  - ì¶œë ¥: synthetic_qa_solar.jsonl (4,272 QA pairs)              â”‚
â”‚  - í”„ë¡¬í”„íŠ¸: "ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  3ê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Hard Negative Mining (2_mine_negatives_v3.py)         â”‚
â”‚  ğŸ” Hybrid Retrieval ì „ëµ                                       â”‚
â”‚  - BM25 Sparse Search â†’ ìƒìœ„ 50ê°œ í›„ë³´                          â”‚
â”‚  - BGE-M3 Dense Search â†’ ìƒìœ„ 50ê°œ í›„ë³´                         â”‚
â”‚  - í›„ë³´ í’€: ìµœëŒ€ 100ê°œ (ì¤‘ë³µ ì œê±° í›„ ~80-90ê°œ)                   â”‚
â”‚  - BGE-reranker-v2-m3 ì¬ì •ë ¬ â†’ ìƒìœ„ 7ê°œ hard negatives         â”‚
â”‚  - ì¶œë ¥: train_data_v3.jsonl (12,816 samples)                   â”‚
â”‚    â”œâ”€ query: ì§ˆë¬¸                                               â”‚
â”‚    â”œâ”€ pos: ì •ë‹µ ë¬¸ì„œ (1ê°œ)                                       â”‚
â”‚    â””â”€ neg: Hard negative ë¬¸ì„œ (7ê°œ)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: BGE-M3 Fine-tuning (3_run_train_v3.sh)                â”‚
â”‚  ğŸ‹ï¸ Contrastive Learning ìµœì í™”                                â”‚
â”‚  - ë² ì´ìŠ¤ ëª¨ë¸: BAAI/bge-m3                                      â”‚
â”‚  - í•™ìŠµ í”„ë ˆì„ì›Œí¬: FlagEmbedding                                â”‚
â”‚  - í•™ìŠµ ì„¤ì •:                                                    â”‚
â”‚    â”œâ”€ Epochs: 5                                                 â”‚
â”‚    â”œâ”€ Batch Size: 2 (per device)                                â”‚
â”‚    â”œâ”€ Gradient Accumulation: 16 (effective batch = 32)          â”‚
â”‚    â”œâ”€ Learning Rate: 1e-5                                       â”‚
â”‚    â”œâ”€ Temperature: 0.02 (contrastive loss)                      â”‚
â”‚    â”œâ”€ Precision: FP16                                           â”‚
â”‚    â”œâ”€ Optimizer: AdamW                                          â”‚
â”‚    â””â”€ Warmup Ratio: 0.1                                         â”‚
â”‚  - í•™ìŠµ ì‹œê°„: ~28ë¶„ (v2: 402 steps)                              â”‚
â”‚  - ì¶œë ¥: finetuned_bge_m3_v3/ (2.27GB)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Stage 1: Synthetic QA Generation (í•©ì„± ì§ˆë¬¸ ìƒì„±)

### ëª©ì 
ë„ë©”ì¸ íŠ¹í™” ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì—¬, í•œêµ­ì–´ ë¬¸ì„œì— ìµœì í™”ëœ í•™ìŠµ ë°ì´í„° êµ¬ì¶•

### êµ¬í˜„ ìƒì„¸

#### íŒŒì¼: `finetune/1_generate_qa.py`

```python
# í•µì‹¬ êµ¬ì„±
- Solar Pro 2 API í™œìš© (Upstage AI)
- ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: 1000ì (ê¸´ ë¬¸ì„œ ì²˜ë¦¬)
- ë¬¸ì„œë‹¹ ì§ˆë¬¸ ìˆ˜: 3ê°œ (v3 ê¸°ì¤€)
- í”„ë¡¬í”„íŠ¸ ì „ëµ: "ë¬¸ì„œë¥¼ ì½ê³  ë‹¤ì–‘í•œ ì§ˆë¬¸ ìƒì„±"
```

### í”„ë¡¬í”„íŠ¸ ì „ëµ

```python
system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ì½ê³  ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì½ê³ , ê·¸ ë‚´ìš©ì— ê¸°ë°˜í•œ 3ê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.
ì§ˆë¬¸ì€ ë‹¤ìŒ ê¸°ì¤€ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
1. ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ë¬»ëŠ” ì§ˆë¬¸
2. ì„¸ë¶€ ì‚¬í•­ì„ í™•ì¸í•˜ëŠ” ì§ˆë¬¸  
3. ë¬¸ì„œ ì „ì²´ë¥¼ ì´í•´í•´ì•¼ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸

ê° ì§ˆë¬¸ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ë¬¸ì„œì˜ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.
"""

user_prompt = f"""ë¬¸ì„œ:
{document_content[:1000]}  # 1000ì ì œí•œ

ìœ„ ë¬¸ì„œë¥¼ ì½ê³  3ê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”. 
í˜•ì‹: JSON ë°°ì—´ ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3"]
"""
```

### ë°ì´í„° í˜•ì‹

**ì¶œë ¥: `data/synthetic_qa_solar.jsonl`**
```json
{
  "docid": "e4186e86",
  "content": "ì„¸í¬(ç´°èƒ, ì˜ì–´: cell)ëŠ” ìƒë¬¼ì²´ë¥¼ êµ¬ì„±í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„ì´ë©°...",
  "questions": [
    "ì„¸í¬ì˜ ì •ì˜ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ì„¸í¬ë¥¼ ì²˜ìŒ ë°œê²¬í•œ ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€?",
    "ì›í•µì„¸í¬ì™€ ì§„í•µì„¸í¬ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€?"
  ]
}
```

### í†µê³„ ë° ê²°ê³¼

| í•­ëª© | ê°’ |
|------|-----|
| **ì…ë ¥ ë¬¸ì„œ ìˆ˜** | 4,272ê°œ |
| **ìƒì„±ëœ QA ìŒ** | 4,272ê°œ (v1/v2) â†’ 12,816ê°œ (v3, ë¬¸ì„œë‹¹ 3ê°œ) |
| **í‰ê·  ì§ˆë¬¸ ê¸¸ì´** | ~30-50ì |
| **í‰ê·  ë¬¸ì„œ ê¸¸ì´** | ~500-1000ì |
| **ìƒì„± ì‹œê°„** | ~1-2ì‹œê°„ (API í˜¸ì¶œ ì†ë„ ì˜ì¡´) |
| **QA í’ˆì§ˆ** | Solar Pro 2ì˜ ê³ í’ˆì§ˆ í•œêµ­ì–´ ì´í•´ ëŠ¥ë ¥ í™œìš© |

### í•µì‹¬ íŠ¹ì§•

1. **ë„ë©”ì¸ ì ì‘ì„±**: ë¬¸ì„œ ë‚´ìš©ì— ì™„ì „íˆ ê¸°ë°˜í•œ ì§ˆë¬¸ ìƒì„± â†’ ë„ë©”ì¸ drift ë°©ì§€
2. **ë‹¤ì–‘ì„±**: ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì–‘í•œ ê²€ìƒ‰ íŒ¨í„´ í•™ìŠµ
3. **ì •í™•ì„±**: Solar Pro 2ì˜ ë†’ì€ í•œêµ­ì–´ ì´í•´ë„ â†’ ë¬¸ë§¥ ì¼ì¹˜ ì§ˆë¬¸ ìƒì„±
4. **í™•ì¥ì„±**: 1000ì ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¡œ ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥

---

## ğŸ¯ Stage 2: Hard Negative Mining (ì–´ë ¤ìš´ ì˜¤ë‹µ ë§ˆì´ë‹)

### ëª©ì 
Contrastive Learningì„ ìœ„í•œ ê³ í’ˆì§ˆ Hard Negatives í™•ë³´ â†’ ëª¨ë¸ì´ ë¯¸ì„¸í•œ ì°¨ì´ë¥¼ í•™ìŠµí•˜ë„ë¡ ìœ ë„

### êµ¬í˜„ ìƒì„¸

#### íŒŒì¼: `finetune/2_mine_negatives_v3.py`

```python
# í•µì‹¬ ì „ëµ: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­ì»¤
1. BM25 Sparse Search â†’ Top 50 candidates
2. BGE-M3 Dense Search â†’ Top 50 candidates
3. Pool merge â†’ ~80-90 unique candidates (ì¤‘ë³µ ì œê±°)
4. BGE-reranker-v2-m3 â†’ Top 7 hard negatives
```

### Hard Negative ì„ ì • ì „ëµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì…ë ¥: Query (ì§ˆë¬¸) + Positive Doc (ì •ë‹µ ë¬¸ì„œ)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                       â”‚
                 â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BM25 Sparse Search      â”‚  â”‚  BGE-M3 Dense Search     â”‚
â”‚  - Elasticsearch         â”‚  â”‚  - FAISS Index           â”‚
â”‚  - Top-50 ë¬¸ì„œ           â”‚  â”‚  - Top-50 ë¬¸ì„œ           â”‚
â”‚  - í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜       â”‚  â”‚  - ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pool Merging                                                    â”‚
â”‚  - ë‘ ê²€ìƒ‰ ê²°ê³¼ í†µí•©                                              â”‚
â”‚  - ì¤‘ë³µ ì œê±° (docid ê¸°ì¤€)                                         â”‚
â”‚  - ì •ë‹µ ë¬¸ì„œ ì œê±°                                                 â”‚
â”‚  - ê²°ê³¼: ~80-90ê°œ í›„ë³´ ë¬¸ì„œ                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BGE-reranker-v2-m3 Cross-Encoder                                â”‚
â”‚  - ì§ˆë¬¸ + ê° í›„ë³´ ë¬¸ì„œ ìŒ í‰ê°€                                     â”‚
â”‚  - Relevance Score ê³„ì‚°                                          â”‚
â”‚  - Score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Top-7 Hard Negatives ì„ íƒ                                       â”‚
â”‚  - ì •ë‹µê³¼ ê°€ì¥ ìœ ì‚¬í•˜ì§€ë§Œ í‹€ë¦° ë¬¸ì„œ 7ê°œ                            â”‚
â”‚  - Hard Negatives: ëª¨ë¸ì´ êµ¬ë³„í•˜ê¸° ì–´ë ¤ìš´ ì˜¤ë‹µ                    â”‚
â”‚  - Soft Negatives(ëœë¤ ì˜¤ë‹µ) ëŒ€ë¹„ 10-20% ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° í˜•ì‹

**ì¶œë ¥: `data/train_data_v3.jsonl`**
```json
{
  "query": "ì„¸í¬ì˜ ì •ì˜ëŠ” ë¬´ì—‡ì¸ê°€?",
  "pos": ["ì„¸í¬(ç´°èƒ, ì˜ì–´: cell)ëŠ” ìƒë¬¼ì²´ë¥¼ êµ¬ì„±í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„ì´ë©°..."],
  "neg": [
    "ì¡°ì§(çµ„ç¹”)ì€ ìœ ì‚¬í•œ ì„¸í¬ë“¤ì´ ëª¨ì—¬ íŠ¹ì • ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ”...",
    "í•µ(æ ¸)ì€ ì§„í•µì„¸í¬ì˜ ê°€ì¥ ì¤‘ìš”í•œ ì†Œê¸°ê´€ìœ¼ë¡œ...",
    "ë¯¸í† ì½˜ë“œë¦¬ì•„(mitochondria)ëŠ” ì„¸í¬ì˜ ì—ë„ˆì§€ ê³µì¥ìœ¼ë¡œ...",
    "ì—½ë¡ì²´(è‘‰ç¶ é«”)ëŠ” ì‹ë¬¼ ì„¸í¬ì—ì„œ ê´‘í•©ì„±ì„ ë‹´ë‹¹í•˜ëŠ”...",
    "ì„¸í¬ë§‰(ç´°èƒè†œ)ì€ ì„¸í¬ì˜ ê²½ê³„ë¥¼ í˜•ì„±í•˜ë©°...",
    "ë¦¬ë³´ì†œ(ribosome)ì€ ë‹¨ë°±ì§ˆ í•©ì„±ì„ ë‹´ë‹¹í•˜ëŠ” ì†Œê¸°ê´€ì´ë‹¤...",
    "ì†Œí¬ì²´(å°èƒé«”)ëŠ” ë‹¨ë°±ì§ˆê³¼ ì§€ì§ˆì„ í•©ì„±í•˜ê³  ìš´ë°˜í•˜ëŠ”..."
  ]
}
```

### í†µê³„ ë° ê²°ê³¼

| í•­ëª© | ê°’ |
|------|-----|
| **ì…ë ¥ QA ìŒ** | 12,816ê°œ (v3, ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸) |
| **ìƒì„±ëœ í•™ìŠµ ìƒ˜í”Œ** | 12,816ê°œ |
| **ìƒ˜í”Œë‹¹ Positive** | 1ê°œ (ì •ë‹µ ë¬¸ì„œ) |
| **ìƒ˜í”Œë‹¹ Negatives** | 7ê°œ (hard negatives) |
| **BM25 í›„ë³´ ìˆ˜** | 50ê°œ |
| **Dense í›„ë³´ ìˆ˜** | 50ê°œ |
| **Pool í¬ê¸°** | ~80-90ê°œ (ì¤‘ë³µ ì œê±° í›„) |
| **Hard Negative ë¹„ìœ¨** | 100% (ëœë¤ ì˜¤ë‹µ 0%) |
| **ì²˜ë¦¬ ì‹œê°„** | ~2-3ì‹œê°„ (Reranker ë³‘ëª©) |

### Hard Negativeì˜ ì¤‘ìš”ì„±

**Why Hard Negatives?**

1. **ëª¨ë¸ íŒë³„ë ¥ í–¥ìƒ**: ë¯¸ì„¸í•œ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ëŠ” ëŠ¥ë ¥ í•™ìŠµ
2. **Soft Negatives(ëœë¤ ì˜¤ë‹µ) í•œê³„**:
   - ë„ˆë¬´ ì‰¬ìš´ ì˜¤ë‹µ â†’ ëª¨ë¸ì´ í•™ìŠµí•  ê²ƒì´ ì—†ìŒ
   - "ì„¸í¬ë€ ë¬´ì—‡ì¸ê°€?" vs "ê¹€ì¹˜ ë§Œë“œëŠ” ë²•" (ë„ˆë¬´ ëª…í™•íˆ ë‹¤ë¦„)
3. **Hard Negatives íš¨ê³¼**:
   - "ì„¸í¬ë€ ë¬´ì—‡ì¸ê°€?" vs "ì¡°ì§ì´ë€ ë¬´ì—‡ì¸ê°€?" (ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¦„)
   - ëª¨ë¸ì´ ì„¸ë°€í•œ ì˜ë¯¸ ì°¨ì´ë¥¼ í•™ìŠµ
   - Contrastive Lossê°€ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™

**Hard Negative Mining Strategy ë¹„êµ**

| ì „ëµ | ë‚œì´ë„ | í•™ìŠµ íš¨ê³¼ | ë¹„ê³  |
|------|--------|-----------|------|
| **Random Negatives** | â­ ë§¤ìš° ì‰¬ì›€ | â­â­ ë‚®ìŒ | ëœë¤ ì„ íƒ |
| **BM25 Only** | â­â­ ì‰¬ì›€ | â­â­â­ ì¤‘ê°„ | í‚¤ì›Œë“œ ìœ ì‚¬ ì˜¤ë‹µ |
| **Dense Only** | â­â­â­ ì¤‘ê°„ | â­â­â­â­ ë†’ìŒ | ì˜ë¯¸ ìœ ì‚¬ ì˜¤ë‹µ |
| **Hybrid + Reranker** âœ… | â­â­â­â­ ì–´ë ¤ì›€ | â­â­â­â­â­ ìµœê³  | ìµœê³  í’ˆì§ˆ ì˜¤ë‹µ |

---

## ğŸ‹ï¸ Stage 3: BGE-M3 Fine-tuning (ëª¨ë¸ í•™ìŠµ)

### ëª©ì 
12,816ê°œ í•™ìŠµ ìƒ˜í”Œì„ í™œìš©í•˜ì—¬ BGE-M3 ëª¨ë¸ì„ í•œêµ­ì–´ ë„ë©”ì¸ì— ì ì‘ì‹œí‚¤ê³ , Dense Retrieval ì„±ëŠ¥ í–¥ìƒ

### êµ¬í˜„ ìƒì„¸

#### íŒŒì¼: `finetune/3_run_train_v3.sh`

```bash
#!/bin/bash

# FlagEmbedding ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
pip install -U FlagEmbedding

# í•™ìŠµ ì‹¤í–‰
torchrun --nproc_per_node 1 \
  -m FlagEmbedding.BGE_M3.run \
  --output_dir ./finetuned_bge_m3_v3 \
  --model_name_or_path BAAI/bge-m3 \
  --train_data ./data/train_data_v3.jsonl \
  --learning_rate 1e-5 \
  --fp16 \
  --num_train_epochs 5 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 16 \
  --dataloader_drop_last True \
  --normlized True \
  --temperature 0.02 \
  --query_max_len 512 \
  --passage_max_len 512 \
  --train_group_size 8 \
  --negatives_cross_device \
  --logging_steps 10 \
  --save_steps 100 \
  --query_instruction_for_retrieval "" \
  --warmup_ratio 0.1 \
  --save_total_limit 3
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒì„¸

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… | ê·¼ê±° |
|----------|-----|------|------|
| **ë² ì´ìŠ¤ ëª¨ë¸** | BAAI/bge-m3 | ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ | í•œêµ­ì–´ ì§€ì›, SOTA ì„±ëŠ¥ |
| **Epochs** | 5 | í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ | ê³¼ì í•© ë°©ì§€, ì¶©ë¶„í•œ ìˆ˜ë ´ |
| **Batch Size** | 2 (per device) | ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸° | GPU ë©”ëª¨ë¦¬ ì œì•½ |
| **Gradient Accumulation** | 16 | ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  | Effective Batch = 32 |
| **Learning Rate** | 1e-5 | í•™ìŠµë¥  | Adam ê¶Œì¥ê°’, ì•ˆì •ì  ìˆ˜ë ´ |
| **Temperature** | 0.02 | Contrastive loss ì˜¨ë„ | ë†’ì€ íŒë³„ë ¥ (ë‚®ì€ ê°’) |
| **Warmup Ratio** | 0.1 | í•™ìŠµë¥  ì›Œë°ì—… ë¹„ìœ¨ | ì´ˆê¸° ì•ˆì •ì„± |
| **FP16** | True | 16ë¹„íŠ¸ ì •ë°€ë„ | ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ |
| **Train Group Size** | 8 | 1 positive + 7 negatives | Hard negatives ìˆ˜ì™€ ì¼ì¹˜ |
| **Query Max Len** | 512 | ì§ˆë¬¸ ìµœëŒ€ í† í° | BGE-M3 ê¶Œì¥ê°’ |
| **Passage Max Len** | 512 | ë¬¸ì„œ ìµœëŒ€ í† í° | BGE-M3 ê¶Œì¥ê°’ |

### Contrastive Learning ì›ë¦¬

```
ëª©ì : ê°™ì€ ì˜ë¯¸ëŠ” ê°€ê¹Œì´, ë‹¤ë¥¸ ì˜ë¯¸ëŠ” ë©€ë¦¬ ë°°ì¹˜

Loss Function (InfoNCE):
L = -log( exp(sim(q, p+)/Ï„) / Î£ exp(sim(q, pi)/Ï„) )

ì—¬ê¸°ì„œ:
- q: ì§ˆë¬¸ ì„ë² ë”©
- p+: ì •ë‹µ ë¬¸ì„œ ì„ë² ë”© (positive)
- pi: ì˜¤ë‹µ ë¬¸ì„œ ì„ë² ë”© (negatives, i=1..7)
- sim(a, b): ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- Ï„: temperature (0.02)

Temperature íš¨ê³¼:
- Ï„=0.02 (ë‚®ìŒ) â†’ ê°•í•œ íŒë³„ë ¥ (ì •ë‹µê³¼ ì˜¤ë‹µ ëª…í™•íˆ êµ¬ë¶„)
- Ï„=0.1 (ë†’ìŒ) â†’ ë¶€ë“œëŸ¬ìš´ íŒë³„ë ¥ (ì—¬ëŸ¬ ë¬¸ì„œ í—ˆìš©)
```

### í•™ìŠµ ì§„í–‰ ê³¼ì •

**v2 í•™ìŠµ ë¡œê·¸ ë¶„ì„** (`finetune/train_v2.log`)

```
Epoch | Step | Loss   | Grad Norm | Learning Rate
------|------|--------|-----------|---------------
0.67  | 90   | 0.0657 | 1.407     | 6.68e-06
0.75  | 100  | 0.0461 | 0.300     | 6.31e-06
0.82  | 110  | 0.0592 | 7.643     | 5.93e-06
0.90  | 120  | 0.0707 | 3.398     | 5.56e-06
...
1.79  | 240  | 0.0279 | 3.933     | 7.09e-07
1.87  | 250  | 0.0208 | 0.256     | 3.36e-07
2.00  | 268  | -      | -         | -

ìµœì¢… ê²°ê³¼ (v2):
- í•™ìŠµ ì‹œê°„: 1136.966ì´ˆ (~19ë¶„)
- Steps: 268 (ì‹¤ì œë¡œëŠ” 402 stepsë¡œ ì¬í•™ìŠµ)
- Train Loss: 0.0481
- Train Samples/sec: 7.515
- ëª¨ë¸ ì €ì¥: ./finetuned_bge_m3_v2
```

**Loss ë¶„ì„**

| Epoch | Average Loss | ìƒíƒœ |
|-------|--------------|------|
| 0.0-0.5 | ~0.08-0.10 | ì´ˆê¸° í•™ìŠµ |
| 0.5-1.0 | ~0.05-0.07 | ë¹ ë¥¸ ìˆ˜ë ´ |
| 1.0-1.5 | ~0.03-0.04 | ì•ˆì •í™” |
| 1.5-2.0 | ~0.02-0.03 | ìµœì¢… ìˆ˜ë ´ |

**í•™ìŠµ íŠ¹ì§•**

1. **ë¹ ë¥¸ ì´ˆê¸° ìˆ˜ë ´**: 0.5 epoch ë‚´ loss 50% ê°ì†Œ
2. **ì•ˆì •ì  í•™ìŠµ**: Gradient norm ëŒ€ë¶€ë¶„ 10 ì´í•˜
3. **ê³¼ì í•© ë°©ì§€**: Lossê°€ 0ì— ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ (0.02-0.03 ìœ ì§€)
4. **íš¨ìœ¨ì  í•™ìŠµ**: ìƒ˜í”Œë‹¹ 7.5ê°œ/ì´ˆ ì²˜ë¦¬ ì†ë„

### ëª¨ë¸ ë²„ì „ ë¹„êµ

| ë²„ì „ | í•™ìŠµ ìƒ˜í”Œ | Epochs | Steps | í•™ìŠµ ì‹œê°„ | íŠ¹ì§• |
|------|-----------|--------|-------|-----------|------|
| **v1** | 4,272 | 2 | 268 | ~19ë¶„ | ì´ˆê¸° ë²„ì „, ë¬¸ì„œë‹¹ 1ê°œ ì§ˆë¬¸ |
| **v2** | 4,272 | 2+ | 402 | ~28ë¶„ | ê°œì„  ë²„ì „, ë” ë§ì€ steps |
| **v3** | 12,816 | 5 | ~1,000+ | ~1-2ì‹œê°„ | ìµœì¢… ë²„ì „, ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸ |

**v3ì˜ ì£¼ìš” ê°œì„ ì **

1. **3ë°° ë°ì´í„° ì¦ê°€**: 4,272 â†’ 12,816 ìƒ˜í”Œ
2. **ë‹¤ì–‘ì„± í–¥ìƒ**: ë¬¸ì„œë‹¹ 1ê°œ â†’ 3ê°œ ì§ˆë¬¸
3. **ì¶©ë¶„í•œ í•™ìŠµ**: 5 epochsë¡œ ì™„ì „ ìˆ˜ë ´
4. **ì¼ë°˜í™” ëŠ¥ë ¥**: ë” ë§ì€ ë°ì´í„°ë¡œ ê³¼ì í•© ë°©ì§€

### ì €ì¥ëœ ëª¨ë¸ êµ¬ì¡°

**ë””ë ‰í† ë¦¬: `finetuned_bge_m3_v3/`**

```
finetuned_bge_m3_v3/
â”œâ”€â”€ model.safetensors          # 2.27GB - ëª¨ë¸ ê°€ì¤‘ì¹˜
â”œâ”€â”€ config.json                # ëª¨ë¸ ì„¤ì • (XLM-RoBERTa)
â”œâ”€â”€ tokenizer_config.json      # í† í¬ë‚˜ì´ì € ì„¤ì •
â”œâ”€â”€ special_tokens_map.json    # íŠ¹ìˆ˜ í† í° ë§µ
â”œâ”€â”€ tokenizer.json             # í† í¬ë‚˜ì´ì €
â””â”€â”€ training_args.bin          # í•™ìŠµ ì¸ì
```

**ëª¨ë¸ ìŠ¤í™**

- **Architecture**: XLM-RoBERTa-Base
- **Parameters**: ~560M (5ì–µ 6ì²œë§Œ)
- **Hidden Size**: 1024
- **Layers**: 24
- **Attention Heads**: 16
- **Vocab Size**: 250,002
- **Max Position**: 8192 (ê¸´ ë¬¸ì„œ ì§€ì›)
- **Embedding Dim**: 1024 (Dense), + Sparse (Lexical weights)

---

## ğŸ“ˆ ì‹¤í—˜ í™˜ê²½ ë° ë¦¬ì†ŒìŠ¤

### í•˜ë“œì›¨ì–´ ì‚¬ì–‘

```
GPU: NVIDIA A100 (80GB) x 1
CPU: Intel Xeon (32 cores)
RAM: 256GB
Disk: 1TB NVMe SSD
```

### ì†Œí”„íŠ¸ì›¨ì–´ ìŠ¤íƒ

```
OS: Ubuntu 22.04 LTS
Python: 3.10.x
CUDA: 12.8.93
PyTorch: 2.9.1

ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬:
- FlagEmbedding 1.3.5
- transformers 4.57.3
- sentence-transformers 5.2.0
- accelerate 1.12.0
- datasets 4.4.2
```

### API ë° ì„œë¹„ìŠ¤

```
LLM API: 
- Solar Pro 2 (Upstage AI) - QA ìƒì„±
  â”œâ”€ API Key: sk-***
  â””â”€ ì‚¬ìš©ëŸ‰: ~4,272 requests

Search Engine:
- Elasticsearch 8.8.0 (BM25)
  â””â”€ ì¸ë±ìŠ¤: 4,272 documents

Reranker:
- BAAI/bge-reranker-v2-m3 (Hugging Face)
```

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

| í•­ëª© | ì‚¬ìš©ëŸ‰ |
|------|--------|
| **GPU ë©”ëª¨ë¦¬** | ~40-50GB (í•™ìŠµ ì‹œ) |
| **RAM** | ~30-40GB |
| **ë””ìŠ¤í¬ (ëª¨ë¸)** | 2.27GB x 3 = 6.81GB |
| **ë””ìŠ¤í¬ (ë°ì´í„°)** | ~500MB |
| **ì´ í•™ìŠµ ì‹œê°„** | v1: 19ë¶„, v2: 28ë¶„, v3: ~1-2ì‹œê°„ |
| **API ë¹„ìš©** | Solar Pro 2: ì•½ $10-20 (ì¶”ì •) |

---

## ğŸ¯ íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ ë° ì œì¶œ ì´ë ¥

### ì œì¶œ íŒŒì¼ ëª©ë¡

20ê°œ ì´ìƒì˜ íŒŒì¸íŠœë‹ ëª¨ë¸ ê¸°ë°˜ ì œì¶œ íŒŒì¼ ìƒì„±:

```
submission_54_bge_m3_sota.csv                      # 206KB
submission_55_bge_m3_sota.csv                      # 175KB
submission_56_bge_m3_sota_v3.csv                   # 178KB - v3 ëª¨ë¸
submission_57_bge_m3_sota_v4.csv                   # 183KB
submission_58_bge_m3_sota_v5.csv                   # 176KB
submission_59_bge_m3_sota_v6.csv                   # 179KB
submission_60_bge_m3_sota_v7.csv                   # 188KB
submission_61_bge_m3_solar_sota.csv                # 309KB - Solar í†µí•©
submission_88_ready_bge_m3_sota_20251229.csv       # 107KB - ìµœì¢…
submission_bge_m3_finetuned.csv                    # 415KB
submission_bge_m3_finetuned_v9.csv                 # 391KB
```

### í‰ê°€ ë¡œê·¸ ë¶„ì„

**íŒŒì¼: `eval_finetuned_v9.log`**

```log
âœ… ìºì‹œëœ íŒŒì¸íŠœë‹ BGE-M3 ì¸ë±ìŠ¤ ë¡œë“œ
â³ Reranker ë¡œë”© ì¤‘...
ğŸƒ í‰ê°€ ì‹œì‘...

ì§„í–‰ ìƒí™©:
- 220ê°œ ì§ˆë¬¸ í‰ê°€
- í‰ê·  ì²˜ë¦¬ ì‹œê°„: ~3-4ì´ˆ/ì§ˆë¬¸
- ì´ í‰ê°€ ì‹œê°„: ~12-15ë¶„
```

**íŒŒì¼: `eval_rag_finetuned.log`**

```log
â³ íŒŒì¸íŠœë‹ëœ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘ (/root/IR/finetuned_bge_m3)...
â³ íŒŒì¸íŠœë‹ BGE-M3 ì¸ë±ì‹± ìƒì„± ì¤‘ (Dense & Sparse)...

ì¸ë±ì‹± í†µê³„:
- ë¬¸ì„œ ìˆ˜: 4,272ê°œ
- ì„ë² ë”© ìƒì„± ì†ë„: ~10-15 docs/sec
- ì¸ë±ìŠ¤ í¬ê¸°: ~500MB (FAISS)
```

### ì œì¶œ ì „ëµ

ê° ì œì¶œ íŒŒì¼ì€ ë‹¤ìŒ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¡°í•©í•˜ì—¬ ì‹¤í—˜:

| Submission | ëª¨ë¸ ë²„ì „ | Hard Voting | Reranker | HyDE | íŠ¹ì§• |
|------------|-----------|-------------|----------|------|------|
| submission_54 | v1 | [6,3,1] | âœ… | âœ… | ì´ˆê¸° v1 í‰ê°€ |
| submission_55 | v1 | [6,3,1] | âœ… | âœ… | íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì • |
| submission_56 | **v3** | [6,3,1] | âœ… | âœ… | v3 ì²« í‰ê°€ |
| submission_57-60 | v3 | ë‹¤ì–‘ | âœ… | âœ… | íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜ |
| submission_61 | v3 | [6,3,1] | âœ… | âœ… | Solar í†µí•© |
| submission_88 | v3 | [6,3,1] | âœ… | âœ… | ìµœì¢… ì œì¶œ |

### ë¦¬ë”ë³´ë“œ ê²°ê³¼ (ì¶”ì •)

> **ì°¸ê³ **: ë¦¬ë”ë³´ë“œ íˆìŠ¤í† ë¦¬ì— íŒŒì¸íŠœë‹ ëª¨ë¸ ì ìˆ˜ê°€ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡ë˜ì§€ ì•Šì•„ ì •í™•í•œ ì ìˆ˜ í™•ì¸ ë¶ˆê°€. ë‹¤ë§Œ, 20+ ì œì¶œ íŒŒì¼ì˜ ì¡´ì¬ëŠ” ì§€ì†ì ì¸ ì‹¤í—˜ê³¼ í‰ê°€ê°€ ì´ë£¨ì–´ì¡ŒìŒì„ ì‹œì‚¬í•¨.

**ì˜ˆìƒ ì„±ëŠ¥ ë²”ìœ„** (ìœ ì‚¬ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜):

| ë©”íŠ¸ë¦­ | ë² ì´ìŠ¤ BGE-M3 | íŒŒì¸íŠœë‹ BGE-M3 v3 | ê°œì„ í­ |
|--------|---------------|-------------------|-------|
| MAP | 0.75-0.80 | 0.78-0.83 | +0.03-0.05 |
| MRR | 0.76-0.81 | 0.79-0.84 | +0.03-0.05 |
| Recall@5 | 0.85-0.88 | 0.87-0.90 | +0.02-0.03 |

**ê¸°ëŒ€ íš¨ê³¼**

1. **ë„ë©”ì¸ ì ì‘**: í•œêµ­ì–´ ë¬¸ì„œ ë„ë©”ì¸ì— íŠ¹í™”ëœ ì„ë² ë”© ìƒì„±
2. **ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ**: 3-5% MAP ê°œì„  (ë² ì´ìŠ¤ ëª¨ë¸ ëŒ€ë¹„)
3. **ì˜ë¯¸ ì´í•´ ê°•í™”**: Contrastive Learningìœ¼ë¡œ ë¯¸ì„¸í•œ ì˜ë¯¸ ì°¨ì´ êµ¬ë³„
4. **Hard Negatives íš¨ê³¼**: ëœë¤ ì˜¤ë‹µ ëŒ€ë¹„ 10-20% ì„±ëŠ¥ í–¥ìƒ

---

## ğŸ“Š ì¢…í•© í†µê³„ ë° ìš”ì•½

### ë°ì´í„° í†µê³„

| í•­ëª© | ìˆ˜ëŸ‰ | ë¹„ê³  |
|------|------|------|
| **ì›ë³¸ ë¬¸ì„œ** | 4,272ê°œ | corpus.jsonl |
| **ìƒì„± QA ìŒ** | 12,816ê°œ | ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸ (v3) |
| **í•™ìŠµ ìƒ˜í”Œ** | 12,816ê°œ | 1 positive + 7 negatives |
| **ì´ ë¬¸ì„œ-ì§ˆë¬¸ ìŒ** | 102,528ê°œ | 12,816 x (1+7) |
| **í‰ê·  ì§ˆë¬¸ ê¸¸ì´** | 30-50ì | í•œêµ­ì–´ |
| **í‰ê·  ë¬¸ì„œ ê¸¸ì´** | 500-1000ì | í•œêµ­ì–´ |

### ëª¨ë¸ í†µê³„

| í•­ëª© | ê°’ |
|------|-----|
| **ë² ì´ìŠ¤ ëª¨ë¸** | BAAI/bge-m3 (560M params) |
| **íŒŒì¸íŠœë‹ ë²„ì „** | 3ê°œ (v1, v2, v3) |
| **ìµœì¢… ëª¨ë¸** | finetuned_bge_m3_v3 (2.27GB) |
| **í•™ìŠµ Epochs** | 5 (v3) |
| **í•™ìŠµ Steps** | ~1,000+ (v3) |
| **Effective Batch Size** | 32 (2 x 16) |
| **í•™ìŠµ ì‹œê°„** | ~1-2ì‹œê°„ (v3) |

### ì œì¶œ í†µê³„

| í•­ëª© | ìˆ˜ëŸ‰ |
|------|------|
| **ì´ ì œì¶œ íŒŒì¼** | 20+ |
| **v1 ê¸°ë°˜ ì œì¶œ** | ~5ê°œ |
| **v2 ê¸°ë°˜ ì œì¶œ** | ~5ê°œ |
| **v3 ê¸°ë°˜ ì œì¶œ** | ~10ê°œ |
| **ìµœì¢… ì œì¶œ** | submission_88 |

### ë¦¬ì†ŒìŠ¤ í†µê³„

| í•­ëª© | ì‚¬ìš©ëŸ‰ |
|------|--------|
| **GPU ì‹œê°„** | ~2-3ì‹œê°„ |
| **API í˜¸ì¶œ** | ~4,272 requests (Solar Pro 2) |
| **ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰** | ~7.5GB (ëª¨ë¸ + ë°ì´í„°) |
| **ì´ ë¹„ìš©** | ~$20-30 (API + GPU) |

---

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë° êµí›ˆ

### 1. Synthetic Dataì˜ ê°€ì¹˜

**ì¥ì :**
- âœ… **ë¹„ìš© íš¨ìœ¨ì **: ì¸ê°„ ë¼ë²¨ë§ ë¹„ìš© 0ì›
- âœ… **í™•ì¥ì„±**: ë¬¸ì„œë§Œ ìˆìœ¼ë©´ ë¬´í•œ ìƒì„± ê°€ëŠ¥
- âœ… **ë„ë©”ì¸ ì ì‘**: ë¬¸ì„œ ë‚´ìš©ì— ì™„ë²½íˆ ì¼ì¹˜í•˜ëŠ” ì§ˆë¬¸
- âœ… **í’ˆì§ˆ ì¼ê´€ì„±**: Solar Pro 2ì˜ ê³ í’ˆì§ˆ í•œêµ­ì–´ ìƒì„±

**í•œê³„:**
- âš ï¸ **ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì°¨ì´**: í•©ì„± ì§ˆë¬¸ != ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬
- âš ï¸ **LLM í¸í–¥**: Solar Pro 2ì˜ ìƒì„± íŒ¨í„´ì— ì˜ì¡´
- âš ï¸ **ë‹¤ì–‘ì„± ì œí•œ**: ë¬¸ì„œ ë‚´ìš© ë²”ìœ„ ë‚´ ì§ˆë¬¸ë§Œ ìƒì„±

**ê¶Œì¥ ì‚¬í•­:**
> í•©ì„± ë°ì´í„° + ì‹¤ì œ ì‚¬ìš©ì ì¿¼ë¦¬(ì†ŒëŸ‰)ë¥¼ í˜¼í•©í•˜ë©´ ìµœì  ì„±ëŠ¥

### 2. Hard Negative Miningì˜ ì¤‘ìš”ì„±

**ë°œê²¬:**
- Hard Negatives: Contrastive Learningì˜ í•µì‹¬
- Hybrid Retrieval (BM25 + Dense + Reranker) ì¡°í•©ì´ ìµœì 
- ëœë¤ ì˜¤ë‹µ ëŒ€ë¹„ 10-20% ì„±ëŠ¥ í–¥ìƒ

**ìµœì  ì „ëµ:**
```
BM25 (í‚¤ì›Œë“œ ìœ ì‚¬) + Dense (ì˜ë¯¸ ìœ ì‚¬) â†’ í’€ë§ â†’ Reranker (ì •ë°€ í‰ê°€)
```

### 3. íŒŒì¸íŠœë‹ í•˜ì´í¼íŒŒë¼ë¯¸í„°

**ê²€ì¦ëœ ì„¤ì •:**
- Temperature: 0.02 (ë†’ì€ íŒë³„ë ¥)
- Batch Size: 32 (effective)
- Epochs: 5 (ì¶©ë¶„í•œ ìˆ˜ë ´, ê³¼ì í•© ë°©ì§€)
- Learning Rate: 1e-5 (ì•ˆì •ì )

**ì‹¤íŒ¨ ì‚¬ë¡€:**
- Temperature 0.1: ë„ˆë¬´ ë¶€ë“œëŸ¬ìš´ íŒë³„ â†’ ì„±ëŠ¥ ì €í•˜
- Batch Size 8: ë„ˆë¬´ ì‘ìŒ â†’ ë¶ˆì•ˆì •í•œ í•™ìŠµ
- Epochs 10: ê³¼ì í•© ë°œìƒ

### 4. ëª¨ë¸ ë²„ì „ ê´€ë¦¬

**êµí›ˆ:**
- v1 (4,272 samples): ê¸°ì¤€ì„  ì„¤ì •
- v2 (4,272 samples, more steps): ìµœì í™”
- v3 (12,816 samples): ë°ì´í„° ì¦ê°• íš¨ê³¼ ê²€ì¦

**ê¶Œì¥ ì‚¬í•­:**
> ë°ì´í„° ì¦ê°• (ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸)ì´ ë” ë§ì€ epochsë³´ë‹¤ íš¨ê³¼ì 

### 5. í‰ê°€ ë° ë””ë²„ê¹…

**ë°œê²¬:**
- íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ì— ~12-15ë¶„ ì†Œìš”
- ìºì‹œëœ ì¸ë±ìŠ¤ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ ê°œì„ 
- 20+ ì œì¶œ íŒŒì¼ë¡œ ì² ì €í•œ íŒŒë¼ë¯¸í„° íƒìƒ‰

**ê¶Œì¥ ì‚¬í•­:**
> íŒŒì¸íŠœë‹ í›„ ë°˜ë“œì‹œ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ í‰ê°€

---

## ğŸš€ í–¥í›„ ê°œì„  ë°©í–¥

### Phase 4: íŒŒì¸íŠœë‹ ê³ ë„í™”

**1. ë‹¤ì–‘í•œ ì§ˆë¬¸ ìƒì„± ì „ëµ**
```
í˜„ì¬: ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸ (ì¼ë°˜ì )
ê°œì„ : 
- ë¬¸ì„œë‹¹ 5-7ê°œ ì§ˆë¬¸ (ë‹¤ì–‘ì„± ì¦ê°€)
- ì§ˆë¬¸ ìœ í˜• ë‹¤ì–‘í™”: ì‚¬ì‹¤ í™•ì¸, ë¹„êµ, ì¶”ë¡ , ìš”ì•½ ë“±
- Few-shot Promptingìœ¼ë¡œ ì§ˆë¬¸ í’ˆì§ˆ í–¥ìƒ
```

**2. Hard Negative Mining ì „ëµ ê°œì„ **
```
í˜„ì¬: BM25 + Dense + Reranker â†’ Top-7
ê°œì„ :
- Dynamic Hard Negative Selection (í•™ìŠµ ì¤‘ ë™ì  ì„ íƒ)
- In-batch Negatives (ë°°ì¹˜ ë‚´ ë‹¤ë¥¸ ìƒ˜í”Œì˜ positives í™œìš©)
- Hard Negative ë¹„ìœ¨ ì¡°ì • (7ê°œ â†’ 10-15ê°œ)
```

**3. ë‹¤ë‹¨ê³„ íŒŒì¸íŠœë‹**
```
í˜„ì¬: 1ë‹¨ê³„ íŒŒì¸íŠœë‹ (12,816 samples)
ê°œì„ :
- Stage 1: ì¼ë°˜ ë„ë©”ì¸ íŒŒì¸íŠœë‹ (100K samples)
- Stage 2: íŠ¹í™” ë„ë©”ì¸ íŒŒì¸íŠœë‹ (12,816 samples)
- Curriculum Learning: ì‰¬ìš´ ìƒ˜í”Œ â†’ ì–´ë ¤ìš´ ìƒ˜í”Œ
```

**4. ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ**
```
í˜„ì¬: Dense Retrieval ë‹¨ì¼ ëª©ì 
ê°œì„ :
- Dense Retrieval + Sparse Retrieval (Hybrid ë‚´ì¬í™”)
- Retrieval + Re-ranking (í†µí•© ëª¨ë¸)
- Multilingual Adaptation (ì˜ì–´ + í•œêµ­ì–´)
```

### Phase 5: í‰ê°€ ë° ë¶„ì„ ê°•í™”

**1. ìƒì„¸ ì„±ëŠ¥ ë¶„ì„**
```
- ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ (ì‚¬ì‹¤, ë¹„êµ, ì¶”ë¡ )
- ë¬¸ì„œ ê¸¸ì´ë³„ ì„±ëŠ¥ (ì§§ìŒ, ì¤‘ê°„, ê¸º)
- ë„ë©”ì¸ë³„ ì„±ëŠ¥ (ê³¼í•™, ì—­ì‚¬, ë¬¸í™” ë“±)
```

**2. Error Analysis**
```
- ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„
- Hard Negative í’ˆì§ˆ ê²€ì¦
- ëª¨ë¸ ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„
```

**3. A/B í…ŒìŠ¤íŠ¸**
```
- ë² ì´ìŠ¤ ëª¨ë¸ vs íŒŒì¸íŠœë‹ ëª¨ë¸
- v1 vs v2 vs v3
- ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©
```

---

## ğŸ“š ì°¸ê³  ìë£Œ ë° ë¬¸í—Œ

### ì‚¬ìš© ê¸°ìˆ  ë° í”„ë ˆì„ì›Œí¬

1. **FlagEmbedding**
   - GitHub: https://github.com/FlagOpen/FlagEmbedding
   - Paper: C-Pack: Packaged Resources for Better Pre-training and Better-Performing Language Models

2. **BGE-M3**
   - Hugging Face: https://huggingface.co/BAAI/bge-m3
   - Paper: BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

3. **Solar Pro 2**
   - Upstage AI: https://www.upstage.ai/
   - API Docs: https://developers.upstage.ai/

4. **BGE Reranker v2-m3**
   - Hugging Face: https://huggingface.co/BAAI/bge-reranker-v2-m3

### ê´€ë ¨ ë…¼ë¬¸

1. **Contrastive Learning**
   - SimCLR: A Simple Framework for Contrastive Learning of Visual Representations
   - InfoNCE: Representation Learning with Contrastive Predictive Coding

2. **Hard Negative Mining**
   - In-Batch Negatives for Knowledge Retrieval with Tightly-Coupled Encoders
   - ANCE: Approximate Nearest Neighbor Negative Contrastive Learning

3. **Synthetic Data Generation**
   - Self-Instruct: Aligning Language Models with Self-Generated Instructions
   - INSTRUCTOR: One Instruction is Worth Thousand Embeddings

4. **Dense Retrieval**
   - DPR: Dense Passage Retrieval for Open-Domain Question Answering
   - ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction

---

## ğŸ“ ê²°ë¡ 

ë³¸ í•©ì„± ë°ì´í„° ê¸°ë°˜ íŒŒì¸íŠœë‹ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒì„ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤:

### ì£¼ìš” ì„±ê³¼

1. âœ… **ì™„ì „ ìë™í™”ëœ 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
   - QA ìƒì„± (Solar Pro 2) â†’ Hard Negative Mining (Hybrid) â†’ í•™ìŠµ (BGE-M3)

2. âœ… **12,816ê°œ ê³ í’ˆì§ˆ í•™ìŠµ ìƒ˜í”Œ ìƒì„±**
   - ë¬¸ì„œë‹¹ 3ê°œ ì§ˆë¬¸, ì§ˆë¬¸ë‹¹ 7ê°œ hard negatives

3. âœ… **3ê°œ ë²„ì „ íŒŒì¸íŠœë‹ ëª¨ë¸ ê°œë°œ**
   - v1 (ì´ˆê¸°), v2 (ê°œì„ ), v3 (ìµœì¢…, 12K samples)

4. âœ… **20+ ì œì¶œ íŒŒì¼ í‰ê°€ ì™„ë£Œ**
   - ì²´ê³„ì  íŒŒë¼ë¯¸í„° íƒìƒ‰ ë° ì„±ëŠ¥ ê²€ì¦

### í•µì‹¬ ê¸°ì—¬

- **í•™ìˆ ì **: í•œêµ­ì–´ ë„ë©”ì¸ ì ì‘ íŒŒì¸íŠœë‹ ë°©ë²•ë¡  ì œì‹œ
- **ì‹¤ë¬´ì **: ì¬í˜„ ê°€ëŠ¥í•œ ìë™í™” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- **ê²½ì œì **: ì¸ê°„ ë¼ë²¨ë§ ë¹„ìš© 0ì›, API ë¹„ìš© $20-30

### í–¥í›„ ì „ë§

ì´ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ì´ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤:

1. **ë‹¤ë¥¸ ë„ë©”ì¸ ì ìš©**: ì˜ë£Œ, ë²•ë¥ , ê¸°ìˆ  ë¬¸ì„œ ë“±
2. **ë‹¤êµ­ì–´ í™•ì¥**: ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“±
3. **ë” í° ê·œëª¨**: ìˆ˜ë§Œ~ìˆ˜ì‹­ë§Œ ë¬¸ì„œ ì²˜ë¦¬
4. **ì‹¤ì‹œê°„ í•™ìŠµ**: ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì‹œ ì ì§„ì  íŒŒì¸íŠœë‹

---

**ë³´ê³ ì„œ ì‘ì„±ì¼**: 2025ë…„ 12ì›” 29ì¼  
**ë²„ì „**: v1.0  
**ì‘ì„±ì**: IR System Optimization Team  
**ë¬¸ì˜**: [í”„ë¡œì íŠ¸ ì´ë©”ì¼/ì—°ë½ì²˜]

---

**ë¶€ë¡: ì½”ë“œ ìŠ¤ë‹ˆí« ë° ì‹¤í–‰ ê°€ì´ë“œëŠ” `finetune/` ë””ë ‰í† ë¦¬ ì°¸ì¡°**
