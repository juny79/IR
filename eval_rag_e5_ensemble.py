import os
import json
import sys
import numpy as np
import faiss
from pathlib import Path
from tqdm import tqdm
from kiwipiepy import Kiwi
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, CrossEncoder
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# LLM í´ë¼ì´ì–¸íŠ¸ë“¤
from models.solar_client import solar_client
from models.openai_client import openai_client

# ==========================================
# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ==========================================
DOC_PATH = "/root/IR/data/documents.jsonl"
EVAL_PATH = "/root/IR/data/eval.jsonl"
OUTPUT_FILE = "/root/IR/submission_e5_super_ensemble.csv"

# ëª¨ë¸ ì„¤ì •
EMBED_MODEL = "intfloat/multilingual-e5-large"
RERANK_MODEL = "BAAI/bge-reranker-v2-m3"

# íŒŒë¼ë¯¸í„° (ì•™ìƒë¸” ìµœì í™”)
RRF_K = 60
BM25_TOPN = 60
DENSE_TOPN = 60
TOP_CANDIDATES = 150 # í›„ë³´êµ° í™•ëŒ€
RERANK_BATCH = 32
FINAL_TOPK = 5

# ê°€ì¤‘ì¹˜ ì„¤ì • (GPT-4o ì¿¼ë¦¬ 3ê°œ + Solar Pro ì¿¼ë¦¬ 3ê°œ)
# [GPT_BM1, GPT_BM2, GPT_BM3, GPT_DS1, GPT_DS2, GPT_DS3, SLR_BM1, SLR_BM2, SLR_BM3, SLR_DS1, SLR_DS2, SLR_DS3]
# GPT-4oì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
W_GPT = [0.7, 0.4, 0.4, 1.8, 1.2, 1.2]
W_SLR = [0.4, 0.2, 0.2, 1.2, 0.8, 0.8]
SUPER_WEIGHTS = W_GPT + W_SLR

def load_jsonl(path: str):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            data.append(json.loads(line))
    return data

print("ğŸš€ ë°ì´í„° ë¡œë”© ë° ì¸ë±ì‹± ì¤€ë¹„...")
sys.stdout.flush()
documents = load_jsonl(DOC_PATH)
eval_data = load_jsonl(EVAL_PATH)
doc_contents = [d["content"] for d in documents]
doc_ids = [d["docid"] for d in documents]

kiwi = Kiwi()
def tokenizer(text: str):
    tokens = kiwi.tokenize(text)
    return [t.form for t in tokens if t.tag.startswith("N") or t.tag in ["SL", "SN"]]

print("BM25 ì¸ë±ì‹±...")
sys.stdout.flush()
tokenized_corpus = [tokenizer(doc) for doc in doc_contents]
bm25 = BM25Okapi(tokenized_corpus)

print(f"Vector ì¸ë±ì‹± ({EMBED_MODEL})...")
sys.stdout.flush()
embedder = SentenceTransformer(EMBED_MODEL, device="cuda") # GPU ì‚¬ìš©
FAISS_CACHE_PATH = "/root/IR/cache/faiss_e5_large.index"
if os.path.exists(FAISS_CACHE_PATH):
    index = faiss.read_index(FAISS_CACHE_PATH)
else:
    # ìƒëµ (ì´ë¯¸ ì¡´ì¬í•œë‹¤ê³  ê°€ì •)
    pass

print(f"Reranker ë¡œë”© ({RERANK_MODEL})...")
sys.stdout.flush()
reranker = CrossEncoder(RERANK_MODEL, max_length=512, device="cuda") # GPU ì‚¬ìš©

# ==========================================
# 2. ì•™ìƒë¸” ì¿¼ë¦¬ ìƒì„± í•¨ìˆ˜
# ==========================================
def get_queries(client, model_name, messages):
    system_prompt = """ë‹¹ì‹ ì€ RAGìš© ì§ˆë¬¸ ë¶„ì„ê¸°ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
{
  "should_search": true,
  "standalone_query": "êµ¬ì²´ì ì¸ ì§ˆë¬¸",
  "queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]
}"""
    try:
        if hasattr(client, 'model'): client.model = model_name
        resp = client._call_with_retry(
            prompt=[{"role": "system", "content": system_prompt}] + messages,
            temperature=0,
            max_tokens=1024,
            response_format={"type": "json_object"} if "gpt" in model_name else None
        )
        # JSON í´ë¦¬ë‹
        clean_text = resp.strip()
        if clean_text.startswith("```"):
            clean_text = clean_text.split("```")[1].replace("json", "").strip()
        parsed = json.loads(clean_text)
        return parsed.get("queries", [messages[-1]["content"]])[:3], parsed.get("standalone_query", messages[-1]["content"])
    except:
        return [messages[-1]["content"]], messages[-1]["content"]

def reciprocal_rank_fusion_weighted(rank_lists, k=60, weights=None):
    if weights is None: weights = [1.0] * len(rank_lists)
    scores = {}
    for w, rank_list in zip(weights, rank_lists):
        for rank, doc_idx in enumerate(rank_list):
            scores[doc_idx] = scores.get(doc_idx, 0.0) + w * (1.0 / (k + rank + 1))
    return sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

# ==========================================
# 3. ì‹¤í–‰
# ==========================================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for i, entry in enumerate(tqdm(eval_data)):
        eval_id = entry["eval_id"]
        messages = entry["msg"]
        
        # 1. GPT-4o ì¿¼ë¦¬
        gpt_queries, main_q = get_queries(openai_client, "gpt-4o", messages)
        # 2. Solar Pro ì¿¼ë¦¬
        slr_queries, _ = get_queries(solar_client, "solar-pro", messages)
        
        all_queries = gpt_queries + slr_queries # ì´ 6ê°œ
        rank_lists = []
        
        # BM25 Search
        for q in all_queries:
            tokens = tokenizer(q)
            if tokens:
                scores = bm25.get_scores(tokens)
                rank_lists.append(np.argsort(scores)[::-1][:BM25_TOPN].tolist())
            else:
                rank_lists.append([])
        
        # Dense Search
        q_embs = embedder.encode(["query: " + q for q in all_queries], normalize_embeddings=True)
        for q_emb in q_embs:
            _, f_idx = index.search(np.expand_dims(q_emb, 0), DENSE_TOPN)
            rank_lists.append(f_idx[0].tolist())
            
        # RRF (BM25 6ê°œ + Dense 6ê°œ = 12ê°œ ë¦¬ìŠ¤íŠ¸)
        # SUPER_WEIGHTS ìˆœì„œ: [GPT_BM1,2,3, SLR_BM1,2,3, GPT_DS1,2,3, SLR_DS1,2,3]
        # ìœ„ì—ì„œ rank_listsì— ë„£ì€ ìˆœì„œëŠ” [BM_GPT1,2,3, BM_SLR1,2,3, DS_GPT1,2,3, DS_SLR1,2,3]
        current_weights = [0.7, 0.4, 0.4, 0.4, 0.2, 0.2, 1.8, 1.2, 1.2, 1.2, 0.8, 0.8]
        
        candidate_indices = reciprocal_rank_fusion_weighted(rank_lists, k=RRF_K, weights=current_weights)
        top_candidates = candidate_indices[:TOP_CANDIDATES]
        
        # Rerank
        pairs = [[main_q, doc_contents[idx]] for idx in top_candidates]
        rerank_scores = reranker.predict(pairs, batch_size=RERANK_BATCH, show_progress_bar=False)
        sorted_ranks = sorted(zip(top_candidates, rerank_scores), key=lambda x: x[1], reverse=True)
        
        final_top_idx = [idx for idx, _ in sorted_ranks[:FINAL_TOPK]]
        final_topk = [doc_ids[idx] for idx in final_top_idx]
        
        res = {
            "eval_id": eval_id,
            "standalone_query": main_q,
            "topk": final_topk,
            "answer": doc_contents[final_top_idx[0]] if final_top_idx else ""
        }
        f.write(json.dumps(res, ensure_ascii=False) + "\n")
        f.flush()

print(f"ğŸš€ ìŠˆí¼ ì•™ìƒë¸” ì™„ë£Œ! ê²°ê³¼: {OUTPUT_FILE}")
